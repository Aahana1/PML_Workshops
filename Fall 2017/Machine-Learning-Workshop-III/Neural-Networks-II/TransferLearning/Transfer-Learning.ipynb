{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "This notebook requires the following package to be installed:\n",
    "\n",
    "1. tensorflow\n",
    "\n",
    "This can be installed via Anaconda (**`conda install <package>`**), or via PIP: (**`pip install <package>`**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    ">In this lab, we will be using transfer learning, which means we are starting with a model that has been already trained on another problem. We will then be retraining it on a similar problem. **Deep learning from scratch can take days, but transfer learning can be done in short order.**\n",
    "\n",
    "We're going to use a model trained on ImageNet's (http://image-net.org/) Large Visual Recognition Challenge dataset (http://www.image-net.org/challenges/LSVRC/2012/).\n",
    "\n",
    "These models can differentiate between 1,000 different classes, like Dalmatian or dishwasher. You will have a choice of model architectures, so you can determine the right tradeoff between speed, size and accuracy for your problem.\n",
    "\n",
    "We will use this same model, but retrain it to tell apart a small number of classes based on our own examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overview\n",
    "\n",
    "![transfer](imgs/transferlearningworkflow.png)\n",
    "\n",
    "In summary, by using a network trained on a similar problem, we can dramatically reduce the training time for a similar problem, and still obtain decent results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objectives\n",
    "\n",
    "We will learn:\n",
    "\n",
    "1. How to use Python and Tensorflow to train an image classifier\n",
    "2. How to classify images with this classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Data\n",
    "\n",
    "We'll be using an archive of creative-commons licensed flower photos (~ 278 MB in size). These can be found in the **tf_files/flower_photos** directory.\n",
    "\n",
    "Which, should contain the following:\n",
    "\n",
    "```\n",
    "    daisy/\n",
    "    dandelion/\n",
    "    roses/\n",
    "    sunflowers/\n",
    "    tulip/\n",
    "    LICENSE.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Retraining the network\n",
    "\n",
    "Google's retraining scropt can retrain either the Inception V3 Model (https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models) or ImageNet (https://research.googleblog.com/2017/06/mobilenets-open-source-models-for.html). \n",
    "\n",
    "The principal difference is that Inception V3 is optimized for accuracy, while the MobileNets are optimized to be small and efficient, at the cost of some accuracy.\n",
    "\n",
    "**We'll be using the MobileNet.**\n",
    "\n",
    "Inception V3 has a first-choice accuracy of 78% on ImageNet, but is the model is 85MB, and requires many times more processing than even the largest MobileNet configuration, which achieves 70.5% accuracy, with just a 19MB download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Configuration Options\n",
    "\n",
    "Pick the following configuration options:\n",
    "\n",
    "1. Input image resolution: 128,160,192, or 224px. Unsurprisingly, feeding in a higher resolution image takes more processing time, but results in better classification accuracy. Google recommends 224 as an initial setting.\n",
    "\n",
    "2. The relative size of the model as a fraction of the largest MobileNet: 1.0, 0.75, 0.50, or 0.25. Google recommends 0.5 as an initial setting. The smaller models run significantly faster, at a cost of accuracy.\n",
    "\n",
    "With the recommended settings, it typically takes only a couple of minutes to retrain on a laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You will pass the settings inside Linux shell variables. Set those shell variables as follows:\n",
    "\n",
    "```\n",
    "IMAGE_SIZE=224\n",
    "ARCHITECTURE=\"mobilenet_0.50_${IMAGE_SIZE}\"\n",
    "```\n",
    "\n",
    "*For Windows, add an environment variable following this link (https://www.computerhope.com/issues/ch000549.htm). If this doesn't work as intended, state the values of the paramters exactly when the values are passed to the script.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "The graph below shows the first-choice-accuracies of these configurations (y-axis), vs the number of calculations required (x-axis), and the size of the model (circle area).\n",
    "\n",
    "16 points are shown for mobilenet. For each of the 4 model sizes (circle area in the figure) there is one point for each image resolution setting. The 128px image size models are represented by the lower-left point in each set, while the 224px models are in the upper right.\n",
    "\n",
    "Other notable architectures are also included for reference. \"GoogleNet\" in this figure is \"Inception V1\" (https://github.com/tensorflow/models/tree/master/slim#pre-trained-models). An extended version of this figure is available in slides 84-89 from here: (http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tensorboard\n",
    "\n",
    "Before starting the training, launch tensorboard in the background. TensorBoard is a monitoring and inspection tool included with tensorflow. You will use it to monitor the training progress.\n",
    "\n",
    "```\n",
    "tensorboard --logdir tf_files/training_summaries &\n",
    "```\n",
    "\n",
    "*If you're on Windows, run this program in another command prompt window with*\n",
    "\n",
    "```\n",
    "START /B tensorboard --logdir tf_files/training_summaries\n",
    "``` \n",
    "\n",
    "*in the same directory as the notebook. (This is the closest Windows equivalent of running a program in the background on Unix/Linux.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The retrain script\n",
    "\n",
    "The retrain script is part of the tensorflow repo, but it is not installed as part of the pip package. So for simplicity I've included it in the codelab repository. You can run the script using the python command. Take a minute to skim its \"help\".\n",
    "\n",
    "```\n",
    "python -m scripts.retrain -h\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Running the training\n",
    "\n",
    "As noted in the introduction, Imagenet models are networks with millions of parameters that can differentiate a large number of classes. We're only training the final layer of that network, so training will end in a reasonable amount of time.\n",
    "\n",
    "Start your retraining with one big command (note the `--summaries_dir` option, sending training progress reports to the directory that tensorboard is monitoring) :\n",
    "\n",
    "```\n",
    "python -m scripts.retrain \\\n",
    "  --bottleneck_dir=tf_files/bottlenecks \\\n",
    "  --how_many_training_steps=500 \\\n",
    "  --model_dir=tf_files/models/ \\\n",
    "  --summaries_dir=tf_files/training_summaries/\"${ARCHITECTURE}\" \\\n",
    "  --output_graph=tf_files/retrained_graph.pb \\\n",
    "  --output_labels=tf_files/retrained_labels.txt \\\n",
    "  --architecture=\"${ARCHITECTURE}\" \\\n",
    "  --image_dir=tf_files/flower_photos\n",
    "```\n",
    "\n",
    "For Windows users, if you were able to set the environment variable, use this:\n",
    "\n",
    "```\n",
    "python -m scripts.retrain \\\n",
    "  --bottleneck_dir=tf_files/bottlenecks \\\n",
    "  --how_many_training_steps=500 \\\n",
    "  --model_dir=tf_files/models/ \\\n",
    "  --summaries_dir=tf_files/training_summaries/%ARCHITECTURE% \\\n",
    "  --output_graph=tf_files/retrained_graph.pb \\\n",
    "  --output_labels=tf_files/retrained_labels.txt \\\n",
    "  --architecture=%ARCHITECTURE% \\\n",
    "  --image_dir=tf_files/flower_photos\n",
    "```\n",
    "\n",
    "*If you're unable to make this work, refer to the explicit values of the `ARCHITECTURE` variable and replace it in the above command.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In the background\n",
    "\n",
    "This script downloads the pre-trained model, adds a new final layer, and trains that layer on the flower photos you've downloaded. \n",
    "\n",
    "ImageNet does not include any of these flower species we're training on here. However, the kinds of information that make it possible for ImageNet to differentiate among 1,000 classes are also useful for distinguishing other objects. By using this pre-trained network, we are using that information as input to the final classification layer that distinguishes our flower classes.\n",
    "\n",
    "*If you want to get higher accuracy, you can train for longer - e.g. instead of 500 iterations, increase this to a few thousand. Naturally, this will take longer, though.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How retraining works\n",
    "\n",
    "The first phase analyzes all the images on disk and calculates the bottleneck values for each of them. What's a bottleneck?\n",
    "\n",
    "![bottle](imgs/stack.png)\n",
    "\n",
    "These ImageNet models are made up of many layers stacked on top of each other, a simplified picture of Inception V3 from TensorBoard, is shown above (all the details are available in this paper: http://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf, with a complete picture on page 6). These layers are pre-trained and are already very valuable at finding and summarizing information that will help classify most images. For this codelab, you are training only the last layer (`final_training_ops` in the figure below). While all the previous layers retain their already-trained state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the below figure, the node labeled \"softmax\", on the left side, is the output layer of the original model. While all the nodes to the right of the \"softmax\" were added by the retraining script.\n",
    "\n",
    "![network](imgs/network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bottlenecks\n",
    "\n",
    "A bottleneck is an informal term we often use for the layer just before the final output layer that actually does the classification. \"Bottelneck\" is not used to imply that the layer is slowing down the network. We use the term bottleneck because near the output, the representation is much more compact than in the main body of the network.\n",
    "\n",
    "Every image is reused multiple times during training. Calculating the layers behind the bottleneck for each image takes a significant amount of time. Since these lower layers of the network are not being modified their outputs can be cached and reused.\n",
    "\n",
    "So the script is running the constant part of the network, everything below the node labeled Bottlene... above, and caching the results.\n",
    "\n",
    "The command you ran saves these files to the bottlenecks/ directory. If you rerun the script, they'll be reused, so you don't have to wait for this part again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More about training\n",
    "\n",
    "Once the script finishes generating all the bottleneck files, the actual training of the final layer of the network begins.\n",
    "\n",
    "The training operates efficiently by feeding the cached value for each image into the Bottleneck layer. The true label for each image is also fed into the node labeled `GroundTruth`. Just these two inputs are enough to calculate the classification probabilities, training updates, and the various performance metrics.\n",
    "\n",
    "As it trains you'll see a series of step outputs, each one showing training accuracy, validation accuracy, and the cross entropy:\n",
    "\n",
    "1. The **training accuracy** shows the percentage of the images used in the current training batch that were labeled with the correct class.\n",
    "2. **Validation accuracy**: The validation accuracy is the precision (percentage of correctly-labelled images) on a randomly-selected group of images from a different set.\n",
    "3. **Cross entropy** is a loss function that gives a glimpse into how well the learning process is progressing (lower numbers are better here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More on training\n",
    "\n",
    "The training's objective is to make the cross entropy as small as possible, so you can tell if the learning is working by keeping an eye on whether the loss keeps trending downwards, ignoring the short-term noise.\n",
    "\n",
    "If your model has finished generating the bottleneck files you can check your model's progress by opening TensorBoard (http://127.0.0.1:6006), and clicking on the figure's name to show them. TensorBoard may print out warnings to your command line. These can safely be ignored.\n",
    "\n",
    "As the program runs, at each step 10 images are chosen at random from the training set, finds their bottlenecks from the cache, and feeds them into the final layer to get predictions. Those predictions are then compared against the actual labels to update the final layer's weights through a backpropagation process.\n",
    "\n",
    "As the process continues, you should see the reported accuracy improve. After all the training steps are complete, the script runs a final test accuracy evaluation on a set of images that are kept separate from the training and validation pictures. This test evaluation provides the best estimate of how the trained model will perform on the classification task.\n",
    "\n",
    "You should see an accuracy value of between 85% and 99%, though the exact value will vary from run to run since there's randomness in the training process. (If you are only training on two classes, you should expect higher accuracy.) This number value indicates the percentage of the images in the test set that are given the correct label after the model is fully trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Using the retrained model\n",
    "\n",
    "The retraining script writes data to the following two files:\n",
    "\n",
    "1. `tf_files/retrained_graph.pb`, which contains a version of the selected network with a final layer retrained on your categories.\n",
    "2. `tf_files/retrained_labels.txt`, which is a text file containing labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classifying an image\n",
    "\n",
    "The codelab repo also contains a copy of tensorflow's label_image.py example, which you can use to test your network. Take a minute to read the help for this script:\n",
    "\n",
    "```\n",
    "python -m  scripts.label_image -h\n",
    "```\n",
    "\n",
    "As you can see, this Python program takes quite a few arguments. The defaults are all set for this project, but if you used a MobileNet architecture with a different image size you will need to set the `--input_size` argument using the variable you created earlier: `--input_size=${IMAGE_SIZE}` (for Windows users, this should be `--input_size=%IMAGE_SIZE`).\n",
    "\n",
    "Now, let's run the script on the image of a daisy (`tf_files/flower_photos/daisy/21652746_cc379e0eea_m.jpp`)\n",
    "\n",
    "![daisy](tf_files/flower_photos/daisy/21652746_cc379e0eea_m.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Testing the network\n",
    "\n",
    "Run:\n",
    "\n",
    "```\n",
    "python -m scripts.label_image \\\n",
    "    --graph=tf_files/retrained_graph.pb  \\\n",
    "    --image=tf_files/flower_photos/daisy/21652746_cc379e0eea_m.jpg\n",
    "```\n",
    "\n",
    "Each execution will print a list of flower labels (these match the folder labels in `tf_files`), in most cases with the correct flower on top (though each retrained model may be slightly different).\n",
    "\n",
    "You might get results like this for a daisy photo:\n",
    "\n",
    "```\n",
    "    daisy (score = 0.99071)\n",
    "    sunflowers (score = 0.00595)\n",
    "    dandelion (score = 0.00252)\n",
    "    roses (score = 0.00049)\n",
    "    tulips (score = 0.00032)\n",
    "```\n",
    "\n",
    "This indicates a high confidence (~99%) that the image is a daisy, and low confidence for any other label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further Testing\n",
    "\n",
    "You can use `label_image.py` to classify any image file you choose, either from your downloaded collection, or new ones. You just have to change the `--image` file name argument to the script.\n",
    "\n",
    "For example (`tf_files/flower_photos/roses/2414954629_3708a1a04d.jpg`):\n",
    "\n",
    "![rose](tf_files/flower_photos/roses/2414954629_3708a1a04d.jpg)\n",
    "\n",
    "```\n",
    "python -m scripts.label_image \\\n",
    "    --graph=tf_files/retrained_graph.pb  \\\n",
    "    --image=tf_files/flower_photos/roses/2414954629_3708a1a04d.jpg \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activity: What happens when you increase/decrease the learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extra: Try your own data set! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References:\n",
    "\n",
    "1. Heavily adapted from: https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
