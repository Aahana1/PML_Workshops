{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop2_exercise5",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1UKMYru5LzU",
        "colab_type": "text"
      },
      "source": [
        "# Day 2 Example 5\n",
        "# Object Detection and Tracking\n",
        "\n",
        "Last edited 2019/07/15\n",
        "\n",
        "some cells depend on imports or outputs from previous cells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZwPvMthvmmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change Runtime type by going to Runtime>Change runtime type>Hardware accelerator>GPU\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# import several packages and files with associated file structure\n",
        "# requirements\n",
        "!pip install pyamg\n",
        "!pip install opencv-python\n",
        "\n",
        "# data\n",
        "!apt-get install subversion\n",
        "!svn checkout \"https://github.com/jojker/PML_Workshops/trunk/Summer 2019/Day 2 - Goal 1 - Turning Images into Data/Ex 5 - object detection and tracking/Data\"\n",
        "\n",
        "path = 'Data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shQnDsxoELxQ",
        "colab_type": "text"
      },
      "source": [
        "# Finding contours and the center of a blob with OpenCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMG9OPStDNKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# read image through command line\n",
        "img = cv2.imread(path + 'multiple-blob.png')\n",
        "\n",
        "# convert the image to grayscale\n",
        "gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "print(\"Grayscale Image\")\n",
        "cv2_imshow(gray_image)\n",
        "\n",
        "# convert the grayscale image to binary image\n",
        "ret,thresh = cv2.threshold(gray_image,127,255,0)\n",
        "\n",
        "# find contours in the binary image\n",
        "im2, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# draw the contours on the image\n",
        "img_contours = img.copy()\n",
        "cv2.drawContours(img_contours, contours, -1, (10,10,255), 2)\n",
        "\n",
        "print(\"Image with Contours\")\n",
        "cv2_imshow(img_contours)\n",
        "\n",
        "for c in contours:\n",
        "  # calculate moments for each contour\n",
        "  M = cv2.moments(c)\n",
        "  \n",
        "  # catch zero M00 term and calculate x,y coordinate of center\n",
        "  if M[\"m00\"] != 0:\n",
        "    cX = int(M[\"m10\"] / M[\"m00\"])\n",
        "    cY = int(M[\"m01\"] / M[\"m00\"])\n",
        "  else:\n",
        "    cX, cY = 0, 0\n",
        "    \n",
        "  # add point on image indicating the centroid\n",
        "  cv2.circle(img, (cX, cY), 4, (255, 255, 255), -1)\n",
        "  \n",
        "  # add text to the image if desired\n",
        "  # cv2.putText(img, \"centroid\", (cX - 25, cY - 25),cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "# display image with centroids\n",
        "print(\"\\n\\nImage with Centroids\")\n",
        "cv2_imshow(img)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuLvjCbhdBaJ",
        "colab_type": "text"
      },
      "source": [
        "# Object Recognition and Object Detection\n",
        "\n",
        "**Object recognition**  identifies which objects are present in an image. It takes the entire image as an input and outputs class labels and class probabilities of objects present in that image\n",
        "\n",
        " **Object detection** not only tells you which objects are present in the image, it also outputs bounding boxes (x, y, width, height) to indicate the location of the objects inside the image\n",
        "\n",
        "\n",
        "To localize an object, we have to select sub-regions (patches) of the image and apply the object recognition algorithm to these patches. The location of the objects is given by the location of the image patches where the class probability returned by the object recognition algorithm is high.\n",
        "\n",
        "The most straightforward way to generate smaller sub-regions (patches) is called the **sliding window** approach where a box slides over an image to select a patch and classify each patch using the object recognition model.  It is an exhaustive search and can be costly because often times it is important to search multiple aspect ratios over the entire image\n",
        "\n",
        "<img src=https://www.learnopencv.com/wp-content/uploads/2017/09/object-recognition-dogs-768x436.jpg width=\"500\">\n",
        "\n",
        "Sliding window problems can be solved with **region proposal** where the output is all patches that are most likely to be objects. The proposed regions can be noisy, overlapping, and may not contain the object perfectly. The regions with high probability scores are the object locations.\n",
        "\n",
        "<img src=https://www.learnopencv.com/wp-content/uploads/2017/10/object-recognition-false-positives-true-positives-768x436.jpg width=\"500\">\n",
        "\n",
        "In region proposal algorithms possible objects are identified using segmentation where similar adjacent regions are grouped based on criteria such as color, texture, etc.  The final number of proposals generated are many times less than that ofthe sliding window approach and the regions are of different aspect ratios.\n",
        "\n",
        "It is okay for the region proposal to produce a lot of false positives so long as it catches all the true positives (high recall) because the false positives will be rejected by the object recognition step.\n",
        "\n",
        "**Selective search** is a popular choice for region proposal because it is fast and has high recall. In selective search the image is segmented by pixel intensity\n",
        "\n",
        "original image\n",
        "\n",
        "<img src=https://www.learnopencv.com/wp-content/uploads/2017/09/breakfast-300x200.jpg width=\"300\">\n",
        "\n",
        "segmented image\n",
        "\n",
        "<img src=https://www.learnopencv.com/wp-content/uploads/2017/09/breakfast_fnh-300x200.jpg width=\"300\">\n",
        "\n",
        "Selective search uses an oversegmented image as the initial input and performs the following things\n",
        "1. Add all bounding boxes corresponding to segmented parts to the list of region proposals\n",
        "2. Group Adjacent segments based on similarity\n",
        "3. Go to step 1\n",
        "\n",
        "oversegmented image\n",
        "\n",
        "<img src=https://www.learnopencv.com/wp-content/uploads/2017/09/breakfast_oversegment-300x200.jpg width=\"300\">\n",
        "\n",
        "At each iteration, larger segments are formed and added to the list of region proposals.\n",
        "\n",
        "<img src=https://www.learnopencv.com/wp-content/uploads/2017/09/hierarchical-segmentation-1.jpg width=\"800\">\n",
        "\n",
        "Selective search uses 4 similarity measures based on color, texture, size, and shape. \n",
        "\n",
        "Color:\n",
        "A color histogram of 25 bins is calculated for each channel of the image and histograms for all channels are concatenated to obtain a color descriptor resulting into a 25×3 = 75-dimensional color descriptor.\n",
        "Color similarity of two regions is based on histogram intersection and can be calculated as:\n",
        "\n",
        "<img src=https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-3a99604c3b9fc1664b0ebd9b16aa190c_l3.png width=\"300\">\n",
        "\n",
        "\n",
        "c<sub>i</sub><sup>k</sup> is the histogram value for k<sup>th</sup> bin in color descriptor\n",
        "\n",
        "Texture:\n",
        "Texture features are calculated by extracting Gaussian derivatives at 8 orientations for each channel. For each orientation and for each color channel, a 10-bin histogram is computed resulting into a 10x8x3 = 240-dimensional feature descriptor. \n",
        "\n",
        "<img src=https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-169d419080f56b69f9645cd13ee5b0ac_l3.png width=\"300\">\n",
        "\n",
        "Size Similarity: \n",
        "Size similarity encourages smaller regions to merge early. It ensures that region proposals at all scales are formed at all parts of the image. If this similarity measure is not taken into consideration a single region will keep gobbling up all the smaller adjacent regions one by one and hence region proposals at multiple scales will be generated at this location only. Size similarity is defined as:\n",
        "\n",
        "<img src=https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-ed6bd32a9661aa84228d1ca1c75f5d29_l3.png width=\"325\">\n",
        "\n",
        "where size(im) is the size of image in pixels\n",
        "\n",
        "Shape Compatibility:\n",
        "Shape compatibility measures how well two regions (r<sub>i</sub> and r<sub>j</sub>) fit into each other. If r<sub>i</sub> fits into r<sub>j</sub> we would like to merge them in order to fill gaps.  If they are not touching they should not be merged.  Shape compatibility is defined as:\n",
        "\n",
        "<img src=https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-9a3fdf638488b3c77915b9b83bf2f3e1_l3.png width=\"400\">\n",
        "\n",
        "where size(BB<sub>ij</sub>) is a bounding box around r<sub>i</sub> and r<sub>j</sub>.\n",
        "\n",
        "Final Similarity:\n",
        "The final similarity between two regions is defined as a linear combination of aforementioned 4 similarities.\n",
        "\n",
        "<img src=https://www.learnopencv.com/wp-content/ql-cache/quicklatex.com-67a3c5c3f45a9407ee513056c759f095_l3.png width=\"700\">\n",
        "\n",
        "where r<sub>i</sub> and r<sub>j</sub> are two regions or segments in the image and a<sub>i</sub> is either 0 or 1 denoting if the similarity measure is used or not.\n",
        "\n",
        "Selective Search implementation in OpenCV gives thousands of region proposals arranged in decreasing order of objectness. For clarity, we are sharing results with top 200-250 boxes drawn over the image. In general 1000-1200 proposals are good enough to get all the correct region proposals.\n",
        "\n",
        "<img src=https://www.learnopencv.com/wp-content/uploads/2017/09/breakfast-top-200-proposals-300x200.jpg width=\"500\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrMJjOkGeIb8",
        "colab_type": "text"
      },
      "source": [
        "# Selective Search Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBUztkLldHZQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        " \n",
        "import sys\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# select fast ('f') but low recall selective search or slow ('q') but high recall\n",
        "# selective search\n",
        "selection = 'q'\n",
        "\n",
        "im_loc = path + 'dog.jpg'\n",
        "\n",
        "# speed-up using multithreads\n",
        "cv2.setUseOptimized(True);\n",
        "cv2.setNumThreads(4);\n",
        "\n",
        "# read image\n",
        "im = cv2.imread(im_loc)\n",
        "\n",
        "# resize image\n",
        "newHeight = 300\n",
        "newWidth = int(im.shape[1]*newHeight/im.shape[0])\n",
        "im = cv2.resize(im, (newWidth, newHeight))    \n",
        "\n",
        "# create Selective Search Segmentation Object using default parameters\n",
        "ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
        "\n",
        "# set input image on which we will run segmentation\n",
        "ss.setBaseImage(im)\n",
        "\n",
        "# Switch to fast but low recall Selective Search method\n",
        "if (selection == 'f'):\n",
        "  ss.switchToSelectiveSearchFast()\n",
        "\n",
        "# Switch to high recall but slow Selective Search method\n",
        "elif (selection == 'q'):\n",
        "  ss.switchToSelectiveSearchQuality()\n",
        "\n",
        "# run selective search segmentation on input image\n",
        "rects = ss.process()\n",
        "print('Total Number of Region Proposals: {}'.format(len(rects)))\n",
        "\n",
        "# number of region proposals to show\n",
        "numShowRects = 60\n",
        "\n",
        "print('showing', numShowRects, 'region proposals')\n",
        " \n",
        "# create a copy of original image\n",
        "imOut = im.copy()\n",
        "  \n",
        "# itereate over all the region proposals\n",
        "for i, rect in enumerate(rects):\n",
        "  # draw rectangle for region proposal till numShowRects\n",
        "  if (i < numShowRects):\n",
        "    x, y, w, h = rect\n",
        "    cv2.rectangle(imOut, (x, y), (x+w, y+h), (0, 255, 0), 1, cv2.LINE_AA)\n",
        "  else:\n",
        "    break\n",
        "\n",
        "# show output of resized image with proposal boxes\n",
        "cv2_imshow(imOut)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QndqFH2uzN7d",
        "colab_type": "text"
      },
      "source": [
        "#Face Detection\n",
        "\n",
        "Viola-Jones Object Detection Framework\n",
        "\n",
        "This algorithm is named after two computer vision researchers who proposed the method in 2001: Paul Viola and Michael Jones.\n",
        "\n",
        "Given an image, the algorithm looks at many smaller subregions and tries to find a face by looking for specific features in each subregion. It needs to check many different positions and scales because an image can contain many faces of various sizes. Viola and Jones used Haar-like features to detect faces.\n",
        "\n",
        "Haar-Like Features:\n",
        "\n",
        "All human faces share some similarities. These similarities help the algorithm understand if an image contains a human face.\n",
        "\n",
        "A simple way to find contrast between regions is to sum up the pixel values of both regions and compare them. The sum of pixel values in the darker region will be smaller than the sum of pixels in the lighter region. This can be accomplished using Haar-like features.\n",
        "\n",
        "A Haar-like feature is represented by taking a rectangular part of an image and dividing that rectangle into multiple parts. They are often visualized as black and white adjacent rectangles:\n",
        "\n",
        "<img src=https://files.realpython.com/media/Haar.885b5c872b35.png width=\"500\">\n",
        "\n",
        "In this image, you can see 4 basic types of Haar-like features:\n",
        "\n",
        "1. Horizontal feature with two rectangles - edge detection\n",
        "2. Vertical feature with two rectangles - edge detection\n",
        "3. Vertical feature with three rectangles - line detection\n",
        "4. Diagonal feature with four rectangles\n",
        "\n",
        "The value of the feature is calculated as a single number: the sum of pixel values in the black area minus the sum of pixel values in the white area. For uniform areas like a wall, this number would be close to zero.\n",
        "\n",
        "To be useful, a Haar-like feature needs to give a large number, meaning the areas in the black and white rectangles are very different. There are known features that perform very well to detect human faces\n",
        "\n",
        "Integral Images:\n",
        "\n",
        "An integral image is the name of both a data structure and an algorithm used to obtain this data structure. It is used as a quick and efficient way to calculate the sum of pixel values in an image or rectangular part of an image. The value of each point is the sum of all pixels above and to the left, including the target pixel:\n",
        "\n",
        "<img src=https://files.realpython.com/media/Integral_image.ff570b17c188.png width='400'>\n",
        "\n",
        "The integral image can be calculated in a single pass over the original image. This reduces summing the pixel intensities within a rectangle into only three operations with four numbers, regardless of rectangle size:\n",
        "\n",
        "<img src=https://files.realpython.com/media/ABCD.97ca0ef04d39.png width='200'>\n",
        "\n",
        "The sum of pixels in the rectangle ABCD can be derived from the values of points A, B, C, and D, using the formula D - B - C + A\n",
        "\n",
        "But how do you decide which of these features and in what sizes to use for finding faces in images? This is solved by a machine learning algorithm called boosting\n",
        "\n",
        "Adaptive Boosting:\n",
        "\n",
        "Boosting is based on the following question: “Can a set of weak learners create a single strong learner?” A weak learner is defined as a classifier that is only slightly better than random guessing.\n",
        "\n",
        "In face detection, this means that a weak learner can classify a subregion of an image as a face or not-face only slightly better than random guessing.\n",
        "\n",
        "The power of boosting comes from combining many (thousands) of weak classifiers into a single strong classifier. In the Viola-Jones algorithm, each Haar-like feature represents a weak learner. To decide the type and size of a feature that goes into the final classifier, adaptive boosting checks the performance of all classifiers that you supply to it.\n",
        "\n",
        "To calculate the performance of a classifier, you evaluate it on all subregions of all the images used for training. Some subregions will produce a strong response in the classifier. Those will be classified as positives, meaning the classifier thinks it contains a human face.\n",
        "\n",
        "<img src=https://files.realpython.com/media/AdaBoost-7.2ec2db197252.png width='300'>\n",
        "\n",
        "Cascading Classifiers:\n",
        "\n",
        "The definition of a cascade is a series of waterfalls coming one after another. A similar concept is used in computer science to solve a complex problem with simple units. Here we want to reduce the number of computations for each image.\n",
        "\n",
        "Viola and Jones turned their strong classifier (consisting of thousands of weak classifiers) into a cascade where each weak classifier represents one stage. The job of the cascade is to quickly discard non-faces and avoid wasting precious time and computations.\n",
        "\n",
        "When an image subregion enters the cascade, it is evaluated by the first stage. If that stage evaluates the subregion as positive, meaning that it thinks it’s a face, the output of the stage is maybe.\n",
        "\n",
        "If any stage gives a negative evaluation, then the image is immediately discarded as not containing a human face.\n",
        "\n",
        "<img src=https://files.realpython.com/media/Classifier_cascade.e3b2a5652044.png width='500'>\n",
        "\n",
        "This is designed so that non-faces get discarded very quickly, which saves a lot of time and computational resources.\n",
        "\n",
        "Though the theory may sound complicated, in practice it is quite easy. The cascades themselves are just a bunch of XML files that contain OpenCV data used to detect objects. You initialize your code with the cascade you want, and then it does the work for you.\n",
        "\n",
        "Since face detection is such a common case, OpenCV comes with a number of built-in cascades for detecting everything from faces to eyes to hands to legs.\n",
        "\n",
        "We will load a casade set called haarcascade_frontalface_alt.xml from the opencv database\n",
        "https://github.com/opencv/opencv/blob/master/data/haarcascades/haarcascade_frontalface_alt.xml"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rAmFkADzNbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "\n",
        "# Read image from your local file system\n",
        "im = cv2.imread(path + 'people.jpg')\n",
        "\n",
        "# resize image\n",
        "newHeight = 400\n",
        "newWidth = int(im.shape[1]*newHeight/im.shape[0])\n",
        "im = cv2.resize(im, (newWidth, newHeight))   \n",
        "\n",
        "# Convert color image to grayscale for Viola-Jones\n",
        "grayscale_image = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Load the classifier and create a cascade object for face detection\n",
        "face_cascade = cv2.CascadeClassifier(path + 'haarcascade_frontalface_alt.xml')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L86AvssS2GjL",
        "colab_type": "text"
      },
      "source": [
        "The face_cascade object has a method detectMultiScale(), which receives an image as an argument and runs the classifier cascade over the image. The term MultiScale indicates that the algorithm looks at subregions of the image in multiple scales, to detect faces of varying sizes.\n",
        "\n",
        "The detectMultiScale function is a general function that detects objects. Since we are calling it on the face cascade, that’s what it detects. The function has several options\n",
        "\n",
        "1. image to be searched for feature.\n",
        "\n",
        "2. **scaleFactor** - since some faces may be closer to the camera, they would appear bigger than the faces in the back. The scale factor compensates for this.\n",
        "\n",
        "3. The detection algorithm uses a moving window to detect objects. **minNeighbors** defines how many objects are detected near the current one before it declares the face found. **minSize** gives the size of each window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8flw9pL2M5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Detect faces in the image\n",
        "detected_faces = face_cascade.detectMultiScale(\n",
        "    grayscale_image,\n",
        "    scaleFactor=1.1,\n",
        "    minNeighbors=7,\n",
        "    minSize=(30, 30)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEWA2nZD2lih",
        "colab_type": "text"
      },
      "source": [
        "The variable detected_faces now contains all the detections for the target image. To visualize the detections, you need to iterate over all detections and draw rectangles over the detected faces.\n",
        "\n",
        "OpenCV’s rectangle() draws rectangles over images, and it needs to know the pixel coordinates of the top-left and bottom-right corner. The coordinates indicate the row and column of pixels in the image.\n",
        "\n",
        "Luckily, detections are saved as pixel coordinates. Each detection is defined by its top-left corner coordinates and width and height of the rectangle that encompasses the detected face.\n",
        "\n",
        "Adding the width to the row and height to the column will give you the bottom-right corner of the image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a6oRojn2nJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for (column, row, width, height) in detected_faces:\n",
        "    cv2.rectangle(\n",
        "        im,\n",
        "        (column, row),\n",
        "        (column + width, row + height),\n",
        "        (0, 255, 0),\n",
        "        2\n",
        "    )\n",
        "\n",
        "\"\"\"\n",
        "rectangle() accepts the following arguments:\n",
        "\n",
        "The original image\n",
        "The coordinates of the top-left point of the detection\n",
        "The coordinates of the bottom-right point of the detection\n",
        "The color of the rectangle (a tuple that defines the amount of red, green, and blue (0-255))\n",
        "The thickness of the rectangle lines\n",
        "\"\"\"\n",
        "\n",
        "# Dislplay the image\n",
        "cv2_imshow(im)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6chZNawXNczi",
        "colab_type": "text"
      },
      "source": [
        "#Brief Example on Reinforcement Learning\n",
        "- quickly train a model to identify new objects\n",
        "\n",
        "ImageNet is a research project to develop a large database of images with annotations, e.g. images and their descriptions. The goal is to classify all objects into 1000 categories. \n",
        "\n",
        "Researchers from the Oxford Visual Geometry Group, or VGG for short, participated in the challenge and created the VGG16 model (16 layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "303kCUcv3Ewy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "from keras.applications.vgg16 import VGG16\n",
        "\n",
        "# load the model (this may take a couple of mintues, but you only have to do it once per colab session)\n",
        "model = VGG16()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH9r_UL3ICSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize the model (don't get hung up on the details here)\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1DUVi48H5YA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# some imports to get plotting working dynamically through a for loop\n",
        "import matplotlib.pyplot as plt\n",
        "from time import sleep\n",
        "import glob\n",
        "\n",
        "# select all the images to test using glob to extract all files with the extension .jpg\n",
        "new_image_files = glob.glob(path + '*.jpg')\n",
        "\n",
        "for file in new_image_files:\n",
        "  \n",
        "  # load the image\n",
        "  image = load_img(file, target_size=(224, 224))\n",
        "  \n",
        "  # convert the image pixels to a numpy array\n",
        "  im = img_to_array(image)\n",
        "\n",
        "  # reshape data for the model\n",
        "  im = im.reshape((1, im.shape[0], im.shape[1], im.shape[2]))\n",
        "\n",
        "  # prepare the image for the VGG model\n",
        "  im = preprocess_input(im)\n",
        "  \n",
        "  # predict the probability across all output classes\n",
        "  yhat = model.predict(im)\n",
        "  \n",
        "  # convert the probabilities to class labels\n",
        "  label = decode_predictions(yhat)\n",
        "  \n",
        "  # retrieve the most likely result, e.g. highest probability\n",
        "  label = label[0][0]\n",
        "\n",
        "  # print the classification and show the image\n",
        "  print('%s (%.2f%%)' % (label[1], label[2]*100))\n",
        "  img = plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  sleep(1)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsgAyGvRM-Vm",
        "colab_type": "text"
      },
      "source": [
        "#How to go from this to a new data set?\n",
        "\n",
        "Remove fully-connected layers at the top of the network and add new fully connected layers with the correct number of outputs\n",
        "\n",
        "Training will train the new fully-connected layers and fine-tune the convolutional layers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5i5GEBzNu_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.applications.vgg16 import decode_predictions\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.models import Model\n",
        "import numpy as np\n",
        "\n",
        "#Get back the convolutional part of a VGG network trained on ImageNet\n",
        "model_vgg16_conv = VGG16(weights='imagenet', include_top=False)\n",
        "\n",
        "# make the VGG16 layers non trainable (making use of their pre training)\n",
        "# this reduces the number of training parameters from 100 Million to 1 Million (really depends on first layer after vgg16 layers)\n",
        "for layer in model_vgg16_conv.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "#Create your own input format (here 224x224x3)\n",
        "input = Input(shape=(224,224,3),name = 'image_input')\n",
        "\n",
        "#Use the generated model \n",
        "output_vgg16_conv = model_vgg16_conv(input)\n",
        "\n",
        "#Add the fully-connected layers \n",
        "x = Flatten(name='flatten')(output_vgg16_conv)\n",
        "x = Dense(32, activation='relu', name='fc1')(x)\n",
        "x = Dense(64, activation='relu', name='fc2')(x)\n",
        "\n",
        "x = Dense(7, activation='softmax', name='predictions')(x)\n",
        "\n",
        "#Create your own model \n",
        "my_model = Model(input=input, output=x)\n",
        "my_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AvvzwCGPJZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# collect data to train the new model\n",
        "import glob\n",
        "\n",
        "# select all the images to train with using glob to extract all files int he folder with the extension .jpg\n",
        "poke_folders = glob.glob(path +'pokemon/*/')\n",
        "poke_type = []\n",
        "for p in poke_folders:\n",
        "  p = p[:-1]\n",
        "  p = p.split('/')\n",
        "  poke_type.append(p[-1])\n",
        "\n",
        "print(\"new labels\")\n",
        "print(poke_type)\n",
        "\n",
        "# create integers associated with each label\n",
        "poke_num = range(len(poke_type))\n",
        "\n",
        "labels = []\n",
        "labels_num = []\n",
        "total_count = 0\n",
        "label_count = 0\n",
        "\n",
        "for label in poke_type:\n",
        "  \n",
        "  new_images = glob.glob(path + 'pokemon/' + label + '/*.jpg')\n",
        "  for file in new_images:\n",
        "    \n",
        "    # specify the label and a corresponding number for the label\n",
        "    labels.append(label)\n",
        "    labels_num.append(label_count)\n",
        "    \n",
        "    # load the images\n",
        "    image = load_img(file, target_size=(224, 224))\n",
        "\n",
        "    # convert the image pixels to a numpy array\n",
        "    im = img_to_array(image)\n",
        "\n",
        "    # reshape data for the model\n",
        "    im = im.reshape((1, im.shape[0], im.shape[1], im.shape[2]))\n",
        "    \n",
        "    # preprocess the image for the input type of the VGG16 model\n",
        "    # im = preprocess_input(im)\n",
        "    \n",
        "    # initialize the data varialbe if this is the first time through the loops\n",
        "    if total_count==0:\n",
        "      data = im\n",
        "    \n",
        "    # otherwise, stack the arrays to create a single input array\n",
        "    else:\n",
        "      # stack the new arrays\n",
        "      data = np.concatenate((data,im))\n",
        "    \n",
        "    # bump the counter\n",
        "    total_count = total_count + 1\n",
        "  \n",
        "  # bump the label count\n",
        "  label_count = label_count + 1\n",
        "\n",
        "\n",
        "# show the shape of the new data\n",
        "print('\\ndata shape: ', data.shape)\n",
        "print('number of images: ',total_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_iahjdmKU7Kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make the labels a numpy array\n",
        "labels_num1 = np.reshape(np.array(labels_num),(len(labels_num),1))\n",
        "\n",
        "shape0 = data.shape[0]\n",
        "shape1 = data.shape[1]\n",
        "shape2 = data.shape[2]\n",
        "shape3 = data.shape[3]\n",
        "\n",
        "data = np.reshape(data, (shape0,shape1*shape2*shape3))\n",
        "data = np.append(data,labels_num1,axis=1)\n",
        "\n",
        "# shuffle the array so the data is not fed in sequentially by type\n",
        "np.random.shuffle(data)\n",
        "print('data shape after shuffle: ', data.shape)\n",
        "\n",
        "# separate the last column to extract the labels from the data set\n",
        "labels_num = data[:, -1] # for last column\n",
        "data = data[:, :-1]        # for all but last column\n",
        "print('data shape after shuffle with the labels removed: ', data.shape)\n",
        "\n",
        "data = np.reshape(data,(shape0,shape1,shape2,shape3))\n",
        "\n",
        "print('data shape after reshape to the original shape: ', data.shape)\n",
        "print('labels shape: ', labels_num.shape)\n",
        "\n",
        "# make the labels into one hot encoded labels\n",
        "# example 3 -> (0,0,0,1,0,0,0) and 0 -> (1,0,0,0,0,0,0) and 5 -> (0,0,0,0,0,1,0)\n",
        "labels_num.tolist()\n",
        "labels_one_hot = []\n",
        "for i in labels_num:\n",
        "  if i==0:\n",
        "    labels_one_hot.append((1,0,0,0,0,0,0))\n",
        "  if i==1:\n",
        "    labels_one_hot.append((0,1,0,0,0,0,0))\n",
        "  if i==2:\n",
        "    labels_one_hot.append((0,0,1,0,0,0,0))\n",
        "  if i==3:\n",
        "    labels_one_hot.append((0,0,0,1,0,0,0))\n",
        "  if i==4:\n",
        "    labels_one_hot.append((0,0,0,0,1,0,0))\n",
        "  if i==5:\n",
        "    labels_one_hot.append((0,0,0,0,0,1,0))\n",
        "  if i==6:\n",
        "    labels_one_hot.append((0,0,0,0,0,0,1))\n",
        "\n",
        "labels_one_hot = np.array(labels_one_hot)\n",
        "print('one hot labels shape: ', labels_one_hot.shape)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHpQAIz9U9Dw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# compile the model\n",
        "import keras.optimizers\n",
        "\n",
        "# select an optimizer\n",
        "\n",
        "#sgd = keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
        "#rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
        "#adagrad = keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
        "#adadelta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)\n",
        "#adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "#adamax = keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
        "nadam = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
        "\n",
        "my_model.compile(optimizer=nadam, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyLZFr1zLYxU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check that the GPU is available\n",
        "\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtGUXReynHIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train the model using keras fit for a small number of epochs (this may take ~2 minutes per epoch without the GPU)\n",
        "with tf.device('/gpu:0'):\n",
        "  my_model.fit(data,labels_one_hot, epochs=20, verbose=1, shuffle=True, batch_size=32)\n",
        "\n",
        "  # evaluate the model\n",
        "  scores = my_model.evaluate(data,labels_one_hot)\n",
        "\n",
        "print(\"%s: %.2f%%\" % (my_model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5lr05bhn4VO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save model weights\n",
        "my_model.save_weights(path + 'pokemon_model.h5')\n",
        "print(\"Saved model to disk\")\n",
        "print(\"Need to export from Keras before closing session!\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDn91Kkin-zS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OR load some pretrained weights instead of training the model using the above two above cells\n",
        "my_model.load_weights(path + 'pokemon_model.h5')\n",
        "print(\"Loaded model from disk\")\n",
        "\n",
        "# evaluate the model\n",
        "scores = my_model.evaluate(data,labels_one_hot)\n",
        "print(\"%s: %.2f%%\" % (my_model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax8jDhiKrP9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run the model on some images it hasn't seen\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "from time import sleep\n",
        "\n",
        "\n",
        "files = glob.glob(path + '/pokemon/*.jpg')\n",
        "for picture in files:\n",
        "  image = load_img(picture, target_size=(224, 224))\n",
        "  \n",
        "  # convert the image pixels to a numpy array\n",
        "  im = img_to_array(image)\n",
        "  \n",
        "  # reshape data for the model\n",
        "  im = im.reshape((1, im.shape[0], im.shape[1], im.shape[2]))\n",
        "\n",
        "  # predict the probability across all output classes\n",
        "  predicted_label = my_model.predict(im)\n",
        "  val = np.argmax(predicted_label[0])\n",
        "  print(poke_type[val])\n",
        "  print('activation percentage: ', round(100*predicted_label[0][val],4), '%')\n",
        "\n",
        "  # show the image\n",
        "  img = plt.imshow(image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  sleep(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJNw1kQ6sywR",
        "colab_type": "text"
      },
      "source": [
        "It may be helpful to learn how to save and load entire models \n",
        "\n",
        "see the link below for a description\n",
        "\n",
        "https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
        "\n",
        "Additional Resources\n",
        "\n",
        "https://www.learnopencv.com/object-tracking-using-opencv-cpp-python/\n",
        "https://www.learnopencv.com/deep-learning-based-object-detection-and-instance-segmentation-using-mask-r-cnn-in-opencv-python-c/\n",
        "https://www.learnopencv.com/find-center-of-blob-centroid-using-opencv-cpp-python/\n",
        "https://www.learnopencv.com/selective-search-for-object-detection-cpp-python/\n",
        "https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html?highlight=object%20detection\n",
        "https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html?highlight=object%20detection\n",
        "https://realpython.com/traditional-face-detection-python/\n",
        "https://realpython.com/face-recognition-with-python/\n",
        "https://www.pyimagesearch.com/2017/12/11/image-classification-with-keras-and-deep-learning/\n",
        "https://www.tensorflow.org/lite/models/object_detection/overview\n",
        "https://research.google.com/seedbank/seed/tfhub_action_recognition_model\n",
        "https://research.google.com/seedbank/seed/the_whatif_tool_analyzing_an_image_classifier"
      ]
    }
  ]
}