{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop2_exercise4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPwXQG7wPuiq",
        "colab_type": "text"
      },
      "source": [
        "# 3D Structure from overlapping 2D images\n",
        "\n",
        "## Authors: Muneeb Aadil, Sibt Ul Hussain\n",
        "\n",
        "### Tutorial adapted to Google Colab from: https://github.com/muneebaadil/how-to-sfm/blob/master/tutorial/tutorial.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnFObki81nUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove any unnecessary colab folders\n",
        "# !rm -rf <folder_name>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u3UIMWYo9K4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# requirements\n",
        "!pip install opencv-python==3.4.2.16\n",
        "!pip install opencv-contrib-python==3.4.2.16\n",
        "!pip install open3d-python\n",
        "\n",
        "# data\n",
        "!apt-get install subversion\n",
        "!svn checkout \"https://github.com/jojker/PML_Workshops/trunk/Summer 2019/Day 2 - Goal 1 - Turning Images into Data/Ex 4 - 3D structure from overlapping 2D images/Data\"\n",
        "\n",
        "# move two needed python files (utils.py and SfM.py) to the current directory\n",
        "!mv /content/Data/utils.py /content\n",
        "!mv /content/Data/SfM.py /content\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op7hIHKbQTE6",
        "colab_type": "text"
      },
      "source": [
        "# Chapter 2. Epipolar Geometry\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TQjjHgys_zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# loading needed libraries\n",
        "import utils as ut \n",
        "import SfM as sfmnp\n",
        "import matplotlib.pyplot as plt \n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import cv2\n",
        "import numpy as np\n",
        "import open3d as o3d\n",
        "\n",
        "path = \"/content/Data/images/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkeZASULKjR9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run this cell to use plotly in google colab\n",
        "def configure_plotly_browser_state():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M09yqaKGQdlS",
        "colab_type": "text"
      },
      "source": [
        "##Reading a pair of images, and comparing SIFT matches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqquHDKZwENC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading two images for reference\n",
        "img1 = cv2.imread(path + 'fountain3.jpg')\n",
        "img2 = cv2.imread(path + 'fountain5.jpg')\n",
        "\n",
        "# Converting from BGR to RGB format\n",
        "img1 = img1[:,:,::-1]\n",
        "img2 = img2[:,:,::-1]\n",
        "\n",
        "# NOTE: you can adjust appropriate figure size according to the size of your screen\n",
        "fig,ax=plt.subplots(ncols=2,figsize=(14,7)) \n",
        "ax[0].imshow(img1)\n",
        "ax[1].imshow(img2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hsckzPTfHN_",
        "colab_type": "text"
      },
      "source": [
        "**keypoints**  --  spatial locations, or points in an image that define what is interesting or stands out in the image. Keypoints are special because no matter how the image changes (rotation, scaling, translation, or distortion through projective transformation) you should be able to find the same keypoints in the modified image as the original image.\n",
        "\n",
        "example of keypoints in two images where one is rotated - the same points are found in both images and are connected\n",
        "\n",
        "<img src=https://i.stack.imgur.com/L4RUT.png width=\"700\">\n",
        "\n",
        "\n",
        "What makes keypoints different between frameworks is the way you describe these keypoints. These are known as **descriptors**. Each detected keypoint has an associated descriptor that accompanies it. Some frameworks only do a keypoint detection, while other frameworks are simply a description framework. SIFT and SURF are examples of frameworks that both detect and describe the keypoints.\n",
        "\n",
        "SIFT  --   scale-invariant feature transform (feature detection algorithm used in computer vision)\n",
        "\n",
        "SURF  --  speeded up robust features (feature detection and descriptor algorithm in computer vision)\n",
        "\n",
        "Descriptors are primarily concerned with both the scale and the orientation of the keypoint. Descriptor are needed to match keypoints in different images.\n",
        "\n",
        "An example descriptor is keypoint orientation. For an origientation descriptor, the algorithm searches the image pixels near the keypoint for the most dominant orientation of the gradient angles in the patch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlsrGpa54NKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Getting SIFT/SURF features for image matching (this might take a few minutes)\n",
        "\n",
        "# kp1 and kp2 are the keypoints of image 1 and 2 found using the cv2.xfeatures2d.SURF_create() function inside utils.py\n",
        "# desc1 and desc2 are the descriptors of image 1 and 2 found using the cv2.xfeatures2d.SURF_create() function inside utils.py\n",
        "\n",
        "kp1,desc1,kp2,desc2,matches=ut.GetImageMatches(img1,img2)\n",
        "\n",
        "# Aligning two keypoint vectors by \n",
        "# 1) retrieving the corresponding indices of keypoints (img1idx and img2idx)\n",
        "# 2) filtering out the keypoints that were NOT matched and \n",
        "# 3) retreiving the image coordinates of matched keypoints (img1pts and img2pts)\n",
        "\n",
        "img1pts,img2pts,img1idx,img2idx=ut.GetAlignedMatches(kp1,desc1,kp2,desc2,matches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJX2R-n1Qlnh",
        "colab_type": "text"
      },
      "source": [
        "## Fundamental Matrix Computation\n",
        "\n",
        "The **fundamental matrix** relates corresponding points between a pair of uncalibrated images. The matrix transforms homogeneous image points in one image to epipolar lines in the other image.\n",
        "\n",
        "**epipolar line**  --  straight line of intersection of the epipolar plane with the image plane. It is the image in one camera of a ray through the optical centre and image point in the other camera. All epipolar lines intersect at the epipole.\n",
        "\n",
        "A point x in one image generates a line in the other on which its corresponding point x' must lie. We see that the search for correspondences is thus reduced from a region to a line. The corresponding point for x must lie on the epipolar line.\n",
        "\n",
        "<img src=http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT10/img17.gif width=\"500\">\n",
        "\n",
        "See the following link for additiona linformation on epipolar geometry and triangulation\n",
        "\n",
        "http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/OWENS/LECT10/node3.html\n",
        "\n",
        "\n",
        "### Eight Point Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqqUKJdh-IOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select the first 8 matched keypoints for an eight point algorithm\n",
        "img1pts_, img2pts_ = img1pts[:8], img2pts[:8] # underscores used to avoid conflict between python variables\n",
        "\n",
        "# determine the fundamental matrix using opencv with the 8 point algorithm (cv2.FM_8POINT)\n",
        "Fgt, mask = cv2.findFundamentalMat(img1pts_,img2pts_,method=cv2.FM_8POINT)\n",
        "\n",
        "# in house determination of fundamental matrix inside SfM.py file\n",
        "F = sfmnp.EstimateFundamentalMatrix(img1pts_,img2pts_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vujltKvurUCF",
        "colab_type": "text"
      },
      "source": [
        "The in house methods are included here in the additional files, SfM.py and utils.py, so you can see the details of the calculations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQUEyMu9-OZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check that the in house detmerination and the opencv determination of the fundamental matrix are similar\n",
        "print(F,'\\n\\n',Fgt)\n",
        "\n",
        "# gives an error if the two variables F and Fgt are not similar enough (rtol - relative tolerance, atol - absolute tolerance)\n",
        "np.testing.assert_allclose(F, Fgt, rtol=1e-3,atol=1e-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDeu7p7NQvy6",
        "colab_type": "text"
      },
      "source": [
        "### Normalized 8 Point Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXNUez-O-Y07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# in house determination of normalized fundamental matrix\n",
        "F_normalized = sfmnp.EstimateFundamentalMatrixNormalized(img1pts_,img2pts_)\n",
        "print(F_normalized)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZZfPtbnQ250",
        "colab_type": "text"
      },
      "source": [
        "### with RANSAC (Random Sample Consensus)\n",
        "\n",
        "The RANSAC algorithm allows for more than 8 keypoints to be used to determine the fundamental matrix\n",
        "\n",
        "Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHBhdVyH-aI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the fundamental matrix using the RANSAC algorithm\n",
        "Fgt, maskgt = cv2.findFundamentalMat(img1pts,img2pts,method=cv2.FM_RANSAC)\n",
        "\n",
        "# The mask is an output array, every element of which is set to 0 for outliers and to 1 for the other points. \n",
        "# The array is computed only in the RANSAC and LMedS methods.\n",
        "\n",
        "# convert to boolean (true/false)\n",
        "maskgt = maskgt.astype(bool).flatten()\n",
        "\n",
        "# in house estimation of the fundamental matrix using the RANSAC method\n",
        "F, mask = sfmnp.EstimateFundamentalMatrixRANSAC(img1pts,img2pts,.1,iters=20000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0WG6dgx-aM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check that the two methods are similar\n",
        "print(F,'\\n\\n',Fgt)\n",
        "np.testing.assert_allclose(F, Fgt, rtol=1e-3,atol=1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vckyyTirQ_Cz",
        "colab_type": "text"
      },
      "source": [
        "## Epipolar Lines Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGS1wASx-aSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# determine the epipolar lines\n",
        "lines2=sfmnp.ComputeEpiline(img1pts[maskgt],1,Fgt)\n",
        "lines1=sfmnp.ComputeEpiline(img2pts[maskgt],2,Fgt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJKud-pORKk8",
        "colab_type": "text"
      },
      "source": [
        "## Visualizations I: Epipolar Geometry\n",
        "### Epipolar Lines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im6EUv9z5Au5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drawing epipolar lines and keypoints with cv2.lines() and cv2.circle()\n",
        "\n",
        "num_to_draw = 25  # number of lines and points to draw\n",
        "\n",
        "# returns new images with the lines and points drawn on\n",
        "tup = ut.drawlines(img2,img1,lines2,img2pts[maskgt],img1pts[maskgt],drawOnly=num_to_draw,\n",
        "                   linesize=10,circlesize=35)\n",
        "\n",
        "# making the images with the lines and points viewable with imshow() in matplotlib\n",
        "epilines2 = np.concatenate(tup[::-1],axis=1) #reversing the order of left and right images\n",
        "\n",
        "# show the first set of points and lines\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.imshow(epilines2)\n",
        "\n",
        "\n",
        "# repeat for the second image\n",
        "\n",
        "tup = ut.drawlines(img1,img2,lines1,img1pts[maskgt],img2pts[maskgt],drawOnly=num_to_draw,\n",
        "                   linesize=10,circlesize=35)\n",
        "epilines1 = np.concatenate(tup,axis=1) \n",
        "\n",
        "plt.figure(figsize=(9,4))\n",
        "plt.imshow(epilines1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSCf_zEORT14",
        "colab_type": "text"
      },
      "source": [
        "## Pose Estimation\n",
        "\n",
        "Geometric camera calibration, also referred to as camera resectioning, estimates the parameters of a lens and image sensor of an image or video camera. You can use these parameters to correct for lens distortion, measure the size of an object, or determine the location of the camera in the scene. These tasks are used in applications such as machine vision to detect and measure objects. \n",
        "\n",
        "<img src=https://www.mathworks.com/help/vision/ug/calibration_applications.png width=\"500\">\n",
        "\n",
        "The camera calibration matrix is a 3x3 or 3x4 matrix containing 5 parameters encompassing focal length, image sensor format, and the principal point\n",
        "\n",
        "focal length - distance of convergence of light through a lens\n",
        "\n",
        "image sensor fomat - shape and size of the image sensor\n",
        "\n",
        "principal point - A point R at the intersection of the optical axis and the image plane (ideally the image center)\n",
        "\n",
        "<img src=https://wikimedia.org/api/rest_v1/media/math/render/svg/e712d4b11c471fba8d70b4ae2f8790328040c6fd width=\"300\">\n",
        "\n",
        "The parameters $ \\alpha _x = f \\cdot m _x $ and $ \\alpha _y = f \\cdot m _y $ represent focal length in terms of pixels, where $ m_ x $ and $ m_ y $ are the scale factors relating pixels to distance and $ f $ is the focal length. $ \\gamma $ represents the skew coefficient between the x and the y axis, and is often 0. $ u _0 $ and $ v _0 $ is the principal point\n",
        "\n",
        "Given a point in one image the  **essential matrix** tells us which epipolar line to search along in the second view"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F5JgUgV5bDY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# known camera calibration matrix\n",
        "K = np.array([[2759.48,0,1520.69],[0,2764.16,1006.81],[0,0,1]])\n",
        "\n",
        "# determine the essential matrix, E\n",
        "E = K.T.dot(Fgt.dot(K))\n",
        "\n",
        "# determining the camera pose possible locations.\n",
        "# R is the 3x3 rotation matrix to convert from world to camera coordinate system\n",
        "# t is the 3x1 translation vector from the camer'as origin to the world's origin\n",
        "R1,R2,t = sfmnp.ExtractCameraPoses(E)\n",
        "t = t[:,np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGBXwITMRZE1",
        "colab_type": "text"
      },
      "source": [
        "## Visualizations II: Camera Poses\n",
        "\n",
        "There are several possible solutions following the reconstruction from the essential matrix.\n",
        "\n",
        "<img src=https://lh3.googleusercontent.com/-CN6u5U6R4u0/WECN2oY-HJI/AAAAAAAAAz8/Fhzukgas4u0MuswsKDXz564GLHTIPTnbACLcB/s1600/geometry.jpg width=\"400\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTWvupCg5aSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for R_ in [R1,R2]: \n",
        "    for t_ in [t,-t]:\n",
        "        \n",
        "        fig = plt.figure(figsize=(9,6))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        ax.set_xlabel('X')\n",
        "        ax.set_ylabel('Y')\n",
        "        ax.set_zlabel('Z')\n",
        "\n",
        "        ut.PlotCamera(np.eye(3,3),np.zeros((3,)),ax)\n",
        "        ut.PlotCamera(R_,t_[:,0],ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9__AC0jRiCA",
        "colab_type": "text"
      },
      "source": [
        "# 3D Scene Estimations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwtLUUVFRlPo",
        "colab_type": "text"
      },
      "source": [
        "## Triangulation: Direct Linear Transform (DLT) Method\n",
        "\n",
        "The most commonly used camera calibration method is perhaps the DLT (direct linear\n",
        "transformation) method originally reported by Abdel-Aziz and Karara [1]. The DLT method uses a\n",
        "set of control points whose object space/plane coordinates are already known. The control points\n",
        "are normally fixed to a rigid frame, known as the calibration frame. The problem is essentially to\n",
        "calculate the mapping between the 2D image space coordinates (xi\n",
        ") and the 3D object space\n",
        "coordinates (Xi\n",
        "). For this 3D 2D correspondence the mapping should take the form of a 3x4\n",
        "projection matrix (P) such that xi\n",
        " = PXi\n",
        " for all i. \n",
        " \n",
        "[1] Abdel-Aziz, Karara., Direct linear transformation into object space coordinates in closerange photogrametry. In Proc. Symp. Close-Range Photogrametry, 1971: p. 1-18. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj2-ooMk5kbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GetTriangulatedPts(img1pts,img2pts,K,R,t,triangulateFunc): \n",
        "    img1ptsHom = cv2.convertPointsToHomogeneous(img1pts)[:,0,:]\n",
        "    img2ptsHom = cv2.convertPointsToHomogeneous(img2pts)[:,0,:]\n",
        "\n",
        "    img1ptsNorm = (np.linalg.inv(K).dot(img1ptsHom.T)).T\n",
        "    img2ptsNorm = (np.linalg.inv(K).dot(img2ptsHom.T)).T\n",
        "\n",
        "    img1ptsNorm = cv2.convertPointsFromHomogeneous(img1ptsNorm)[:,0,:]\n",
        "    img2ptsNorm = cv2.convertPointsFromHomogeneous(img2ptsNorm)[:,0,:]\n",
        "    \n",
        "    pts4d = triangulateFunc(np.eye(3,4),np.hstack((R,t)),img1ptsNorm.T,img2ptsNorm.T)\n",
        "    pts3d = cv2.convertPointsFromHomogeneous(pts4d.T)[:,0,:]\n",
        "    \n",
        "    return pts3d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMiWqcvA5kjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pts3dgt = GetTriangulatedPts(img1pts[maskgt],img2pts[maskgt],K,R2,t,cv2.triangulatePoints)\n",
        "pts3d = GetTriangulatedPts(img1pts[maskgt],img2pts[maskgt],K,R2,t,sfmnp.Triangulate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8z7_yzQ5koB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check the in house vs cv2 methods\n",
        "print(pts3dgt[:5],'\\n\\n',pts3d[:5])\n",
        "np.testing.assert_allclose(pts3d,pts3dgt,rtol=1e-2,atol=1e-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpABRY2k5xIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# placing the data into a convenient form\n",
        "configSet = [None,None,None,None]\n",
        "configSet[0] = (R1,t,GetTriangulatedPts(img1pts[maskgt],img2pts[maskgt],K,R1,t,cv2.triangulatePoints))\n",
        "configSet[1] = (R1,-t,GetTriangulatedPts(img1pts[maskgt],img2pts[maskgt],K,R1,-t,cv2.triangulatePoints))\n",
        "configSet[2] = (R2,t,GetTriangulatedPts(img1pts[maskgt],img2pts[maskgt],K,R2,t,cv2.triangulatePoints))\n",
        "configSet[3] = (R2,-t,GetTriangulatedPts(img1pts[maskgt],img2pts[maskgt],K,R2,-t,cv2.triangulatePoints))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndm_bRC6Rt8A",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing the triangulated points of configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgDpfSVq5xK2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for cs in configSet: \n",
        "    fig = plt.figure(figsize=(9,6))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_zlabel('Z')\n",
        "\n",
        "    ut.PlotCamera(np.eye(3,3),np.zeros((3,)),ax,scale=5,depth=5)\n",
        "    ut.PlotCamera(cs[0],cs[1][:,0],ax,scale=5,depth=5)\n",
        "\n",
        "    pts3d = cs[-1]\n",
        "    ax.scatter3D(pts3dgt[:,0],pts3dgt[:,1],pts3dgt[:,2])\n",
        "\n",
        "    ax.set_xlim(left=-50,right=50)\n",
        "    ax.set_ylim(bottom=-50,top=50)\n",
        "    ax.set_zlim(bottom=-20,top=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ejcFdTeR6cr",
        "colab_type": "text"
      },
      "source": [
        "## Camera Pose Disambiguation\n",
        "\n",
        "There are multiple camera poses that solve the triangulation methods. Here we select the correct camera pose by counting the number of points in front of each camera for each pose. The pose with the most points is correct"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQkghT8x5xPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# determine the correct camera pose transformation matrix elements R and t\n",
        "_,Rgt,tgt,mask2=cv2.recoverPose(E,img1pts[maskgt],img2pts[maskgt],K)\n",
        "R,t,count = sfmnp.DisambiguateCameraPose(configSet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xnrFwRj6OXa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# comparing the in house and opencv methods\n",
        "print(R,'\\n\\n',Rgt,'\\n\\n',t,'\\n\\n',tgt)\n",
        "np.testing.assert_allclose(R,Rgt,rtol=1e-7,atol=1e-4)\n",
        "np.testing.assert_allclose(t,tgt,rtol=1e-7,atol=1e-4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "222-GYX7SCIo",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing Point Cloud Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ur9o0Gy6OZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build the construction into a point cloud (x,y,z) and save it\n",
        "pts3d = GetTriangulatedPts(img1pts[maskgt],img2pts[maskgt],K,Rgt,tgt,cv2.triangulatePoints)\n",
        "ut.pts2ply(pts3d,'fountain_2view.ply')\n",
        "\n",
        "### this section could use some work. Currently the RGB color of the point is not taken. However,\n",
        "### point clouds often have associated RGB with each point. One recommendation would be to take one\n",
        "### or an average of the RGB values of the pixel that is being triangulated.  MeshLab is a convenient\n",
        "### program to view RGB associated point clouds. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLd0Ju6XV95y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the point clouid with open3d and plot using plotly\n",
        "pcd = o3d.io.read_point_cloud(\"fountain_2view.ply\")\n",
        "\n",
        "print(pcd)\n",
        "\n",
        "points = np.asarray(pcd.points)\n",
        "\n",
        "# remove poitns that are clearly outside the image location\n",
        "keep_points = points\n",
        "num_deleted = 0\n",
        "for i in range(len(points)):\n",
        "  if np.abs(points[i][0]) > 3:\n",
        "    keep_points = np.delete(keep_points, i-num_deleted, axis=0)\n",
        "    num_deleted = num_deleted+1\n",
        "\n",
        "x = keep_points[:,0]\n",
        "y = keep_points[:,1]\n",
        "z = keep_points[:,2]\n",
        "\n",
        "\n",
        "# allow for colab to use plotly for interactive plotting \n",
        "configure_plotly_browser_state()\n",
        "\n",
        "import plotly\n",
        "from plotly.graph_objs import Scatter3d, Layout\n",
        "\n",
        "plotly.offline.init_notebook_mode(connected=True)\n",
        "\n",
        "plotly.offline.iplot({\n",
        "    \"data\": [Scatter3d(x=x, y=y, z=z, mode='markers', marker=dict(size=4))],\n",
        "    \"layout\": Layout(title=\"fountain 3D reconstruction from 2 images\")\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxiWD1_KSY7N",
        "colab_type": "text"
      },
      "source": [
        "# Reprojection Error: Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-jpS5pfSbO4",
        "colab_type": "text"
      },
      "source": [
        "## Computation\n",
        "\n",
        "It is convenient to calculate the error in the image points and the reprojected points\n",
        "\n",
        "Here we reproduce the points as they are in the image from the points calculated in the world coordinate system from the rotation matrix, the translation vector, and the camera calibration matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTx59YlhC2Jz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "ComputeReprojections(X,R,t,K)\n",
        "\n",
        "    inputs\n",
        "    X: (n,3) 3D triangulated points in world coordinate system\n",
        "    R: (3,3) Rotation Matrix to convert from world to camera coordinate system\n",
        "    t: (3,1) Translation vector (from camera's origin to world's origin)\n",
        "    K: (3,3) Camera calibration matrix\n",
        "    \n",
        "    output: (n,2) Projected points into image plane\n",
        "\"\"\"\n",
        "img1ptsReproj = sfmnp.ComputeReprojections(pts3d,np.eye(3,3),np.zeros((3,1)),K)\n",
        "img2ptsReproj = sfmnp.ComputeReprojections(pts3d,Rgt,tgt,K)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06wzaYJ1C3wY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate the error in the image points compared to the reprojected image points\n",
        "err2 = sfmnp.ComputeReprojectionError(img2pts[maskgt], img2ptsReproj)\n",
        "err1 = sfmnp.ComputeReprojectionError(img1pts[maskgt], img1ptsReproj)\n",
        "\n",
        "err1, err2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1vU8KWaSjID",
        "colab_type": "text"
      },
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmgzW74xC54J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# draw a true point with an x and a reprojected point as a dot (they are nearly on top of one another)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(9,5))\n",
        "ut.DrawCorrespondences(img1,img1pts[maskgt],img1ptsReproj,ax)\n",
        "\n",
        "fig,ax=plt.subplots(figsize=(9,5))\n",
        "ut.DrawCorrespondences(img2,img2pts[maskgt],img2ptsReproj,ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP80UemKSqIV",
        "colab_type": "text"
      },
      "source": [
        "# Perspective-n-Point Algorithm: New Camera Registration\n",
        "\n",
        "## Reading third image and 2D-3D Matching using SIFT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f7dvQwaC56e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read in a 3rd image of the fountain\n",
        "img3 = cv2.imread(path + 'fountain7.jpg')\n",
        "img3 = img3[:,:,::-1]\n",
        "\n",
        "# determine the keypoints and descriptors for the 3rd image\n",
        "surfer=cv2.xfeatures2d.SURF_create()\n",
        "kp3, desc3 = surfer.detectAndCompute(img3,None)\n",
        "\n",
        "# show the 3rd image\n",
        "plt.figure()\n",
        "plt.imshow(img3)\n",
        "\n",
        "# find the points that match between all three image keypoints\n",
        "img3pts,pts3dpts = ut.Find2D3DMatches(desc1,img1idx,desc2,img2idx,desc3,kp3,maskgt,pts3d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy02Pc8BS6Gz",
        "colab_type": "text"
      },
      "source": [
        "## Perspective-n-Point (PnP) Algorithm\n",
        "\n",
        "Perspective-n-Point is the problem of estimating the pose of a calibrated camera given a set of n 3D points in the world and their corresponding 2D projections in the image. The camera pose consists of 6 degrees-of-freedom (DOF) which are made up of the rotation (roll, pitch, and yaw) and 3D translation of the camera with respect to the world.\n",
        "\n",
        "### RANSAC solution\n",
        "\n",
        "The RANSAC algorithm allows for more than 8 keypoints to be used to determine the fundamental matrix\n",
        "\n",
        "Random sample consensus (RANSAC) is an iterative method to estimate parameters of a mathematical model from a set of observed data that contains outliers, when outliers are to be accorded no influence on the values of the estimates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBq5aTmzDT3M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# determine the new rotation matrix, translation vector, and mask\n",
        "retval,Rvec,tnew,mask3gt = cv2.solvePnPRansac(pts3dpts[:,np.newaxis],img3pts[:,np.newaxis],\n",
        "                                            K,None,confidence=.99,flags=cv2.SOLVEPNP_DLS)\n",
        "Rnew,_=cv2.Rodrigues(Rvec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUOolt_lTQES",
        "colab_type": "text"
      },
      "source": [
        "## Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_AhEj0_DT1U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# visualize the camera positions\n",
        "fig = plt.figure(figsize=(9,6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('Z')\n",
        "\n",
        "ut.PlotCamera(np.eye(3,3),np.zeros((3,)),ax)\n",
        "ut.PlotCamera(R,t[:,0],ax)\n",
        "ut.PlotCamera(Rnew,tnew[:,0],ax,faceColor='red')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5NM_s2-TmPA",
        "colab_type": "text"
      },
      "source": [
        "# Re-triangulation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5g80AwzDeTs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kpNew, descNew = kp3, desc3\n",
        "\n",
        "kpOld,descOld = kp1,desc1\n",
        "ROld, tOld = np.eye(3), np.zeros((3,1))\n",
        "\n",
        "accPts = []\n",
        "for (ROld, tOld, kpOld, descOld) in [(np.eye(3),np.zeros((3,1)), kp1,desc1),(Rgt,tgt,kp2,desc2)]: \n",
        "    \n",
        "    #Matching between old view and newly registered view.. \n",
        "    print('[Info]: Feature Matching..')\n",
        "    matcher = cv2.BFMatcher(crossCheck=True)\n",
        "    matches = matcher.match(descOld, desc3)\n",
        "    matches = sorted(matches, key = lambda x:x.distance)\n",
        "    imgOldPts, imgNewPts, _, _ = ut.GetAlignedMatches(kpOld,descOld,kpNew,\n",
        "                                                          descNew,matches)\n",
        "    \n",
        "    #Pruning the matches using fundamental matrix..\n",
        "    print('[Info]: Pruning the Matches..')\n",
        "    F,mask=cv2.findFundamentalMat(imgOldPts,imgNewPts,method=cv2.FM_RANSAC)\n",
        "    mask = mask.flatten().astype(bool)\n",
        "    imgOldPts=imgOldPts[mask]\n",
        "    imgNewPts=imgNewPts[mask]\n",
        "    \n",
        "    #Triangulating new points\n",
        "    print('[Info]: Triangulating..')\n",
        "    newPts = sfmnp.GetTriangulatedPts(imgOldPts,imgNewPts, K, Rnew,tnew,cv2.triangulatePoints,ROld,tOld)\n",
        "    \n",
        "    #Adding newly triangulated points to the collection\n",
        "    accPts.append(newPts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXcoqNNoTuAS",
        "colab_type": "text"
      },
      "source": [
        "# Final Result "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7Cya1hrXXF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Adding the original 2-view-sfm point cloud and saving the whole collection\n",
        "accPts.append(pts3d)\n",
        "ut.pts2ply(np.concatenate((accPts),axis=0),'fountain_nview.ply')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvidftq1DeWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the point cloud with open3d and display with plotly\n",
        "pcd = o3d.io.read_point_cloud(\"fountain_nview.ply\")\n",
        "\n",
        "print(pcd)\n",
        "\n",
        "points = np.asarray(pcd.points)\n",
        "\n",
        "# remove poitns that are clearly outside the image location\n",
        "keep_points = points\n",
        "num_deleted = 0\n",
        "for i in range(len(points)):\n",
        "  if np.abs(points[i][0]) > 3:\n",
        "    keep_points = np.delete(keep_points, i-num_deleted, axis=0)\n",
        "    num_deleted = num_deleted+1\n",
        "\n",
        "x = keep_points[:,0]\n",
        "y = keep_points[:,1]\n",
        "z = keep_points[:,2]\n",
        "\n",
        "# allows for colab to use plotly for interactive plotting \n",
        "configure_plotly_browser_state()\n",
        "\n",
        "import plotly\n",
        "from plotly.graph_objs import Scatter3d, Layout\n",
        "\n",
        "plotly.offline.init_notebook_mode(connected=True)\n",
        "\n",
        "plotly.offline.iplot({\n",
        "    \"data\": [Scatter3d(x=x, y=y, z=z, mode='markers', marker=dict(size=3))],\n",
        "    \"layout\": Layout(title=\"fountain 3D reconstruction from 3 images\")\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJDL6MYAMUGV",
        "colab_type": "text"
      },
      "source": [
        "# Remaining Challenges\n",
        "1. Create a point cloud that uses the RGB pixel values in addition to the traingulated (x,y,z) points and plot it as an interactive plot\n",
        "2. Implement non local histogram normalization\n",
        "3. Use more than 3 images to create the point cloud\n",
        "4. Use your own images to build a 3D point cloud"
      ]
    }
  ]
}