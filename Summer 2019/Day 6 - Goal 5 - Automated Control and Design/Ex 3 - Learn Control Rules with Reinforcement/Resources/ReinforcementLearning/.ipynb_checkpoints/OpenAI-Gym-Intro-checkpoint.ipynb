{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning with OpenAI gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "This notebook requires the following packages to be installed:\n",
    "\n",
    "1. tensorflow\n",
    "2. tflearn\n",
    "3. gym\n",
    "4. numpy\n",
    "\n",
    "These can be installed via Anaconda (**`conda install <package>`**), or via PIP: (**`pip install <package>`**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What we'll be doing:\n",
    "\n",
    "Teaching a Neural Network to balance a CartPole - which uses 2 inputs (left and right).\n",
    "\n",
    "![cartpole](https://i.imgur.com/di0IANk.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## History and Rationale\n",
    "\n",
    "* Game programmers used to use heuristic if-then-else type decisions to make educated guesses. This used to be the norm for a very long time - however, it's major flaw is that game developers can only predict and account for a limited number of scenarios and edge cases.\n",
    "\n",
    "* Game developers then tried to mimic how humans would play a game, and modeled human intelligence in a game bot.\n",
    "\n",
    "* DeepMind generalized modeling intelligence to solve any Atari game using deep learning NNs without previously known information about the game such as input, goals, etc. Much of this is not open-source, and is a trade-secret of Google.\n",
    "\n",
    "* To avoid concentrating the incredible power of AI in the hands of a few, Elon Musk founded OpenAI. It seeks to democratize AI by making it accessible to all.\n",
    "\n",
    "* OpenAI Gym provides a simple interface for interacting with and managing any arbitrary dynamic environment. When integrated with other packages that add flash functionality (such as `universe`), one can access: Atari games, Minecraft, Grand Theft Auto, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Reinforcement Learning?\n",
    "\n",
    "This technique observes the game’s previous state and reward (such as the pixels seen on the screen or the game score). It then comes up with an action to perform on the environment.\n",
    "\n",
    "![reinforce](imgs/reinforcement.jpeg)\n",
    "\n",
    "The goal is to make its next observation better (in our case — to maximize the game score). This action is chosen and performed by an agent (Game Bot) with the intention of maximizing the score. It’s then applied on the environment. The environment records the resulting state and reward based on whether the action was beneficial or not (did it win the game?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Files available:\n",
    "\n",
    "1. **Basic setup**: `01-cartpole-random-i.py`\n",
    "2. **Introduction to using OpenAI gym**: `02-cartpole-random-ii.py`\n",
    "3. **Using reinforcement to achieve a goal (in this case, balancing a pole)**: `03-cartpole-non-random.py`\n",
    "\n",
    "Since this involves interaction, we will run it from the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activity: 01-cartpole-random-i.py\n",
    "\n",
    "Creating and running an environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activity: 02-cartpole-random-ii.py\n",
    "\n",
    "Interacting with that environment. Note that every environment has two spaces: an `action_space` which determines where and how the actions are performed, and an `observation_space` which tells you about the game \"world\".\n",
    "\n",
    "For `CartPole-v0`:\n",
    "\n",
    "```\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "print(env.action_space)\n",
    "#> Discrete(2)\n",
    "print(env.observation_space)\n",
    "#> Box(4,)\n",
    "```\n",
    "\n",
    "The Discrete space allows a fixed range of non-negative numbers, so in this case valid actions are either 0 or 1. While the Box space represents an n-dimensional box, so valid observations will be an array of 4 numbers. We can also check the Box's bounds:\n",
    "\n",
    "```\n",
    "print(env.observation_space.high)\n",
    "#> array([ 2.4       ,         inf,  0.20943951,         inf])\n",
    "print(env.observation_space.low)\n",
    "#> array([-2.4       ,        -inf, -0.20943951,        -inf])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Environment Observations\n",
    "\n",
    "If we ever want to do better than take random actions at each step, it'd probably be good to actually know what our actions are doing to the environment.\n",
    "\n",
    "The environment's step function returns exactly what we need. In fact, step returns four values. These are:\n",
    "\n",
    "1. **observation (object)**: an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "\n",
    "2. **reward (float)**: amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "\n",
    "3. **done (boolean)**: whether it's time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "\n",
    "4. **info (dict)**: diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment's last state change). However, official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "This is just an implementation of the classic \"agent-environment loop\". Each timestep, the agent chooses an action, and the environment returns an observation and a reward.\n",
    "\n",
    "![loop](imgs/loop.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activity: 03-cartpole-non-random.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extra: Try a different game!\n",
    "To examine which games can be used, run:\n",
    "\n",
    "```\n",
    "    from gym import envs\n",
    "    print(envs.registry.all())\n",
    "```\n",
    "\n",
    "**Note that some games do not have 2 inputs, so you will have to change how your inputs are sampled!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "1. https://medium.freecodecamp.org/how-to-build-an-ai-game-bot-using-openai-gym-and-universe-f2eb9bfbb40a\n",
    "2. https://gym.openai.com/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
