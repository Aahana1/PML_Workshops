{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning with OpenAI gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Requirements\n",
    "\n",
    "This notebook requires the following packages to be installed:\n",
    "\n",
    "1. tensorflow\n",
    "2. tflearn\n",
    "3. gym\n",
    "4. numpy\n",
    "\n",
    "These can be installed via Anaconda (**`conda install <package>`**), or via PIP: (**`pip install <package>`**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What we'll be doing:\n",
    "\n",
    "Teaching a Neural Network to balance a CartPole - which uses 2 inputs (left and right).\n",
    "\n",
    "![cartpole](https://i.imgur.com/di0IANk.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## History and Rationale\n",
    "\n",
    "* Game programmers used to use heuristic if-then-else type decisions to make educated guesses. This used to be the norm for a very long time - however, it's major flaw is that game developers can only predict and account for a limited number of scenarios and edge cases.\n",
    "\n",
    "* Game developers then tried to mimic how humans would play a game, and modeled human intelligence in a game bot.\n",
    "\n",
    "* DeepMind generalized modeling intelligence to solve any Atari game using deep learning NNs without previously known information about the game such as input, goals, etc. Much of this is not open-source, and is a trade-secret of Google.\n",
    "\n",
    "* To avoid concentrating the incredible power of AI in the hands of a few, Elon Musk founded OpenAI. It seeks to democratize AI by making it accessible to all.\n",
    "\n",
    "* OpenAI Gym provides a simple interface for interacting with and managing any arbitrary dynamic environment. When integrated with other packages that add flash functionality (such as `universe`), one can access: Atari games, Minecraft, Grand Theft Auto, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is Reinforcement Learning?\n",
    "\n",
    "This technique observes the game’s previous state and reward (such as the pixels seen on the screen or the game score). It then comes up with an action to perform on the environment.\n",
    "\n",
    "![reinforce](imgs/reinforcement.jpeg)\n",
    "\n",
    "The goal is to make its next observation better (in our case — to maximize the game score). This action is chosen and performed by an agent (Game Bot) with the intention of maximizing the score. It’s then applied on the environment. The environment records the resulting state and reward based on whether the action was beneficial or not (did it win the game?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Files available:\n",
    "\n",
    "1. **Basic setup**: `01-cartpole-random-i.py`\n",
    "2. **Introduction to using OpenAI gym**: `02-cartpole-random-ii.py`\n",
    "3. **Using reinforcement to achieve a goal (in this case, balancing a pole)**: `03-cartpole-non-random.py`\n",
    "\n",
    "Since this involves interaction, we will run it from the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activity: 01-cartpole-random-i.py\n",
    "\n",
    "Creating and running an environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activity: 02-cartpole-random-ii.py\n",
    "\n",
    "Interacting with that environment. Note that every environment has two spaces: an `action_space` which determines where and how the actions are performed, and an `observation_space` which tells you about the game \"world\".\n",
    "\n",
    "For `CartPole-v0`:\n",
    "\n",
    "```\n",
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "print(env.action_space)\n",
    "#> Discrete(2)\n",
    "print(env.observation_space)\n",
    "#> Box(4,)\n",
    "```\n",
    "\n",
    "The Discrete space allows a fixed range of non-negative numbers, so in this case valid actions are either 0 or 1. While the Box space represents an n-dimensional box, so valid observations will be an array of 4 numbers. We can also check the Box's bounds:\n",
    "\n",
    "```\n",
    "print(env.observation_space.high)\n",
    "#> array([ 2.4       ,         inf,  0.20943951,         inf])\n",
    "print(env.observation_space.low)\n",
    "#> array([-2.4       ,        -inf, -0.20943951,        -inf])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Environment Observations\n",
    "\n",
    "If we ever want to do better than take random actions at each step, it'd probably be good to actually know what our actions are doing to the environment.\n",
    "\n",
    "The environment's step function returns exactly what we need. In fact, step returns four values. These are:\n",
    "\n",
    "1. **observation (object)**: an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.\n",
    "\n",
    "2. **reward (float)**: amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.\n",
    "\n",
    "3. **done (boolean)**: whether it's time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)\n",
    "\n",
    "4. **info (dict)**: diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment's last state change). However, official evaluations of your agent are not allowed to use this for learning.\n",
    "\n",
    "This is just an implementation of the classic \"agent-environment loop\". Each timestep, the agent chooses an action, and the environment returns an observation and a reward.\n",
    "\n",
    "![loop](imgs/loop.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Activity: 03-cartpole-non-random.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extra: Try a different game!\n",
    "To examine which games can be used, run:\n",
    "\n",
    "```\n",
    "    from gym import envs\n",
    "    print(envs.registry.all())\n",
    "```\n",
    "\n",
    "**Note that some games do not have 2 inputs, so you will have to change how your inputs are sampled!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "1. https://medium.freecodecamp.org/how-to-build-an-ai-game-bot-using-openai-gym-and-universe-f2eb9bfbb40a\n",
    "2. https://gym.openai.com/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-24 17:36:24,403] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out of bounds after 12 steps\n",
      "out of bounds after 16 steps\n",
      "out of bounds after 19 steps\n",
      "out of bounds after 12 steps\n",
      "out of bounds after 28 steps\n",
      "out of bounds after 11 steps\n",
      "out of bounds after 19 steps\n",
      "out of bounds after 16 steps\n",
      "out of bounds after 12 steps\n",
      "out of bounds after 18 steps\n",
      "out of bounds after 42 steps\n",
      "out of bounds after 15 steps\n",
      "out of bounds after 16 steps\n",
      "out of bounds after 16 steps\n",
      "out of bounds after 30 steps\n",
      "out of bounds after 13 steps\n",
      "out of bounds after 46 steps\n",
      "out of bounds after 16 steps\n",
      "out of bounds after 25 steps\n",
      "out of bounds after 13 steps\n",
      "out of bounds after 15 steps\n",
      "out of bounds after 13 steps\n",
      "out of bounds after 18 steps\n",
      "out of bounds after 23 steps\n",
      "out of bounds after 29 steps\n",
      "out of bounds after 15 steps\n",
      "out of bounds after 38 steps\n",
      "out of bounds after 24 steps\n",
      "out of bounds after 12 steps\n",
      "out of bounds after 25 steps\n",
      "out of bounds after 21 steps\n",
      "out of bounds after 23 steps\n",
      "out of bounds after 42 steps\n",
      "out of bounds after 19 steps\n",
      "out of bounds after 12 steps\n",
      "out of bounds after 35 steps\n",
      "out of bounds after 24 steps\n",
      "out of bounds after 27 steps\n",
      "out of bounds after 28 steps\n",
      "out of bounds after 62 steps\n",
      "out of bounds after 24 steps\n",
      "out of bounds after 54 steps\n",
      "out of bounds after 37 steps\n",
      "out of bounds after 17 steps\n",
      "out of bounds after 11 steps\n"
     ]
    }
   ],
   "source": [
    "run \"01-cartpole-random-i.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-09-24 16:50:20,678] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 finished after 15 timesteps.\n",
      "Trial 2 finished after 10 timesteps.\n",
      "Trial 3 finished after 33 timesteps.\n",
      "Trial 4 finished after 14 timesteps.\n",
      "Trial 5 finished after 17 timesteps.\n",
      "Trial 6 finished after 22 timesteps.\n",
      "Trial 7 finished after 17 timesteps.\n",
      "Trial 8 finished after 20 timesteps.\n",
      "Trial 9 finished after 30 timesteps.\n",
      "Trial 10 finished after 19 timesteps.\n",
      "Trial 11 finished after 55 timesteps.\n",
      "Trial 12 finished after 63 timesteps.\n",
      "Trial 13 finished after 11 timesteps.\n",
      "Trial 14 finished after 11 timesteps.\n",
      "Trial 15 finished after 12 timesteps.\n",
      "Trial 16 finished after 8 timesteps.\n",
      "Trial 17 finished after 10 timesteps.\n",
      "Trial 18 finished after 35 timesteps.\n",
      "Trial 19 finished after 14 timesteps.\n",
      "Trial 20 finished after 18 timesteps.\n",
      "Trial 21 finished after 18 timesteps.\n",
      "Trial 22 finished after 16 timesteps.\n",
      "Trial 23 finished after 18 timesteps.\n",
      "Trial 24 finished after 9 timesteps.\n",
      "Trial 25 finished after 36 timesteps.\n",
      "Trial 26 finished after 13 timesteps.\n",
      "Trial 27 finished after 36 timesteps.\n",
      "Trial 28 finished after 13 timesteps.\n",
      "Trial 29 finished after 17 timesteps.\n",
      "Trial 30 finished after 25 timesteps.\n",
      "Trial 31 finished after 16 timesteps.\n",
      "Trial 32 finished after 10 timesteps.\n",
      "Trial 33 finished after 20 timesteps.\n",
      "Trial 34 finished after 14 timesteps.\n",
      "Trial 35 finished after 16 timesteps.\n",
      "Trial 36 finished after 12 timesteps.\n",
      "Trial 37 finished after 31 timesteps.\n",
      "Trial 38 finished after 13 timesteps.\n",
      "Trial 39 finished after 25 timesteps.\n",
      "Trial 40 finished after 13 timesteps.\n",
      "Trial 41 finished after 20 timesteps.\n",
      "Trial 42 finished after 16 timesteps.\n",
      "Trial 43 finished after 26 timesteps.\n",
      "Trial 44 finished after 19 timesteps.\n",
      "Trial 45 finished after 27 timesteps.\n",
      "Trial 46 finished after 10 timesteps.\n",
      "Trial 47 finished after 11 timesteps.\n",
      "Trial 48 finished after 15 timesteps.\n",
      "Trial 49 finished after 22 timesteps.\n",
      "Trial 50 finished after 21 timesteps.\n"
     ]
    }
   ],
   "source": [
    "run \"02-cartpole-random-ii.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1784  | total loss: \u001b[1m\u001b[32m0.66435\u001b[0m\u001b[0m | time: 16.533s\n",
      "| Adam | epoch: 005 | loss: 0.66435 - acc: 0.5938 -- iter: 22784/22827\n",
      "Training Step: 1785  | total loss: \u001b[1m\u001b[32m0.66884\u001b[0m\u001b[0m | time: 16.581s\n",
      "| Adam | epoch: 005 | loss: 0.66884 - acc: 0.5844 -- iter: 22827/22827\n",
      "--\n",
      "Trial 1 finished after 200 timesteps. Score: 200.0\n",
      "Trial 2 finished after 200 timesteps. Score: 200.0\n",
      "Trial 3 finished after 200 timesteps. Score: 200.0\n",
      "Trial 4 finished after 200 timesteps. Score: 200.0\n",
      "Trial 5 finished after 200 timesteps. Score: 200.0\n",
      "Trial 6 finished after 200 timesteps. Score: 200.0\n",
      "Trial 7 finished after 200 timesteps. Score: 200.0\n",
      "Trial 8 finished after 107 timesteps. Score: 107.0\n",
      "Trial 9 finished after 200 timesteps. Score: 200.0\n",
      "Trial 10 finished after 200 timesteps. Score: 200.0\n",
      "Trial 11 finished after 200 timesteps. Score: 200.0\n",
      "Trial 12 finished after 200 timesteps. Score: 200.0\n",
      "Trial 13 finished after 200 timesteps. Score: 200.0\n",
      "Trial 14 finished after 200 timesteps. Score: 200.0\n",
      "Trial 15 finished after 200 timesteps. Score: 200.0\n",
      "Trial 16 finished after 200 timesteps. Score: 200.0\n",
      "Trial 17 finished after 200 timesteps. Score: 200.0\n",
      "Trial 18 finished after 200 timesteps. Score: 200.0\n",
      "Trial 19 finished after 200 timesteps. Score: 200.0\n",
      "Trial 20 finished after 200 timesteps. Score: 200.0\n",
      "Trial 21 finished after 200 timesteps. Score: 200.0\n",
      "Trial 22 finished after 140 timesteps. Score: 140.0\n",
      "Trial 23 finished after 200 timesteps. Score: 200.0\n",
      "Trial 24 finished after 200 timesteps. Score: 200.0\n",
      "Trial 25 finished after 200 timesteps. Score: 200.0\n",
      "Trial 26 finished after 200 timesteps. Score: 200.0\n",
      "Trial 27 finished after 200 timesteps. Score: 200.0\n",
      "Trial 28 finished after 200 timesteps. Score: 200.0\n",
      "Trial 29 finished after 200 timesteps. Score: 200.0\n",
      "Trial 30 finished after 200 timesteps. Score: 200.0\n",
      "Trial 31 finished after 200 timesteps. Score: 200.0\n",
      "Trial 32 finished after 200 timesteps. Score: 200.0\n",
      "Trial 33 finished after 200 timesteps. Score: 200.0\n",
      "Trial 34 finished after 200 timesteps. Score: 200.0\n",
      "Trial 35 finished after 200 timesteps. Score: 200.0\n",
      "Trial 36 finished after 200 timesteps. Score: 200.0\n",
      "Trial 37 finished after 200 timesteps. Score: 200.0\n",
      "Trial 38 finished after 200 timesteps. Score: 200.0\n",
      "Trial 39 finished after 200 timesteps. Score: 200.0\n",
      "Trial 40 finished after 200 timesteps. Score: 200.0\n",
      "Trial 41 finished after 91 timesteps. Score: 91.0\n",
      "Trial 42 finished after 200 timesteps. Score: 200.0\n",
      "Trial 43 finished after 200 timesteps. Score: 200.0\n",
      "Trial 44 finished after 200 timesteps. Score: 200.0\n",
      "Trial 45 finished after 200 timesteps. Score: 200.0\n",
      "Trial 46 finished after 200 timesteps. Score: 200.0\n",
      "Trial 47 finished after 200 timesteps. Score: 200.0\n",
      "Trial 48 finished after 200 timesteps. Score: 200.0\n",
      "Trial 49 finished after 161 timesteps. Score: 161.0\n",
      "Trial 50 finished after 89 timesteps. Score: 89.0\n",
      "Average Score: 191.76\n",
      "choice 1:0.4990613266583229  choice 0:0.5009386733416771\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "run \"03-cartpole-non-random.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
