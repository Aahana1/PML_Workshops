{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ex2_control_with_reinforcement_learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5tbDbPlV7IU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym > /dev/null 2>&1\n",
        "!pip install tflearn  > /dev/null 2>&1\n",
        "!pip install tensorflow > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8DJS09YWRDd",
        "colab_type": "text"
      },
      "source": [
        "## Hi, I'm AI GYM\n",
        "\n",
        "`gym` is a python module created by Open AI to introduce reinforcement learning to game playing!\n",
        "\n",
        "It's famous for using reinforcement learning to beat lots of old Atari games (see below)\n",
        "  \n",
        " `gym` is a wrapper for different game environments. Each environment has the following properties:\n",
        "  \n",
        "\n",
        "*   ``step()``\n",
        "    * evolves one time step forward\n",
        "* ``action()``\n",
        "    * something to do\n",
        "* ``render()``\n",
        "  * make a display\n",
        "*   ``reset()``\n",
        "    * revert to intial state\n",
        "* ``seed()``\n",
        "    * sets local RNG\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDSFmvQYlPe7",
        "colab_type": "code",
        "outputId": "018e698d-0e48-4702-b181-659847dfbc45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 900
        }
      },
      "source": [
        "from IPython.display import HTML\n",
        "HTML('<img src=\"https://miro.medium.com/max/884/1*qFHnCDhep6OmqkbVN6NY_g.gif\">')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://miro.medium.com/max/884/1*qFHnCDhep6OmqkbVN6NY_g.gif\">"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RxuqScAV9FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgUKKjQ7WEje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.make('CartPole-v0')  # create your environment here\n",
        "env.reset();  # prepare the environment for use"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vo87dQpYdpH",
        "colab_type": "text"
      },
      "source": [
        "Cartpole looks like this:\n",
        "\n",
        "There are two controls: \n",
        "  \n",
        "  0=`move left`\n",
        "  \n",
        "  1= `move right`\n",
        "  \n",
        "  The goal is to balance the pole as long as possible.\n",
        "  \n",
        "<img src=https://gym.openai.com/videos/2019-05-31--eRh4Fbp8G5/CartPole-v1/poster.jpg>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqUOn2uNYaek",
        "colab_type": "code",
        "outputId": "9410221e-1282-4a4b-dc2d-87c8eedd887c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "## Our zeroth order learning strategy is do nothing. \n",
        "##    We'll just try a random operation\n",
        "\n",
        "nsteps = 100  # try this many times\n",
        "i_stop=0;\n",
        "action_list = []\n",
        "for i in range(nsteps):\n",
        "    #env.render()  # display the environment at this step\n",
        "    action = env.action_space.sample()  # choose a random action\n",
        "    result=env.step(action)  # now loop through the action \n",
        "    #action_list.append(action)\n",
        "    if result[2]:\n",
        "        print('out of bounds after '+str(i-i_stop+1)+' steps')\n",
        "        env.reset()\n",
        "        i_stop=i\n",
        "        #print(action_list) #' '.join( [str(i) for i in action_list]) )\n",
        "        #action_list = []"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "out of bounds after 14 steps\n",
            "out of bounds after 13 steps\n",
            "out of bounds after 17 steps\n",
            "out of bounds after 13 steps\n",
            "out of bounds after 46 steps\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2kSpd89aWx2",
        "colab_type": "text"
      },
      "source": [
        "## Let's beef up our learning with reinforcement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClWekvmNZDhh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "e9f7712f-9e15-4560-92e0-48911027e033"
      },
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import tflearn\n",
        "from tflearn.layers.core import input_data, dropout, fully_connected\n",
        "from tflearn.layers.estimator import regression\n",
        "from statistics import median, mean\n",
        "from collections import Counter\n",
        "pass;"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0815 14:46:20.551249 140286260918144 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0815 14:46:20.552624 140286260918144 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0815 14:46:20.562654 140286260918144 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0815 14:46:20.567222 140286260918144 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "W0815 14:46:20.573626 140286260918144 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0815 14:46:20.574349 140286260918144 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0yflxMDawa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LR = 1e-3\n",
        "num_games = 50    # number of trials / episodes\n",
        "goal_steps = 200  # number of steps per trial\n",
        "score_requirement = 50  # the minimum score to achieve\n",
        "initial_games = 10000   # number of games to use for training\n",
        "env = gym.make('CartPole-v0')  # create your environment here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFZMw-rJbto0",
        "colab_type": "text"
      },
      "source": [
        "#### Define some helper functions\n",
        "1. Data-generation\n",
        "2. Neural network model\n",
        "3. Training function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1pZVziPbDzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initial_population():\n",
        "    # [OBS, MOVES]\n",
        "    training_data = []\n",
        "    # all scores:\n",
        "    scores = []\n",
        "    # just the scores that met our threshold:\n",
        "    accepted_scores = []\n",
        "    # iterate through however many games we want:\n",
        "    for _ in range(initial_games):\n",
        "        score = 0\n",
        "        # moves specifically from this environment:\n",
        "        game_memory = []\n",
        "        # previous observation that we saw\n",
        "        prev_observation = []\n",
        "        # for each frame in 200\n",
        "        for _ in range(goal_steps):\n",
        "            # choose random action (0 or 1)\n",
        "            action = random.randrange(0,2)\n",
        "            # do it!\n",
        "            observation, reward, done, info = env.step(action)\n",
        "\n",
        "            # notice that the observation is returned FROM the action\n",
        "            # so we'll store the previous observation here, pairing\n",
        "            # the prev observation to the action we'll take.\n",
        "            if len(prev_observation) > 0 :\n",
        "                game_memory.append([prev_observation, action])\n",
        "            prev_observation = observation\n",
        "            score+=reward\n",
        "            if done: break\n",
        "\n",
        "        # IF our score is higher than our threshold, we'd like to save\n",
        "        # every move we made\n",
        "        # NOTE the reinforcement methodology here.\n",
        "        # all we're doing is reinforcing the score, we're not trying\n",
        "        # to influence the machine in any way as to HOW that score is\n",
        "        # reached.\n",
        "        if score >= score_requirement:\n",
        "            accepted_scores.append(score)\n",
        "            for data in game_memory:\n",
        "                # convert to one-hot (this is the output layer for our neural network)\n",
        "                if data[1] == 1:\n",
        "                    output = [0,1]\n",
        "                elif data[1] == 0:\n",
        "                    output = [1,0]\n",
        "                # saving our training data\n",
        "                training_data.append([data[0], output])\n",
        "\n",
        "        # reset env to play again\n",
        "        env.reset()\n",
        "        # save overall scores\n",
        "        scores.append(score)\n",
        "\n",
        "    # just in case you wanted to reference later\n",
        "    training_data_save = np.array(training_data)\n",
        "    np.save('saved.npy',training_data_save)\n",
        "\n",
        "    # some stats here, to further illustrate the neural network magic!\n",
        "    print('Average of all score:',mean(scores))\n",
        "    print('Average accepted score:',mean(accepted_scores))\n",
        "    print('Median score for accepted scores:',median(accepted_scores))\n",
        "    print(Counter(accepted_scores))\n",
        "\n",
        "    return training_data\n",
        "\n",
        "\n",
        "\n",
        "# Create a simple multilayer perceptron model\n",
        "def neural_network_model(input_size):\n",
        "    # our input layer which will be dependent on the number of inputs\n",
        "    # we want to create\n",
        "    network = input_data(shape=[None, input_size, 1], name='input')\n",
        "\n",
        "    # our first hidden layer\n",
        "    network = fully_connected(network, 128, activation='relu')\n",
        "    network = dropout(network, 0.8)\n",
        "\n",
        "    # our second hiddent layer\n",
        "    network = fully_connected(network, 256, activation='relu')\n",
        "    network = dropout(network, 0.8)\n",
        "\n",
        "    # our third hidden layer\n",
        "    network = fully_connected(network, 512, activation='relu')\n",
        "    network = dropout(network, 0.8)\n",
        "\n",
        "    # our fourth hidden layer\n",
        "    network = fully_connected(network, 256, activation='relu')\n",
        "    network = dropout(network, 0.8)\n",
        "\n",
        "    # our fifth hidden layer\n",
        "    network = fully_connected(network, 128, activation='relu')\n",
        "    network = dropout(network, 0.8)\n",
        "\n",
        "    # the output layer\n",
        "    network = fully_connected(network, 2, activation='softmax')\n",
        "    network = regression(network, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
        "    model = tflearn.DNN(network, tensorboard_dir='log')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(training_data, model=False):\n",
        "    X = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n",
        "    y = [i[1] for i in training_data]\n",
        "\n",
        "    if not model:\n",
        "        model = neural_network_model(input_size = len(X[0]))\n",
        "\n",
        "    model.fit({'input': X}, {'targets': y}, n_epoch=5, snapshot_step=500, show_metric=True, run_id='openai_learning')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMvZz6HlcEQC",
        "colab_type": "code",
        "outputId": "b0e6610c-7911-438e-e9cc-18f51ca3bfa9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "env.reset()\n",
        "training_data = initial_population()\n",
        "model = train_model(training_data)\n",
        "print(\"Trained with {} successful instances\".format(len(training_data)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Step: 1844  | total loss: \u001b[1m\u001b[32m0.66296\u001b[0m\u001b[0m | time: 2.430s\n",
            "| Adam | epoch: 005 | loss: 0.66296 - acc: 0.6044 -- iter: 23552/23610\n",
            "Training Step: 1845  | total loss: \u001b[1m\u001b[32m0.66630\u001b[0m\u001b[0m | time: 2.436s\n",
            "| Adam | epoch: 005 | loss: 0.66630 - acc: 0.5924 -- iter: 23610/23610\n",
            "--\n",
            "Trained with 23610 successful instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bScWYxFhlrym",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNCl0h2IcEwb",
        "colab_type": "code",
        "outputId": "9f2f4d90-7b9b-4f61-c97b-0a356841205c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 905
        }
      },
      "source": [
        "scores = list()\n",
        "choices = list()\n",
        "\n",
        "for trial in range(1, num_games+1):\n",
        "    score = 0\n",
        "    game_memory = list()\n",
        "    prev_observations = list()\n",
        "    env.reset() # prepare the environment for this trial / episode\n",
        "\n",
        "    for step in range(1, goal_steps+1):\n",
        "        #env.render()\n",
        "\n",
        "        # previously, we performed a random action:\n",
        "        # `action = env.action_space.sample()`\n",
        "        #\n",
        "        # now, we're going to say that if the angle is positive, move right (1)\n",
        "        # otherwise, move left (0)\n",
        "        if len(prev_observations) == 0:\n",
        "            # create a random action between 0 and 1\n",
        "            action = random.randrange(0, 2)\n",
        "        else:\n",
        "            action = np.argmax(model.predict(prev_observations.reshape(-1, len(prev_observations), 1))[0])\n",
        "\n",
        "        choices.append(action)\n",
        "\n",
        "        # get what was observed, the reward, if the trial was completed\n",
        "        # and information about the run.\n",
        "        new_observation, reward, done, info = env.step(action)\n",
        "        prev_observations = new_observation\n",
        "\n",
        "        game_memory.append([new_observation, action])\n",
        "        score += reward\n",
        "\n",
        "\n",
        "        # from before we know that the application can sometimes terminate before\n",
        "        # the trial is complete. If that should occur, we want to stop.\n",
        "        if done:\n",
        "            print('Trial {0} finished after {1} timesteps. Score: {2}'.format(trial, step, score))\n",
        "            break\n",
        "    scores.append(score)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 1 finished after 200 timesteps. Score: 200.0\n",
            "Trial 2 finished after 124 timesteps. Score: 124.0\n",
            "Trial 3 finished after 200 timesteps. Score: 200.0\n",
            "Trial 4 finished after 161 timesteps. Score: 161.0\n",
            "Trial 5 finished after 200 timesteps. Score: 200.0\n",
            "Trial 6 finished after 124 timesteps. Score: 124.0\n",
            "Trial 7 finished after 200 timesteps. Score: 200.0\n",
            "Trial 8 finished after 139 timesteps. Score: 139.0\n",
            "Trial 9 finished after 135 timesteps. Score: 135.0\n",
            "Trial 10 finished after 200 timesteps. Score: 200.0\n",
            "Trial 11 finished after 200 timesteps. Score: 200.0\n",
            "Trial 12 finished after 151 timesteps. Score: 151.0\n",
            "Trial 13 finished after 197 timesteps. Score: 197.0\n",
            "Trial 14 finished after 128 timesteps. Score: 128.0\n",
            "Trial 15 finished after 142 timesteps. Score: 142.0\n",
            "Trial 16 finished after 127 timesteps. Score: 127.0\n",
            "Trial 17 finished after 200 timesteps. Score: 200.0\n",
            "Trial 18 finished after 200 timesteps. Score: 200.0\n",
            "Trial 19 finished after 200 timesteps. Score: 200.0\n",
            "Trial 20 finished after 200 timesteps. Score: 200.0\n",
            "Trial 21 finished after 200 timesteps. Score: 200.0\n",
            "Trial 22 finished after 200 timesteps. Score: 200.0\n",
            "Trial 23 finished after 200 timesteps. Score: 200.0\n",
            "Trial 24 finished after 200 timesteps. Score: 200.0\n",
            "Trial 25 finished after 200 timesteps. Score: 200.0\n",
            "Trial 26 finished after 200 timesteps. Score: 200.0\n",
            "Trial 27 finished after 200 timesteps. Score: 200.0\n",
            "Trial 28 finished after 179 timesteps. Score: 179.0\n",
            "Trial 29 finished after 200 timesteps. Score: 200.0\n",
            "Trial 30 finished after 200 timesteps. Score: 200.0\n",
            "Trial 31 finished after 200 timesteps. Score: 200.0\n",
            "Trial 32 finished after 200 timesteps. Score: 200.0\n",
            "Trial 33 finished after 192 timesteps. Score: 192.0\n",
            "Trial 34 finished after 200 timesteps. Score: 200.0\n",
            "Trial 35 finished after 200 timesteps. Score: 200.0\n",
            "Trial 36 finished after 200 timesteps. Score: 200.0\n",
            "Trial 37 finished after 200 timesteps. Score: 200.0\n",
            "Trial 38 finished after 133 timesteps. Score: 133.0\n",
            "Trial 39 finished after 200 timesteps. Score: 200.0\n",
            "Trial 40 finished after 107 timesteps. Score: 107.0\n",
            "Trial 41 finished after 102 timesteps. Score: 102.0\n",
            "Trial 42 finished after 200 timesteps. Score: 200.0\n",
            "Trial 43 finished after 200 timesteps. Score: 200.0\n",
            "Trial 44 finished after 130 timesteps. Score: 130.0\n",
            "Trial 45 finished after 193 timesteps. Score: 193.0\n",
            "Trial 46 finished after 200 timesteps. Score: 200.0\n",
            "Trial 47 finished after 200 timesteps. Score: 200.0\n",
            "Trial 48 finished after 200 timesteps. Score: 200.0\n",
            "Trial 49 finished after 176 timesteps. Score: 176.0\n",
            "Trial 50 finished after 143 timesteps. Score: 143.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOqnZPf_cnkX",
        "colab_type": "code",
        "outputId": "f682055e-ea1e-4736-e606-2c090d5045af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print('Average Score:',sum(scores)/float(len(scores)))\n",
        "print('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))\n",
        "print(score_requirement)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Score: 179.66\n",
            "choice 1:0.499944339307581  choice 0:0.500055660692419\n",
            "50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exRb0626iNMX",
        "colab_type": "text"
      },
      "source": [
        "## Compare CartPole to the control theory approach:\n",
        "\n",
        "[YouTube link](https://youtu.be/1_UobILf3cc?t=694)\n",
        "See 8:08 and 11:35"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8Xy9UXhe2oe",
        "colab_type": "text"
      },
      "source": [
        "## Alternative games\n",
        "1. Taxi driver\n",
        "\n",
        "  Actions: \n",
        "    There are 6 discrete deterministic actions:\n",
        "    - 0: move south\n",
        "    - 1: move north\n",
        "    - 2: move east \n",
        "    - 3: move west \n",
        "    - 4: pickup passenger\n",
        "    - 5: dropoff passenger\n",
        "    \n",
        "  Rewards: \n",
        "    * -1 for each action \n",
        "    * +20 for delievering the passenger\n",
        "    * -10 for executing actions \"pickup\" and \"dropoff\" illegally.    \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKFQjaxNi1Iy",
        "colab_type": "code",
        "outputId": "4a9fb3cd-1af9-42ce-e31f-acf913d26997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "env2 = gym.make('Taxi-v2')\n",
        "\n",
        "env2.reset()\n",
        "env2.render()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : :\u001b[43m \u001b[0m: : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhcGTMjBi_L7",
        "colab_type": "code",
        "outputId": "0d511064-2669-4f08-9a30-2db8e6dc5f7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "## play around a bit\n",
        "state, reward, done, info = env2.step(1); env2.render()\n",
        "print(\"reward: \", reward)\n",
        "state, reward, done, info = env2.step(1)\n",
        "state, reward, done, info = env2.step(1)\n",
        "state, reward, done, info = env2.step(1)\n",
        "env2.render()\n",
        "print(\"reward: \", reward)\n",
        "\n",
        "state, reward, done, info = env2.step(4)\n",
        "print(\"reward: \", reward)\n",
        "env2.render()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: |\u001b[43m \u001b[0m: :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "reward:  -1\n",
            "+---------+\n",
            "|R: |\u001b[43m \u001b[0m: :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "reward:  -1\n",
            "reward:  -10\n",
            "+---------+\n",
            "|R: |\u001b[43m \u001b[0m: :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdFPMq26o1AO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "a221dbc9-829e-4e77-eda1-16184d50ee6d"
      },
      "source": [
        "state, reward, done, info = env2.step(0); print(reward, done)\n",
        "state, reward, done, info = env2.step(0); print(reward, done)\n",
        "state, reward, done, info = env2.step(0); print(reward, done)\n",
        "state, reward, done, info = env2.step(0); print(reward, done)\n",
        "env2.render()\n",
        "\n",
        "state, reward, done, info = env2.step(5); print(reward, done)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1 False\n",
            "-1 False\n",
            "-1 False\n",
            "-1 False\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : : : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
            "+---------+\n",
            "  (South)\n",
            "-10 False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dm6fkfV-kKyi",
        "colab_type": "text"
      },
      "source": [
        "## Build your own!\n",
        "* See source code at /usr/local/lib/python3.6/dist-packages/gym/envs/toy_text/taxi.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaYiI0upka6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "\n",
        "\n",
        "class GuessingGame(gym.Env):\n",
        "    \"\"\"Number guessing game\n",
        "\n",
        "    The object of the game is to guess within 1% of the randomly chosen number\n",
        "    within 200 time steps\n",
        "\n",
        "    After each step the agent is provided with one of four possible observations\n",
        "    which indicate where the guess is in relation to the randomly chosen number\n",
        "\n",
        "    0 - No guess yet submitted (only after reset)\n",
        "    1 - Guess is lower than the target\n",
        "    2 - Guess is equal to the target\n",
        "    3 - Guess is higher than the target\n",
        "\n",
        "    The rewards are:\n",
        "    0 if the agent's guess is outside of 1% of the target\n",
        "    1 if the agent's guess is inside 1% of the target\n",
        "\n",
        "    The episode terminates after the agent guesses within 1% of the target or\n",
        "    200 steps have been taken\n",
        "\n",
        "    The agent will need to use a memory of previously submitted actions and observations\n",
        "    in order to efficiently explore the available actions\n",
        "\n",
        "    The purpose is to have agents optimise their exploration parameters (e.g. how far to\n",
        "    explore from previous actions) based on previous experience. Because the goal changes\n",
        "    each episode a state-value or action-value function isn't able to provide any additional\n",
        "    benefit apart from being able to tell whether to increase or decrease the next guess.\n",
        "\n",
        "    The perfect agent would likely learn the bounds of the action space (without referring\n",
        "    to them explicitly) and then follow binary tree style exploration towards to goal number\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.range = 1000  # Randomly selected number is within +/- this value\n",
        "        self.bounds = 10000\n",
        "\n",
        "        self.action_space = spaces.Box(low=np.array([-self.bounds]), high=np.array([self.bounds]),\n",
        "                                       dtype=np.float32)\n",
        "        self.observation_space = spaces.Discrete(4)\n",
        "\n",
        "        self.number = 0\n",
        "        self.guess_count = 0\n",
        "        self.guess_max = 200\n",
        "        self.observation = 0\n",
        "\n",
        "        self.seed()\n",
        "        self.reset()\n",
        "\n",
        "    def seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.action_space.contains(action)\n",
        "\n",
        "        if action < self.number:\n",
        "            self.observation = 1\n",
        "\n",
        "        elif action == self.number:\n",
        "            self.observation = 2\n",
        "\n",
        "        elif action > self.number:\n",
        "            self.observation = 3\n",
        "\n",
        "        reward = 0\n",
        "        done = False\n",
        "\n",
        "        if (self.number - self.range * 0.01) < action < (self.number + self.range * 0.01):\n",
        "            reward = 1\n",
        "            done = True\n",
        "\n",
        "        self.guess_count += 1\n",
        "        if self.guess_count >= self.guess_max:\n",
        "            done = True\n",
        "\n",
        "        return self.observation, reward, done, {\"number\": self.number, \"guesses\": self.guess_count}\n",
        "\n",
        "    def reset(self):\n",
        "        self.number = self.np_random.uniform(-self.range, self.range)\n",
        "        self.guess_count = 0\n",
        "        self.observation = 0\n",
        "        return self.observation\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTtY_ogPeSoU",
        "colab_type": "code",
        "outputId": "ce89eab0-8c08-4887-d231-2afc135d555e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!cat \"/usr/local/lib/python3.6/dist-packages/gym/core.py\""
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "from gym import logger\n",
            "\n",
            "import gym\n",
            "from gym import error\n",
            "from gym.utils import closer\n",
            "\n",
            "env_closer = closer.Closer()\n",
            "\n",
            "# Env-related abstractions\n",
            "\n",
            "class Env(object):\n",
            "    \"\"\"The main OpenAI Gym class. It encapsulates an environment with\n",
            "    arbitrary behind-the-scenes dynamics. An environment can be\n",
            "    partially or fully observed.\n",
            "\n",
            "    The main API methods that users of this class need to know are:\n",
            "\n",
            "        step\n",
            "        reset\n",
            "        render\n",
            "        close\n",
            "        seed\n",
            "\n",
            "    And set the following attributes:\n",
            "\n",
            "        action_space: The Space object corresponding to valid actions\n",
            "        observation_space: The Space object corresponding to valid observations\n",
            "        reward_range: A tuple corresponding to the min and max possible rewards\n",
            "\n",
            "    Note: a default reward range set to [-inf,+inf] already exists. Set it if you want a narrower range.\n",
            "\n",
            "    The methods are accessed publicly as \"step\", \"reset\", etc.. The\n",
            "    non-underscored versions are wrapper methods to which we may add\n",
            "    functionality over time.\n",
            "    \"\"\"\n",
            "\n",
            "    # Set this in SOME subclasses\n",
            "    metadata = {'render.modes': []}\n",
            "    reward_range = (-float('inf'), float('inf'))\n",
            "    spec = None\n",
            "\n",
            "    # Set these in ALL subclasses\n",
            "    action_space = None\n",
            "    observation_space = None\n",
            "\n",
            "    def step(self, action):\n",
            "        \"\"\"Run one timestep of the environment's dynamics. When end of\n",
            "        episode is reached, you are responsible for calling `reset()`\n",
            "        to reset this environment's state.\n",
            "\n",
            "        Accepts an action and returns a tuple (observation, reward, done, info).\n",
            "\n",
            "        Args:\n",
            "            action (object): an action provided by the environment\n",
            "\n",
            "        Returns:\n",
            "            observation (object): agent's observation of the current environment\n",
            "            reward (float) : amount of reward returned after previous action\n",
            "            done (boolean): whether the episode has ended, in which case further step() calls will return undefined results\n",
            "            info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
            "        \"\"\"\n",
            "        raise NotImplementedError\n",
            "\n",
            "    def reset(self):\n",
            "        \"\"\"Resets the state of the environment and returns an initial observation.\n",
            "\n",
            "        Returns: observation (object): the initial observation of the\n",
            "            space.\n",
            "        \"\"\"\n",
            "        raise NotImplementedError\n",
            "\n",
            "    def render(self, mode='human'):\n",
            "        \"\"\"Renders the environment.\n",
            "\n",
            "        The set of supported modes varies per environment. (And some\n",
            "        environments do not support rendering at all.) By convention,\n",
            "        if mode is:\n",
            "\n",
            "        - human: render to the current display or terminal and\n",
            "          return nothing. Usually for human consumption.\n",
            "        - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
            "          representing RGB values for an x-by-y pixel image, suitable\n",
            "          for turning into a video.\n",
            "        - ansi: Return a string (str) or StringIO.StringIO containing a\n",
            "          terminal-style text representation. The text can include newlines\n",
            "          and ANSI escape sequences (e.g. for colors).\n",
            "\n",
            "        Note:\n",
            "            Make sure that your class's metadata 'render.modes' key includes\n",
            "              the list of supported modes. It's recommended to call super()\n",
            "              in implementations to use the functionality of this method.\n",
            "\n",
            "        Args:\n",
            "            mode (str): the mode to render with\n",
            "            close (bool): close all open renderings\n",
            "\n",
            "        Example:\n",
            "\n",
            "        class MyEnv(Env):\n",
            "            metadata = {'render.modes': ['human', 'rgb_array']}\n",
            "\n",
            "            def render(self, mode='human'):\n",
            "                if mode == 'rgb_array':\n",
            "                    return np.array(...) # return RGB frame suitable for video\n",
            "                elif mode is 'human':\n",
            "                    ... # pop up a window and render\n",
            "                else:\n",
            "                    super(MyEnv, self).render(mode=mode) # just raise an exception\n",
            "        \"\"\"\n",
            "        raise NotImplementedError\n",
            "\n",
            "    def close(self):\n",
            "        \"\"\"Override _close in your subclass to perform any necessary cleanup.\n",
            "\n",
            "        Environments will automatically close() themselves when\n",
            "        garbage collected or when the program exits.\n",
            "        \"\"\"\n",
            "        return\n",
            "\n",
            "    def seed(self, seed=None):\n",
            "        \"\"\"Sets the seed for this env's random number generator(s).\n",
            "\n",
            "        Note:\n",
            "            Some environments use multiple pseudorandom number generators.\n",
            "            We want to capture all such seeds used in order to ensure that\n",
            "            there aren't accidental correlations between multiple generators.\n",
            "\n",
            "        Returns:\n",
            "            list<bigint>: Returns the list of seeds used in this env's random\n",
            "              number generators. The first value in the list should be the\n",
            "              \"main\" seed, or the value which a reproducer should pass to\n",
            "              'seed'. Often, the main seed equals the provided 'seed', but\n",
            "              this won't be true if seed=None, for example.\n",
            "        \"\"\"\n",
            "        logger.warn(\"Could not seed environment %s\", self)\n",
            "        return\n",
            "\n",
            "    @property\n",
            "    def unwrapped(self):\n",
            "        \"\"\"Completely unwrap this env.\n",
            "\n",
            "        Returns:\n",
            "            gym.Env: The base non-wrapped gym.Env instance\n",
            "        \"\"\"\n",
            "        return self\n",
            "\n",
            "    def __str__(self):\n",
            "        if self.spec is None:\n",
            "            return '<{} instance>'.format(type(self).__name__)\n",
            "        else:\n",
            "            return '<{}<{}>>'.format(type(self).__name__, self.spec.id)\n",
            "\n",
            "\n",
            "class GoalEnv(Env):\n",
            "    \"\"\"A goal-based environment. It functions just as any regular OpenAI Gym environment but it\n",
            "    imposes a required structure on the observation_space. More concretely, the observation\n",
            "    space is required to contain at least three elements, namely `observation`, `desired_goal`, and\n",
            "    `achieved_goal`. Here, `desired_goal` specifies the goal that the agent should attempt to achieve.\n",
            "    `achieved_goal` is the goal that it currently achieved instead. `observation` contains the\n",
            "    actual observations of the environment as per usual.\n",
            "    \"\"\"\n",
            "\n",
            "    def reset(self):\n",
            "        # Enforce that each GoalEnv uses a Goal-compatible observation space.\n",
            "        if not isinstance(self.observation_space, gym.spaces.Dict):\n",
            "            raise error.Error('GoalEnv requires an observation space of type gym.spaces.Dict')\n",
            "        result = super(GoalEnv, self).reset()\n",
            "        for key in ['observation', 'achieved_goal', 'desired_goal']:\n",
            "            if key not in result:\n",
            "                raise error.Error('GoalEnv requires the \"{}\" key to be part of the observation dictionary.'.format(key))\n",
            "        return result\n",
            "\n",
            "    def compute_reward(self, achieved_goal, desired_goal, info):\n",
            "        \"\"\"Compute the step reward. This externalizes the reward function and makes\n",
            "        it dependent on an a desired goal and the one that was achieved. If you wish to include\n",
            "        additional rewards that are independent of the goal, you can include the necessary values\n",
            "        to derive it in info and compute it accordingly.\n",
            "\n",
            "        Args:\n",
            "            achieved_goal (object): the goal that was achieved during execution\n",
            "            desired_goal (object): the desired goal that we asked the agent to attempt to achieve\n",
            "            info (dict): an info dictionary with additional information\n",
            "\n",
            "        Returns:\n",
            "            float: The reward that corresponds to the provided achieved goal w.r.t. to the desired\n",
            "            goal. Note that the following should always hold true:\n",
            "\n",
            "                ob, reward, done, info = env.step()\n",
            "                assert reward == env.compute_reward(ob['achieved_goal'], ob['goal'], info)\n",
            "        \"\"\"\n",
            "        raise NotImplementedError()\n",
            "\n",
            "# Space-related abstractions\n",
            "\n",
            "class Space(object):\n",
            "    \"\"\"Defines the observation and action spaces, so you can write generic\n",
            "    code that applies to any Env. For example, you can choose a random\n",
            "    action.\n",
            "    \"\"\"\n",
            "    def __init__(self, shape=None, dtype=None):\n",
            "        import numpy as np # takes about 300-400ms to import, so we load lazily\n",
            "        self.shape = None if shape is None else tuple(shape)\n",
            "        self.dtype = None if dtype is None else np.dtype(dtype)\n",
            "\n",
            "    def sample(self):\n",
            "        \"\"\"\n",
            "        Uniformly randomly sample a random element of this space\n",
            "        \"\"\"\n",
            "        raise NotImplementedError\n",
            "\n",
            "    def contains(self, x):\n",
            "        \"\"\"\n",
            "        Return boolean specifying if x is a valid\n",
            "        member of this space\n",
            "        \"\"\"\n",
            "        raise NotImplementedError\n",
            "\n",
            "    def __contains__(self, x):\n",
            "        return self.contains(x)\n",
            "\n",
            "    def to_jsonable(self, sample_n):\n",
            "        \"\"\"Convert a batch of samples from this space to a JSONable data type.\"\"\"\n",
            "        # By default, assume identity is JSONable\n",
            "        return sample_n\n",
            "\n",
            "    def from_jsonable(self, sample_n):\n",
            "        \"\"\"Convert a JSONable data type to a batch of samples from this space.\"\"\"\n",
            "        # By default, assume identity is JSONable\n",
            "        return sample_n\n",
            "\n",
            "\n",
            "warn_once = True\n",
            "\n",
            "def deprecated_warn_once(text):\n",
            "    global warn_once\n",
            "    if not warn_once: return\n",
            "    warn_once = False\n",
            "    logger.warn(text)\n",
            "\n",
            "\n",
            "class Wrapper(Env):\n",
            "    env = None\n",
            "\n",
            "    def __init__(self, env):\n",
            "        self.env = env\n",
            "        self.action_space = self.env.action_space\n",
            "        self.observation_space = self.env.observation_space\n",
            "        self.reward_range = self.env.reward_range\n",
            "        self.metadata = self.env.metadata\n",
            "\n",
            "    @classmethod\n",
            "    def class_name(cls):\n",
            "        return cls.__name__\n",
            "\n",
            "    def step(self, action):\n",
            "        if hasattr(self, \"_step\"):\n",
            "            deprecated_warn_once(\"%s doesn't implement 'step' method, but it implements deprecated '_step' method.\" % type(self))\n",
            "            self.step = self._step\n",
            "            return self.step(action)\n",
            "        else:\n",
            "            deprecated_warn_once(\"%s doesn't implement 'step' method, \" % type(self) +\n",
            "                \"which is required for wrappers derived directly from Wrapper. Deprecated default implementation is used.\")\n",
            "            return self.env.step(action)\n",
            "\n",
            "    def reset(self, **kwargs):\n",
            "        if hasattr(self, \"_reset\"):\n",
            "            deprecated_warn_once(\"%s doesn't implement 'reset' method, but it implements deprecated '_reset' method.\" % type(self))\n",
            "            self.reset = self._reset\n",
            "            return self._reset(**kwargs)\n",
            "        else:\n",
            "            deprecated_warn_once(\"%s doesn't implement 'reset' method, \" % type(self) +\n",
            "                \"which is required for wrappers derived directly from Wrapper. Deprecated default implementation is used.\")\n",
            "            return self.env.reset(**kwargs)\n",
            "\n",
            "    def render(self, mode='human', **kwargs):\n",
            "        return self.env.render(mode, **kwargs)\n",
            "\n",
            "    def close(self):\n",
            "        if self.env:\n",
            "            return self.env.close()\n",
            "\n",
            "    def seed(self, seed=None):\n",
            "        return self.env.seed(seed)\n",
            "\n",
            "    def compute_reward(self, achieved_goal, desired_goal, info):\n",
            "        return self.env.compute_reward(achieved_goal, desired_goal, info)\n",
            "\n",
            "    def __str__(self):\n",
            "        return '<{}{}>'.format(type(self).__name__, self.env)\n",
            "\n",
            "    def __repr__(self):\n",
            "        return str(self)\n",
            "\n",
            "    @property\n",
            "    def unwrapped(self):\n",
            "        return self.env.unwrapped\n",
            "\n",
            "    @property\n",
            "    def spec(self):\n",
            "        return self.env.spec\n",
            "\n",
            "\n",
            "class ObservationWrapper(Wrapper):\n",
            "    def step(self, action):\n",
            "        observation, reward, done, info = self.env.step(action)\n",
            "        return self.observation(observation), reward, done, info\n",
            "\n",
            "    def reset(self, **kwargs):\n",
            "        observation = self.env.reset(**kwargs)\n",
            "        return self.observation(observation)\n",
            "\n",
            "    def observation(self, observation):\n",
            "        deprecated_warn_once(\"%s doesn't implement 'observation' method. Maybe it implements deprecated '_observation' method.\" % type(self))\n",
            "        return self._observation(observation)\n",
            "\n",
            "\n",
            "class RewardWrapper(Wrapper):\n",
            "    def reset(self):\n",
            "        return self.env.reset()\n",
            "\n",
            "    def step(self, action):\n",
            "        observation, reward, done, info = self.env.step(action)\n",
            "        return observation, self.reward(reward), done, info\n",
            "\n",
            "    def reward(self, reward):\n",
            "        deprecated_warn_once(\"%s doesn't implement 'reward' method. Maybe it implements deprecated '_reward' method.\" % type(self))\n",
            "        return self._reward(reward)\n",
            "\n",
            "\n",
            "class ActionWrapper(Wrapper):\n",
            "    def step(self, action):\n",
            "        action = self.action(action)\n",
            "        return self.env.step(action)\n",
            "\n",
            "    def reset(self):\n",
            "        return self.env.reset()\n",
            "\n",
            "    def action(self, action):\n",
            "        deprecated_warn_once(\"%s doesn't implement 'action' method. Maybe it implements deprecated '_action' method.\" % type(self))\n",
            "        return self._action(action)\n",
            "\n",
            "    def reverse_action(self, action):\n",
            "        deprecated_warn_once(\"%s doesn't implement 'reverse_action' method. Maybe it implements deprecated '_reverse_action' method.\" % type(self))\n",
            "        return self._reverse_action(action)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}