{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "workshop5_example6_TimeSeries.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-trTdvbTR4m",
        "colab_type": "text"
      },
      "source": [
        "# Workshop 5 Example 6\n",
        "# Simple time series analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJ3Q4x8RiRYT",
        "colab_type": "text"
      },
      "source": [
        "# Autoregression\n",
        "\n",
        "https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/\n",
        "\n",
        "Autoregression is a time series model that uses observations from previous time steps as input to a regression equation to predict the value at the next time step.\n",
        "\n",
        "A regression model, such as linear regression, models an output value based on a linear combination of input values.\n",
        "\n",
        "For example:\n",
        "\n",
        "$y = b_0 + b_1X$\n",
        "\n",
        "Where $y$ is the prediction,$ b_0$ and $b_1$ are coefficients found by optimizing the model on training data, and $X$ is an input value.\n",
        "\n",
        "This technique can be used on time series where input variables are taken as observations at previous time steps, called lag variables.\n",
        "\n",
        "For example, we can predict the value for the next time step (t+1) given the observations at the last two time steps (t-1 and t-2). As a regression model, this would look as follows:\n",
        "\n",
        "$X(t+1) = b_0 + b_1X(t-1) + b_2X(t-2)$\n",
        "\n",
        "Because the regression model uses data from the same input variable at previous time steps, it is referred to as an autoregression (regression of self)\n",
        "\n",
        "## Autocorrelation\n",
        "\n",
        "An autoregression model makes an assumption that the observations at previous time steps are useful to predict the value at the next time step.\n",
        "\n",
        "This relationship between variables is called correlation.\n",
        "\n",
        "If both variables change in the same direction (e.g. go up together or down together), this is called a positive correlation. If the variables move in opposite directions as values change (e.g. one goes up and one goes down), then this is called negative correlation.\n",
        "\n",
        "We can use statistical measures to calculate the correlation between the output variable and values at previous time steps at various different lags. The stronger the correlation between the output variable and a specific lagged variable, the more weight that autoregression model can put on that variable when modeling.\n",
        "\n",
        "The correlation statistics can also help to choose which lag variables will be useful in a model and which will not.\n",
        "\n",
        "### Minimum Daily Temperatures Dataset\n",
        "This dataset describes the minimum daily temperatures over 10 years (1981-1990) in the city Melbourne, Australia.\n",
        "\n",
        "The units are in degrees Celsius and there are 3,650 observations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvxB7R4YkdE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv\n",
        "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/shampoo.csv\n",
        "  \n",
        "# this is to hide Pandas warnings for using from_csv instead of the updated read_csv\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc37yZ78pk5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# load the data using pandas\n",
        "series = pd.Series.from_csv('daily-min-temperatures.csv', header=0)\n",
        "\n",
        "# show the first couple of rows to visualize the dataframe\n",
        "print(series.head())\n",
        "\n",
        "# plot the data\n",
        "series.plot()\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGGVJPiU-pTx",
        "colab_type": "text"
      },
      "source": [
        "### Quick Check for Autocorrelation\n",
        "\n",
        "We can plot the observation at the previous time step (t-1) with the observation at the next time step (t+1) as a scatter plot.\n",
        "\n",
        "This could be done manually by first creating a lag version of the time series dataset and using a built-in scatter plot function in the Pandas library.\n",
        "\n",
        "Pandas provides a built-in plot to do exactly this, called the lag_plot() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd1kLEbXpk-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot\n",
        "from pandas.plotting import lag_plot\n",
        "\n",
        "# read in the data using pandas\n",
        "series = pd.Series.from_csv('daily-min-temperatures.csv', header=0)\n",
        "\n",
        "# plot a lag plot to check for autocorrelation\n",
        "lag_plot(series)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfzsObw2BwTB",
        "colab_type": "text"
      },
      "source": [
        "We can use a statistical test like the **Pearson correlation coefficient**. This produces a number to summarize how correlated two variables are between -1 (negatively correlated) and +1 (positively correlated) with small values close to zero indicating low correlation and high values above 0.5 or below -0.5 showing high correlation.\n",
        "\n",
        "Correlation can be calculated easily using the corr() function on the DataFrame of the lagged dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu_mzc8_Bvus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import Series\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from matplotlib import pyplot\n",
        "\n",
        "# import data using pandas\n",
        "series = Series.from_csv('daily-min-temperatures.csv', header=0)\n",
        "\n",
        "# get the values of the pandas dataframe\n",
        "values = DataFrame(series.values)\n",
        "\n",
        "# create dataframe with values and shifted values by 1\n",
        "dataframe = concat([values.shift(1), values], axis=1)\n",
        "\n",
        "# dataframe column names\n",
        "dataframe.columns = ['t-1', 't+1']\n",
        "\n",
        "# calculate the Pearson correlation coefficient\n",
        "result = dataframe.corr()\n",
        "print(result,'\\n\\nstrong positive correlation 0.775\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bdq17u82Drr1",
        "colab_type": "text"
      },
      "source": [
        "### Autocorrelation Plots\n",
        "We can plot the correlation coefficient for each lag variable.\n",
        "\n",
        "This can very quickly give an idea of which lag variables may be good candidates for use in a predictive model and how the relationship between the observation and its historic values changes over time.\n",
        "\n",
        "Pandas provides a built-in plot called the autocorrelation_plot() function.\n",
        "\n",
        "The plot provides the lag number along the x-axis and the correlation coefficient value between -1 and 1 on the y-axis. The plot also includes solid and dashed lines that indicate the 95% and 99% confidence interval for the correlation values. Correlation values above these lines are more significant than those below the line, providing a threshold or cutoff for selecting more relevant lag values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1klBmpvGD7g_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import Series\n",
        "from matplotlib import pyplot\n",
        "from pandas.plotting import autocorrelation_plot\n",
        "\n",
        "# import data using pandas\n",
        "series = Series.from_csv('daily-min-temperatures.csv', header=0)\n",
        "\n",
        "# build and plot the autocorrelation\n",
        "autocorrelation_plot(series)\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8ULyhW8FaNK",
        "colab_type": "text"
      },
      "source": [
        "each peak in the autocorrelation plot is 1 year apart\n",
        "\n",
        "\n",
        "### Persistence Model\n",
        "\n",
        "Let’s say that we want to develop a model to predict the last 7 days of minimum temperatures in the dataset given all prior observations.\n",
        "\n",
        "The simplest model that we could use to make predictions would be to persist the last observation. We can call this a persistence model and it provides a baseline of performance for the problem that we can use for comparison with an autoregression model.\n",
        "\n",
        "We can develop a test harness for the problem by splitting the observations into training and test sets, with only the last 7 observations in the dataset assigned to the test set as “unseen” data that we wish to predict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aAww88h4hDT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import Series\n",
        "from pandas import DataFrame\n",
        "from pandas import concat\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# load the data using pandas\n",
        "series = Series.from_csv('daily-min-temperatures.csv', header=0)\n",
        "\n",
        "# create lagged dataset\n",
        "values = DataFrame(series.values)\n",
        "dataframe = concat([values.shift(1), values], axis=1)\n",
        "dataframe.columns = ['t-1', 't+1']\n",
        "\n",
        "# split into train and test sets\n",
        "X = dataframe.values\n",
        "train, test = X[1:len(X)-7], X[len(X)-7:]\n",
        "train_X, train_y = train[:,0], train[:,1]\n",
        "test_X, test_y = test[:,0], test[:,1]\n",
        "\n",
        "# persistence model\n",
        "def model_persistence(x):\n",
        "\treturn x\n",
        "\n",
        "# walk-forward validation\n",
        "predictions = list()\n",
        "for x in test_X:\n",
        "  yhat = model_persistence(x)\n",
        "  predictions.append(yhat)\n",
        "\n",
        "# create a test score\n",
        "test_score = mean_squared_error(test_y, predictions)\n",
        "print('Test MSE: %.3f' % test_score)\n",
        "\n",
        "# plot predictions vs expected\n",
        "plt.plot(test_y, color='blue', label='test')\n",
        "plt.plot(predictions, color='red', label='prediction')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H90yCXIY6iZp",
        "colab_type": "text"
      },
      "source": [
        "### Autoregression Model\n",
        "An autoregression model is a linear regression model that uses lagged variables as input variables. The time step value at timestep $t$, $X_t$, is calculated as\n",
        "\n",
        "$$X_t=c+\\sum_i^p \\phi_i X_{t-i} +\\epsilon_t $$\n",
        "\n",
        "where the $\\phi$ values are the model parameters and $\\epsilon_t$ is noise. $p$ is the lag value (i.e. how far in the past to look to calculate the new value)\n",
        "\n",
        "The statsmodels library provides an autoregression model that automatically selects an appropriate lag value using statistical tests and trains a linear regression model. It is provided in the AR class.\n",
        "\n",
        "We can use this model by first creating the model AR() and then calling fit() to train it on our dataset. This returns an ARResult object.\n",
        "\n",
        "Once fit, we can use the model to make a prediction by calling the predict() function for a number of observations in the future. This creates 1 7-day forecast, which is different from the persistence example above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LNvF89f6tIV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import Series\n",
        "from matplotlib import pyplot as plt\n",
        "from statsmodels.tsa.ar_model import AR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# import data using pandas\n",
        "series = Series.from_csv('daily-min-temperatures.csv', header=0)\n",
        "\n",
        "# split dataset\n",
        "X = series.values\n",
        "train, test = X[1:len(X)-7], X[len(X)-7:]\n",
        "\n",
        "# train autoregression\n",
        "model = AR(train)\n",
        "model_fit = model.fit()\n",
        "print('Lag: %s' % model_fit.k_ar)\n",
        "print('Coefficients: %s' % model_fit.params)\n",
        "\n",
        "# make predictions\n",
        "predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=True)\n",
        "for i in range(len(predictions)):\n",
        "  print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n",
        "\n",
        "# calculate the error\n",
        "error = mean_squared_error(test, predictions)\n",
        "print('\\nTest MSE: %.3f' % error)\n",
        "\n",
        "# plot results\n",
        "plt.plot(test, color='blue', label='test')\n",
        "plt.plot(predictions, color='red', label='prediction')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J213lJyI7p7U",
        "colab_type": "text"
      },
      "source": [
        "Let's see what happens when we use autoregression to look far into the future"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saY3KGEl7pHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import Series\n",
        "from matplotlib import pyplot as plt\n",
        "from statsmodels.tsa.ar_model import AR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# import data using pandas\n",
        "series = Series.from_csv('daily-min-temperatures.csv', header=0)\n",
        "\n",
        "# split dataset\n",
        "X = series.values\n",
        "train = X\n",
        "\n",
        "num_future_days = 2*365\n",
        "\n",
        "# train autoregression\n",
        "model = AR(train)\n",
        "model_fit = model.fit()\n",
        "print('Lag: %s' % model_fit.k_ar)\n",
        "\n",
        "# make predictions\n",
        "predictions = model_fit.predict(start=len(train), end=len(train)+num_future_days, dynamic=True)\n",
        "\n",
        "total = np.append(X,predictions)\n",
        "\n",
        "# plot results\n",
        "plt.figure(1)\n",
        "plt.plot(total, color='blue')\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(predictions, color='red', label='prediction')\n",
        "plt.legend(loc='best')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07KI8qMrVW_T",
        "colab_type": "text"
      },
      "source": [
        "# Moving Average (MA)\n",
        "\n",
        "a common approach for modeling univariate time series. The moving-average model specifies that the output variable depends linearly on the current and various past values of a stochastic (imperfectly predictable) term. The value in a series is calculated as\n",
        "\n",
        "$$X_t=\\mu + \\epsilon_t+\\theta_1\\epsilon_{t-1}+...+\\theta_q\\epsilon_{t-q}$$\n",
        "\n",
        "where $\\mu$ is the mean of the series, the $\\theta$'s are the model parameters and the $\\epsilon$'s are white noise error terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQmo869kVkVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import Series\n",
        "from matplotlib import pyplot as plt\n",
        "from statsmodels.tsa.arima_model import ARMA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# import data using pandas\n",
        "series = Series.from_csv('daily-min-temperatures.csv', header=0)\n",
        "\n",
        "# split dataset\n",
        "X = series.values\n",
        "train, test = X[1:len(X)-7], X[len(X)-7:]\n",
        "\n",
        "# train the moving average model\n",
        "model = ARMA(train, order=(0, 4))\n",
        "model_fit = model.fit()\n",
        "print('Coefficients: %s' % model_fit.params)\n",
        "\n",
        "# make predictions\n",
        "predictions = model_fit.predict(start=len(train), end=len(train)+len(test)-1, dynamic=True)\n",
        "for i in range(len(predictions)):\n",
        "  print('predicted=%f, expected=%f' % (predictions[i], test[i]))\n",
        "\n",
        "# calculate the error\n",
        "error = mean_squared_error(test, predictions)\n",
        "print('\\nTest MSE: %.3f' % error)\n",
        "\n",
        "# plot results\n",
        "plt.plot(test, color='blue', label='test')\n",
        "plt.plot(predictions, color='red', label='prediction')\n",
        "plt.title('looking 7 points into the future')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfKuCi-HsAeS",
        "colab_type": "text"
      },
      "source": [
        "Let's now see how well the model fits the training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4cT1JSVrXjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import Series\n",
        "from matplotlib import pyplot as plt\n",
        "from statsmodels.tsa.arima_model import ARMA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# import data using pandas\n",
        "series = Series.from_csv('daily-min-temperatures.csv', header=0)\n",
        "\n",
        "# get the series values\n",
        "X = series.values\n",
        "\n",
        "# train the moving average model\n",
        "model = ARMA(X, order=(0, 4))\n",
        "model_fit = model.fit()\n",
        "print('Coefficients: %s' % model_fit.params)\n",
        "\n",
        "# make predictions\n",
        "predictions = model_fit.predict(start=0, end=len(X)-1, dynamic=False)\n",
        "\n",
        "# calculate the error\n",
        "error = mean_squared_error(X, predictions)\n",
        "print('\\nTest MSE: %.3f' % error)\n",
        "\n",
        "# plot results\n",
        "plt.figure(1)\n",
        "plt.plot(X, color='blue', label='test')\n",
        "plt.plot(predictions, color='red', label='prediction')\n",
        "plt.legend(loc='best')\n",
        "plt.xlim((1000,2000))\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6kBaxQDUnVl",
        "colab_type": "text"
      },
      "source": [
        "#  Autoregressive Integrated Moving Average (ARIMA)\n",
        "The Autoregressive Integrated Moving Average (ARIMA) method models the next step in the sequence as a linear function of the differenced observations and residual errors at prior time steps.\n",
        "\n",
        "**AR**: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\n",
        "\n",
        "**I*: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n",
        "\n",
        "**MA**: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n",
        "\n",
        "What does it mean for data to be stationary?\n",
        "\n",
        "The mean of the series should not be a function of time. The red graph below is not stationary because the mean increases over time.\n",
        "\n",
        "<img src=https://imgur.com/LjtBXwf.png width=\"500\">\n",
        "\n",
        "The variance of the series should not be a function of time. This property is known as homoscedasticity. Notice in the red graph the varying spread of data over time.\n",
        "\n",
        "<img src=https://imgur.com/v2Uye7X.png width=\"500\">\n",
        "\n",
        "Finally, the covariance of the i th term and the (i + m) th term should not be a function of time. In the following graph, you will notice the spread becomes closer as the time increases. Hence, the covariance is not constant with time for the ‘red series’.\n",
        "\n",
        "<img src=https://i.imgur.com/6HVlvg2.png width=\"500\">\n",
        "\n",
        "The notation for the model involves specifying the order for the AR(p), I(d), and MA(q) models as parameters to an ARIMA function\n",
        "\n",
        "**p**: The number of lag observations included in the model, also called the lag order.\n",
        "\n",
        "**d**: The number of times that the raw observations are differenced, also called the degree of differencing.\n",
        "\n",
        "**q**: The size of the moving average window, also called the order of moving average.\n",
        "\n",
        "Some well-known special cases arise naturally or are mathematically equivalent to other popular forecasting models. For example:\n",
        "\n",
        "An ARIMA(0,1,0) model is given by $ X_{t}=X_{t-1}+\\epsilon_t X_t=X_{t-1}+\\epsilon_t$ — random walk.\n",
        "\n",
        "An ARIMA(0,1,0) with a constant, given by $X_4=c+X_{t-1}+\\epsilon _t X_t=c+X_{t-1}+\\epsilon_t$ — random walk with drift.\n",
        "\n",
        "An ARIMA(0,0,0) - white noise model.\n",
        "\n",
        "An ARIMA(0,1,2) - Damped Holt's model.\n",
        "\n",
        "An ARIMA(0,1,1) - basic exponential smoothing model.\n",
        "\n",
        "An ARIMA(0,2,2) model is given by $X_t=2X_{t-1}-X_{t-2}+(\\alpha +\\beta -2)\\epsilon_{t-1}+(1-\\alpha )\\epsilon_{t-2}+\\epsilon_t$ — which is equivalent to Holt's linear method with additive errors, or double exponential smoothing.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfvhOa54pi3d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import read_csv\n",
        "from pandas import datetime\n",
        "from matplotlib import pyplot\n",
        "\n",
        "def parser(x):\n",
        "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
        "\n",
        "# load in the shampoo sales dataset\n",
        "series = read_csv('shampoo.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
        "\n",
        "# show the first few lines of the pandas dataframe\n",
        "print(series.head())\n",
        "\n",
        "# plot the data\n",
        "series.plot()\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgcUS0Rj8LAz",
        "colab_type": "text"
      },
      "source": [
        "The shampoo sales dataset has a clear trend and is not stationary. This suggests that the time series will require differencing to make it stationary, at least a difference order of 1\n",
        "\n",
        "Let’s also take a quick look at an autocorrelation plot of the time series. This is also built-in to Pandas. The example below plots the autocorrelation for a large number of lags in the time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0icHAEy5g6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import read_csv\n",
        "from pandas import datetime\n",
        "from matplotlib import pyplot as plt\n",
        "from pandas.plotting import autocorrelation_plot\n",
        "\n",
        "def parser(x):\n",
        "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
        "\n",
        "series = read_csv('shampoo.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
        "autocorrelation_plot(series)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DntIHvm8nMl",
        "colab_type": "text"
      },
      "source": [
        "we can see that there is a positive correlation with the first 10-to-12 lags that is perhaps significant for the first 5 lags.\n",
        "\n",
        "A good starting point for the AR parameter of the model may be 5.\n",
        "\n",
        "Let’s start off with something simple. We will fit an ARIMA model to the entire Shampoo Sales dataset and review the residual errors.\n",
        "\n",
        "First, we fit an ARIMA(5,1,0) model. This sets the lag value to 5 for autoregression, uses a difference order of 1 to make the time series stationary, and uses a moving average model of 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9_EtJS985I7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import read_csv\n",
        "from pandas import datetime\n",
        "from pandas import DataFrame\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def parser(x):\n",
        "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
        "\n",
        "series = read_csv('shampoo.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
        "\n",
        "# fit model\n",
        "model = ARIMA(series, order=(5,1,0))\n",
        "model_fit = model.fit(disp=0)\n",
        "\n",
        "# print model summary\n",
        "print(model_fit.summary())\n",
        "\n",
        "# plot residual errors\n",
        "residuals = DataFrame(model_fit.resid)\n",
        "residuals.plot()\n",
        "plt.title('residuals')\n",
        "plt.show()\n",
        "\n",
        "residuals.plot(kind='kde')\n",
        "plt.title('density plot of residuals')\n",
        "plt.show()\n",
        "print(residuals.describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkKedSVm9oXb",
        "colab_type": "text"
      },
      "source": [
        "The density residual plot suggests that the errors are Gaussian\n",
        "\n",
        "Note, that although above we used the entire dataset for time series analysis, ideally we would perform this analysis on just the training dataset when developing a predictive model\n",
        "\n",
        "## Rolling Forecast ARIMA Model\n",
        "A rolling forecast is required given the dependence on observations in prior time steps for differencing and the AR model. A crude way to perform this rolling forecast is to re-create the ARIMA model after each new observation is received.\n",
        "\n",
        "We manually keep track of all observations in a list called history that is seeded with the training data and to which new observations are appended each iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r49agq8m-S1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import read_csv\n",
        "from pandas import datetime\n",
        "from matplotlib import pyplot\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def parser(x):\n",
        "\treturn datetime.strptime('190'+x, '%Y-%m')\n",
        "\n",
        "# read in data with pandas\n",
        "series = read_csv('shampoo.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
        "\n",
        "# extract dataframe values\n",
        "X = series.values\n",
        "\n",
        "# determine the train/test set size and split up the data\n",
        "size = int(len(X) * 0.66)\n",
        "train, test = X[0:size], X[size:len(X)]\n",
        "\n",
        "history = [x for x in train]\n",
        "predictions = list()\n",
        "for t in range(len(test)):\n",
        "  \n",
        "  # define the model using the history which includes the previous prediction\n",
        "\tmodel = ARIMA(history, order=(6,1,1))\n",
        "  \n",
        "  # fit the model\n",
        "\tmodel_fit = model.fit(disp=0)\n",
        "  \n",
        "  # forecast to get the predicted value (this takes care of \n",
        "  # placing data in the original scale if differencing was used)\n",
        "\toutput = model_fit.forecast()\n",
        "\tyhat = output[0]\n",
        "\tpredictions.append(yhat)\n",
        "  \n",
        "  # observed value\n",
        "\tobs = test[t]\n",
        "  \n",
        "  # append the predicted/known value to the history\n",
        "\thistory.append(obs)\n",
        "  \n",
        "  # print the predicted and observed values\n",
        "\tprint('predicted=%f, expected=%f' % (yhat, obs))\n",
        "\n",
        "# determine the error  \n",
        "error = mean_squared_error(test, predictions)\n",
        "print('Test MSE: %.3f' % error)\n",
        "\n",
        "# plot\n",
        "pyplot.plot(test)\n",
        "pyplot.plot(predictions, color='red')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDWoJOjcAGkX",
        "colab_type": "text"
      },
      "source": [
        "## Configuring an ARIMA Model\n",
        "The classical approach for fitting an ARIMA model is to follow the **Box-Jenkins Methodology**.\n",
        "\n",
        "This is a process that uses time series analysis and diagnostics to discover good parameters for the ARIMA model.\n",
        "\n",
        "In summary, the steps of this process are as follows:\n",
        "\n",
        "1. Model Identification. Use plots and summary statistics to identify trends, seasonality, and autoregression elements to get an idea of the amount of differencing and the size of the lag that will be required.\n",
        "\n",
        "2. Parameter Estimation. Use a fitting procedure to find the coefficients of the regression model.\n",
        "\n",
        "3. Model Checking. Use plots and statistical tests of the residual errors to determine the amount and type of temporal structure not captured by the model.\n",
        "\n",
        "The process is repeated until either a desirable level of fit is achieved on training or test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfXzoWmmUrSX",
        "colab_type": "text"
      },
      "source": [
        "# Seasonal Autoregressive Integrated Moving-Average (SARIMA)\n",
        "\n",
        "## What's wrong with ARIMA?\n",
        "\n",
        "ARIMA is that it does not support seasonal data. That is a time series with a repeating cycle.\n",
        "\n",
        "ARIMA expects data that is either not seasonal or has the seasonal component removed, e.g. seasonally adjusted via methods such as seasonal differencing.\n",
        "\n",
        "\n",
        "## What is SARIMA?\n",
        "\n",
        "Seasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component.\n",
        "\n",
        "It adds three new hyperparameters to specify the autoregression (AR), differencing (I) and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality\n",
        "\n",
        "Configuring a SARIMA requires selecting hyperparameters for both the trend and seasonal elements of the series.\n",
        "\n",
        "## Trend Elements\n",
        "\n",
        "There are three trend elements that require configuration just as in ARIMA\n",
        "1. p: autoregression order\n",
        "2. d: difference order\n",
        "3. q: moving average order\n",
        "\n",
        "## Seasonal Elements\n",
        "\n",
        "1. P: seasoonal autoregression order\n",
        "2. D: seasonal difference order\n",
        "3. Q: seasonal moving average order\n",
        "4. m: the number of time seps for a single seasonal period\n",
        "\n",
        "SARIMA(p,d,q)(P,D,Q)m\n",
        "\n",
        "## Hyperparameter selection\n",
        "\n",
        "- auto-correlation function and partial auto-correlaation function plots give a good indication of some parameters\n",
        "- grid search can be used across the trend and seasonal parameters\n",
        "https://machinelearningmastery.com/how-to-grid-search-sarima-model-hyperparameters-for-time-series-forecasting-in-python/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3GF4hIbD3Df",
        "colab_type": "text"
      },
      "source": [
        "## SARIMA in python\n",
        "\n",
        "Let's go back to the temperature data from earlier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn7OQU9LDacM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pandas import Series\n",
        "from matplotlib import pyplot as plt\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# import data using pandas\n",
        "series = Series.from_csv('daily-min-temperatures.csv', header=0)\n",
        "\n",
        "# get the series values\n",
        "X = series.values\n",
        "\n",
        "# define model configuration\n",
        "my_order = (1, 1, 1)\n",
        "my_seasonal_order = (1, 1, 1, 10)\n",
        "\n",
        "# define the model\n",
        "model = SARIMAX(X, order=my_order, seasonal_order=my_seasonal_order)\n",
        "\n",
        "# fit the model\n",
        "model_fit = model.fit()\n",
        "print('Coefficients: %s' % model_fit.params)\n",
        "\n",
        "# make predictions\n",
        "predictions = model_fit.predict(start=0, end=len(X)-1, dynamic=False)\n",
        "\n",
        "# calculate the error\n",
        "error = mean_squared_error(X, predictions)\n",
        "print('\\nTest MSE: %.3f' % error)\n",
        "\n",
        "# plot results\n",
        "plt.figure(1)\n",
        "plt.plot(X, color='blue', label='test')\n",
        "plt.plot(predictions, color='red', label='prediction')\n",
        "plt.legend(loc='best')\n",
        "plt.title('full data set')\n",
        "\n",
        "# zoom in on a section\n",
        "plt.figure(2)\n",
        "plt.plot(X, color='blue', label='test')\n",
        "plt.plot(predictions, color='red', label='prediction')\n",
        "plt.legend(loc='best')\n",
        "plt.xlim((1000,2000))\n",
        "plt.title('zoomed')\n",
        "\n",
        "# zoom in further\n",
        "plt.figure(3)\n",
        "plt.plot(X, color='blue', label='test')\n",
        "plt.plot(predictions, color='red', label='prediction')\n",
        "plt.legend(loc='best')\n",
        "plt.xlim((1400,1500))\n",
        "plt.title('zoomed more')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C8drgRUPm2l",
        "colab_type": "text"
      },
      "source": [
        "**Note**\n",
        "\n",
        "The above example is really only using ARIMA because m=365 as required for a year's worth of day data causes the code to run very slowly.\n",
        "\n",
        "Take a look at the below example to get an idea of predictive power of SARIMA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vnEVQ3hN3FR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SARIMAX example with seasonal component\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from random import random\n",
        "import numpy as np\n",
        "\n",
        "# contrived dataset with a period of 10 data points\n",
        "X = [np.sin(x*2*np.pi/10) + random() for x in range(1, 100)]\n",
        "\n",
        "# define model configuration\n",
        "my_order = (1, 1, 1)\n",
        "my_seasonal_order = (1, 1, 1, 10) # set m=10 because the period is 10 data points\n",
        "\n",
        "# define the model\n",
        "model = SARIMAX(X, order=my_order, seasonal_order=my_seasonal_order)\n",
        "\n",
        "# fit the model\n",
        "model_fit = model.fit()\n",
        "\n",
        "# make predictions\n",
        "num_future = 50\n",
        "predictions = model_fit.predict(start=0, end=len(X)-1, dynamic=False)\n",
        "future = model_fit.predict(start=len(X), end=len(X)+num_future, dynamic=True)\n",
        "\n",
        "total = np.append(predictions, future)\n",
        "\n",
        "# plot results\n",
        "plt.figure(1)\n",
        "plt.plot(X, color='blue', label='test')\n",
        "plt.plot(total, color='red', label='prediction')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKYyxxDcU7vn",
        "colab_type": "text"
      },
      "source": [
        "# Singular Spectrum Analysis\n",
        "\n",
        "Singular spectrum analysis (SSA) is a method for finding structure in time series data. In its basic form, SSA involves no statistical modeling and therefore inference about the significance of the suggested signal features cannot be made\n",
        "\n",
        "In time series analysis, SSA is a nonparametric spectral estimation method. It combines elements of classical time series analysis, multivariate statistics, multivariate geometry, dynamical systems and signal processing. Its roots lie in spectral decomposition of time series and random fields.\n",
        "\n",
        "SSA can be an aid in the decomposition of time series into a sum of components, each having a meaningful interpretation. \n",
        "\n",
        "<img src=https://raw.githubusercontent.com/jojker/PML_Workshops/master/Summer%202019/Day%205%20-%20Goal%204%20-%20Scientific%20Insights%20from%20Learned%20Models/Ex%206%20-%20simple%20TS%20prediction/SSA_mod.jpg width=\"1000\">\n",
        "\n",
        "decomposed signal with the first 21 most significant pieces\n",
        "\n",
        "More SSA details on wikipedia\n",
        "\n",
        "https://en.wikipedia.org/wiki/Singular_spectrum_analysis\n",
        "\n",
        "## Example with Stock Market Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRBCTCrtYsc9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install quandl\n",
        "!pip install quandl\n",
        "\n",
        "# snag some python libraries from github\n",
        "\n",
        "# plotting functionality (mpl_utils.py)\n",
        "!wget https://raw.githubusercontent.com/dmarienko/chaos/master/mpl_utils.py\n",
        "  \n",
        "# SSA functions (ssa_core.py)\n",
        "!wget https://raw.githubusercontent.com/dmarienko/chaos/master/ssa_core.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TqxtbTlX_zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ssa_core import ssa, ssa_predict, ssaview, inv_ssa, ssa_cutoff_order\n",
        "from mpl_utils import set_mpl_theme\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import quandl\n",
        "import pandas as pd\n",
        "import datetime\n",
        "from datetime import timedelta\n",
        "from dateutil import parser\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# customize mpl a bit\n",
        "set_mpl_theme('light')\n",
        "\n",
        "\n",
        "## some handy functions\n",
        "\n",
        "# plotting function\n",
        "def fig(w=16, h=5, dpi=96, facecolor=None, edgecolor=None):\n",
        "    return plt.figure(figsize=(w, h), dpi=dpi, facecolor=facecolor, edgecolor=edgecolor)\n",
        "\n",
        "# mean absolute percent error\n",
        "def mape(f, t):\n",
        "    return 100*((f - t)/t).abs().sum()/len(t)\n",
        "\n",
        "# mean absolute error\n",
        "def mae(f, t):\n",
        "    return 100*((f - t)).abs().sum()/len(t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuD-hN3cZmrH",
        "colab_type": "text"
      },
      "source": [
        "Load adjusted close prices for Microsoft (MSFT) from Quandl."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkY325YkZgyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "symbol = 'MSFT'\n",
        "data = quandl.get('WIKI/%s' % symbol, start_date='2012-01-01', end_date='2017-02-01')\n",
        "closes = data['Adj. Close'].rename('close')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRw0uij0Zz5c",
        "colab_type": "text"
      },
      "source": [
        "Split series into train and test intervals and see how it looks on *chart*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II4ZOwNeZ0sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_date = '2017-01-01'\n",
        "\n",
        "train_d = closes[:test_date]\n",
        "test_d = closes[test_date:]\n",
        "\n",
        "fig(10, 3)\n",
        "plt.plot(train_d, label='Train')\n",
        "plt.plot(test_d, 'r', label='Test')\n",
        "plt.title('%s adjusted daily close prices' % symbol)\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgdV-3j9Z9qC",
        "colab_type": "text"
      },
      "source": [
        "We can see how SSA decomposes original series into trend components and noise.\n",
        "\n",
        "There is chart of original series, reconstructed from first n components and residuals.\n",
        "\n",
        "In statistics, a Q–Q (quantile-quantile) plot is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other\n",
        "\n",
        "\n",
        "**What happens if you increase the number of compenents in the reconstruction?**\n",
        "\n",
        "ex: ssaview(train_d.values, 120, [0,1,2,3,4,5,6,7,8,9,10])\n",
        "\n",
        "\n",
        "**What happens if you change the embedding dimension?**\n",
        "\n",
        "embedding dimension is the window size\n",
        "\n",
        "ex: ssaview(train_d.values, 30, [0,1,2,3])\n",
        "\n",
        "ex: ssaview(train_d.values, 480, [0,1,2,3])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVdJsjQ9Z_P5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig(12,8)\n",
        "ssaview(train_d.values, 120, [0,1,2,3])\n",
        "\n",
        "# function ssaview()\n",
        "\"\"\"\n",
        "    Visualising tools for singular spectrum analysis\n",
        "\n",
        "    Example:\n",
        "    -------\n",
        "    >>> import numpy as np\n",
        "    >>>\n",
        "    >>> x = np.linspace(0, 5, 1000)\n",
        "    >>> y = 2*x + 2*np.sin(5*x) + 0.5*np.random.randn(1000)\n",
        "    >>> ssaview(y, 15, [0,1])\n",
        "\n",
        "    :param y: series\n",
        "    :param dim: the embedding dimension\n",
        "    :param k: components indexes for reconstrunction\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmAFHRPCaEgA",
        "colab_type": "text"
      },
      "source": [
        "Plot the residuals as a density plot to see if it is Gaussian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49yFO0P7aJ1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pc, _, v = ssa(train_d.values, 120)\n",
        "\n",
        "#function ssa()\n",
        "\"\"\"\n",
        "    Singular Spectrum Analysis decomposition for a time series\n",
        "\n",
        "    Example:\n",
        "    -------\n",
        "    >>> import numpy as np\n",
        "    >>>\n",
        "    >>> x = np.linspace(0, 5, 1000)\n",
        "    >>> y = 2*x + 2*np.sin(5*x) + 0.5*np.random.randn(1000)\n",
        "    >>> pc, s, v = ssa(y, 15)\n",
        "\n",
        "    :param y: time series (array)\n",
        "    :param dim: the embedding dimension\n",
        "    :return: (pc, s, v) where\n",
        "             pc is the matrix with the principal components of y\n",
        "             s is the vector of the singular values of y given dim\n",
        "             v is the matrix of the singular vectors of y given dim\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# series reconstruction for a given SSA decomposition using a vector of components\n",
        "reconstructed = inv_ssa(pc, v, [0,1,2,3])\n",
        "\n",
        "# actual valus minus reconstructed values\n",
        "noise = train_d.values - reconstructed\n",
        "\n",
        "# plot the noise as a histogram\n",
        "plt.hist(noise, 50);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjhAnkvaaWgg",
        "colab_type": "text"
      },
      "source": [
        "It's possible to reduce embedding space dimension by finding minimal lag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDeupdPIaYK1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LAG_NUMBER = 120\n",
        "n_co = ssa_cutoff_order(train_d.values, dim=MAX_LAG_NUMBER, show_plot=True)\n",
        "\n",
        "# function ssa_cutoff_order()\n",
        "\"\"\"\n",
        "    Tries to find best cutoff for number of order when increment changes of informational entropy\n",
        "    becomes little and the effective information saturates.\n",
        "\n",
        "    :param x: series\n",
        "    :param dim: embedding dimensions (200 by default)\n",
        "    :param cutoff_pctl: percentile of changes (75%)\n",
        "    :param show_plot: true if we need to see informational curve\n",
        "    :return: cutoff number\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fVuANacad6u",
        "colab_type": "text"
      },
      "source": [
        "Using minimal lag we forecast the price and plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTzHcJ7Yaj9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "days_to_predict = 15\n",
        "\n",
        "# using the best possible lag determined above and 8 reconstruction components\n",
        "# predict the stock values and compare to the test set\n",
        "forecast = ssa_predict(train_d.values, n_co, list(range(8)), days_to_predict, 1e-5)\n",
        "\n",
        "# function ssa_prediction()\n",
        "\"\"\"\n",
        "    Series data prediction based on SSA\n",
        "    \n",
        "    :param x: series to be predicted\n",
        "    :param dim: the embedding dimension\n",
        "    :param k: components indexes for reconstruction\n",
        "    :param n_forecast: number of points to forecast\n",
        "    :param e: minimum value to ensure convergence\n",
        "    :param max_iter: maximum number of iterations\n",
        "    :return: forecasted series\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ45qbmvanIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the predicted values for the test set\n",
        "\n",
        "fig(10, 4)\n",
        "\n",
        "prev_ser = closes[datetime.date.isoformat(parser.parse(test_date) - timedelta(120)):test_date]\n",
        "plt.plot(prev_ser, label='Train Data')\n",
        "\n",
        "test_d = closes[test_date:]\n",
        "f_ser = pd.DataFrame(data=forecast, index=test_d.index[:days_to_predict], columns=['close'])\n",
        "orig = pd.DataFrame(test_d[:days_to_predict])\n",
        "\n",
        "plt.plot(orig, label='Test Data')\n",
        "plt.plot(f_ser, 'r-', marker='.', label='Forecast')\n",
        "plt.ylabel('$',rotation=0)\n",
        "plt.legend()\n",
        "plt.title('Forecasting %s for %d days, MAPE = %.2f%%' % (symbol, days_to_predict, mape(f_ser, orig)));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_YArDiP6JSH",
        "colab_type": "text"
      },
      "source": [
        "# Dynamic Mode Decomposition (DMD)\n",
        "\n",
        "\n",
        "## Toy dataset\n",
        "https://mathlab.github.io/PyDMD/build/html/tutorial1dmd.html\n",
        "\n",
        "In this tutorial we will show how to apply dynamic mode decomposition on snapshots collected during the evolution of a generic system. We present a very simple system since the main purpose of this tutorial is to show the capabilities of the algorithm and the package interface.\n",
        "\n",
        "First of all we import the DMD class from the pydmd package, we set matplotlib for the notebook and we import numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKFZG_dK4J7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pydmd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VupEB6-T6T6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.linalg import pinv2\n",
        "\n",
        "def pinv(x): return pinv2(x, rcond=10*np.finfo(float).eps)\n",
        "\n",
        "from pydmd import DMD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdjBPxdH6Y7C",
        "colab_type": "text"
      },
      "source": [
        "We create the input data by summing two different functions:\n",
        "\n",
        "$f_1(x,t)=sech(x+3)exp(i2.3t) $\n",
        "\n",
        "$f_2(x,t)=2sech(x)tanh(x)exp(i2.8t)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBK-0PBJ6j9C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f1(x,t): \n",
        "    return 1./np.cosh(x+3)*np.exp(2.3j*t)\n",
        "\n",
        "def f2(x,t):\n",
        "    return 2./np.cosh(x)*np.tanh(x)*np.exp(2.8j*t)\n",
        "\n",
        "tnumber=512\n",
        "tmax=8*np.pi\n",
        "x = np.linspace(-5, 5, 128)\n",
        "t = np.linspace(0, tmax, tnumber)\n",
        "\n",
        "xgrid, tgrid = np.meshgrid(x, t)\n",
        "\n",
        "X1 = f1(xgrid, tgrid)\n",
        "X2 = f2(xgrid, tgrid)\n",
        "X = X1 + X2\n",
        "Xtrain=X[:int(tnumber/2),:]\n",
        "Xtest=X[int(tnumber/2):,:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th19n0bx6nbM",
        "colab_type": "text"
      },
      "source": [
        "The plots below represent these functions and the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C2WYmC-6mHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "titles = ['$f_1(x,t)$', '$f_2(x,t)$', '$f$']\n",
        "data = [X1, X2, X]\n",
        "\n",
        "fig = plt.figure(figsize=(17,6))\n",
        "for n, title, d in zip(range(131,134), titles, data):\n",
        "    plt.subplot(n)\n",
        "    plt.pcolor(tgrid, xgrid, d.real)\n",
        "    plt.title(title)\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyxsrcCa6q72",
        "colab_type": "text"
      },
      "source": [
        "Now we have the temporal snapshots in the input matrix rows: we can easily create a new DMD instance and exploit it in order to compute the decomposition on the data. Since the snapshots must be arranged by columns, in this case we need to transpose the matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjXU0Iho6uUz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dmd = DMD(svd_rank=2)\n",
        "dmd.fit(Xtrain.T)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTXpJeHr6ybi",
        "colab_type": "text"
      },
      "source": [
        "The dmd object contains the principal information about the decomposition:\n",
        "\n",
        "*   the attribute modes is a 2D numpy array where the columns are the low-rank structures individuated;\n",
        "*   the attribute dynamics is a 2D numpy array where the rows refer to the time evolution of each mode;\n",
        "*   the attribute eigs refers to the eigenvalues of the low dimensional operator;\n",
        "*   the attribute reconstructed_data refers to the approximated system evolution.\n",
        "Moreover, some helpful methods for the graphical representation are provided.\n",
        "\n",
        "Thanks to the eigenvalues, we can check if the modes are stable or not: if an eigenvalue is on the unit circle, the corresponding mode will be stable; while if an eigenvalue is inside or outside the unit circle, the mode will converge or diverge, respectively. From the following plot, we can note that the two modes are stable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3Nx43ie7AEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for eig in dmd.eigs:\n",
        "    print('Eigenvalue {}: distance from unit circle {}'.format(eig, np.abs(eig.imag**2+eig.real**2 - 1)))\n",
        "\n",
        "dmd.plot_eigs(show_axes=True, show_unit_circle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dx9fDtfd7DtI",
        "colab_type": "text"
      },
      "source": [
        "We can plot the modes and the dynamics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7RIFx_G7Fae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for mode in dmd.modes.T:\n",
        "    plt.plot(x, mode.real)\n",
        "    plt.title('Modes')\n",
        "plt.show()\n",
        "\n",
        "for dynamic in dmd.dynamics:\n",
        "    plt.plot(t[:int(tnumber/2)], dynamic.real)\n",
        "    plt.title('Dynamics')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OhkZBXM7Hgs",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can reconstruct the original dataset as the product of modes and dynamics. We plot the evolution of each mode to emphasize their similarity with the input functions and we plot the reconstructed data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOwEzIpL7IRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(17,6))\n",
        "\n",
        "for n, mode, dynamic in zip(range(131, 133), dmd.modes.T, dmd.dynamics):\n",
        "    plt.subplot(n)\n",
        "    plt.pcolor(xgrid[:int(tnumber/2),:], tgrid[:int(tnumber/2),:], (mode.reshape(-1, 1).dot(dynamic.reshape(1, -1))).real.T)\n",
        "    \n",
        "plt.subplot(133)\n",
        "plt.pcolor(tgrid[:int(tnumber/2),:], xgrid[:int(tnumber/2),:], dmd.reconstructed_data.T.real)\n",
        "plt.colorbar()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhy9KQtf7Nvz",
        "colab_type": "text"
      },
      "source": [
        "We can also plot the absolute error between the data reconstructed from dimensionality reduction and the original one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g0bfGBOiSCIz",
        "colab": {}
      },
      "source": [
        "plt.pcolor(tgrid[:int(tnumber/2),:], xgrid[:int(tnumber/2),:], (Xtrain-dmd.reconstructed_data.T).real)\n",
        "fig = plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TGk8H-K7SST",
        "colab_type": "text"
      },
      "source": [
        "The reconstructed system looks almost equal to the original one: the dynamic mode decomposition made possible the identification of the meaningful structures and the complete reconstruction of the system using only the collected snapshots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0eB9L819XO9",
        "colab_type": "text"
      },
      "source": [
        "# Now we can predict the future states"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq3MZtXt9V1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_one_step(X,dmd):\n",
        "  if dmd._modes.shape[0]!=X.shape[0]:\n",
        "    X=X.T\n",
        "  adjoint_modes = pinv(dmd._modes)\n",
        "  return np.linalg.multi_dot( [dmd._modes, np.diag(dmd._eigs), adjoint_modes, X] )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eGNhUNbALaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we initialize with the first time point of the test set and step onwards from there\n",
        "Y=np.zeros(Xtest.shape)\n",
        "initialization=Xtest[0,:]\n",
        "for tndx in range(int(tnumber/2)):\n",
        "  Y[tndx,:]=predict_one_step(initialization,dmd)\n",
        "  initialization=Y[tndx,:]\n",
        "  \n",
        "  \n",
        "fig = plt.figure(figsize=(6,6))\n",
        "plt.subplot(111)\n",
        "plt.pcolor(tgrid[:int(tnumber/2),:], xgrid[:int(tnumber/2),:], Y.real)\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTKCBBNGSUof",
        "colab_type": "text"
      },
      "source": [
        "Predicting many steps into the future given just one initialization does not capture the dynamics. What if we predict N points ahead while updating the initialization based on the current ground truth. How far ahead can we look?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qmm5n1w1E5Om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now we do a running prediction where we continuously input info\n",
        "look_ahead_amount=10\n",
        "Y=np.zeros(Xtest.shape)\n",
        "for tndx in range(0,int(tnumber/2)):\n",
        "  #initialization=X[tndx+int(tnumber/2)-look_ahead_amount,:]\n",
        "  initialization=Xtest[tndx,:]\n",
        "  for step in range(look_ahead_amount):\n",
        "    initialization = predict_one_step(initialization,dmd)\n",
        "  #Y[tndx,:] = initialization\n",
        "  if (tndx+look_ahead_amount-1)>=int(tnumber/2):\n",
        "    break\n",
        "  Y[tndx+look_ahead_amount-1,:] = initialization\n",
        "  \n",
        "  \n",
        "fig = plt.figure(figsize=(12,6))\n",
        "plt.subplot(131)\n",
        "plt.pcolor(tgrid[:int(tnumber/2),:], xgrid[:int(tnumber/2),:], Y.real)\n",
        "plt.colorbar()\n",
        "plt.subplot(132)\n",
        "plt.pcolor(tgrid[:int(tnumber/2),:], xgrid[:int(tnumber/2),:], Xtest.real)\n",
        "plt.colorbar()\n",
        "plt.subplot(133)\n",
        "plt.pcolor(tgrid[11:int(tnumber/2),:], xgrid[11:int(tnumber/2),:], abs((Xtest[11:,:]-Y[11:,:]).real))\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9XX6-j7SuxQ",
        "colab_type": "text"
      },
      "source": [
        "Looking ahead 10 points, based on the current value does a decent job, how far can we extend this?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqUBHrrmCpgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets step through several values for the look ahead, up to 1/2 the length of Xtest\n",
        "prctiles=np.zeros((1,int(tnumber/4)))\n",
        "for look_ahead_amount in range(int(tnumber/4)):\n",
        "  Y=np.zeros(Xtest.shape)\n",
        "  #look_ahead_amount=lk_ndx\n",
        "  for tndx in range(int(tnumber/2)):\n",
        "    initialization=X[tndx+int(tnumber/2)-look_ahead_amount,:]\n",
        "    for step in range(look_ahead_amount):\n",
        "      initialization = predict_one_step(initialization,dmd)\n",
        "    Y[tndx,:] = initialization\n",
        "  prctiles[0,look_ahead_amount]=np.percentile(np.divide(abs((Xtest-Y).real),abs(Xtest.real)),50)\n",
        "  \n",
        "\n",
        "# the plot does not show. IDK why? -JKJ\n",
        "fig=plt.figure()\n",
        "plt.subplot(111)\n",
        "plt.plot(np.expand_dims(np.arange(prctiles.shape[1]),axis=0),prctiles,linewidth=5.0)\n",
        "plt.title('error')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}