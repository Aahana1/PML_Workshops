{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compute Clusters and parallelization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0ba0lElYEt3",
        "colab_type": "text"
      },
      "source": [
        "# Compute Clusters and parallelization\n",
        "\n",
        "Parallelization, or concurrency is a vital part of your data analysis tool kit. Imagine your analysis takes 3 hours to run on one data point. It would take 2 weeks to run on 100 data points! On an 8 core home computer you could get it down to 2 days, on a large enough cluster you could do it in one pass, a little over 3 hours! \n",
        "\n",
        "In short, if you have a lot of options to explore or have to do the same long calculation on many things then you speed up your code by a factor proportional to the number of cores in your CPU. \n",
        "\n",
        "Today, you don't even need a cluster. If your advisor is open to paying for a new computer, consider getting a multicore processor such as a 32 or 64 core AMD threadripper or 16 core intel i9 and building your own ([it's easier than you think](https://www.howtogeek.com/187797/dont-be-intimidated-building-your-own-computer-is-easier-than-youd-think/)). The cost would be $3-4K and you wouldn't need to wait for any cluster. \n",
        "\n",
        "## contents\n",
        "\n",
        "\n",
        "1.   Multithreading vs multiprocessing\n",
        "2.   Threadsafety\n",
        "2.   Multiprocessing example code\n",
        "2.   Multithreading example code\n",
        "2.   CHPC usage (Mallinckrodt Institute of Radiology)\n",
        "2.   Physics HPC facility\n",
        "2.   GPU acceleration with CUDA\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7xlAhQFB1Qc",
        "colab_type": "text"
      },
      "source": [
        "## Multithreading vs multiprocessing\n",
        "\n",
        "Modern computers have separate modules (cores) within the same central processing unit (CPU). It's always been the case that networking, USB, and memory components are separate from the CPU and much slower than the CPU. Having your separate CPU cores working on your program at the same time is multiprocessing in a nutshell. Having your CPU cores work on the next part of your problem while they wait for the other components is multithreading in a nutshell. \n",
        "\n",
        "If your problem is mostly calculation based and the data is loaded beforehand and held in RAM (held in a variable), then your problem is \"CPU bound\" and fit for multiprocessing. All of my experience is with this type of problem.\n",
        "\n",
        "If your problem involves back and forth communication over the internet or with another device, or if your problem involves loading a datapoint, doing something relatively simple (filtering) and then immediately saving it again, then your problem is \"I/O bound\" and multithreading is probably best. \n",
        "\n",
        "Multiprocessing has a pretty big negative. For CPU bound problems that are pretty short and simple then multiprocessing can take longer than doing it in series or multithreading. This is because multiprocessing involves relatively lengthy set-up and tear down. \n",
        "\n",
        "Multiprocessing has (almost) limitless advantage. The more cores you can run on the bigger the speed up you'll get. However you'll quickly run out of RAM if you keep too much stored in memory. So it is important to avoid having variables that store the same thing and be very careful about what you send to each process. \n",
        "\n",
        "[https://realpython.com/python-concurrency/](https://realpython.com/python-concurrency/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rLsd5X3B1WY",
        "colab_type": "text"
      },
      "source": [
        "## Thread safety\n",
        "\n",
        "\n",
        "Several things can go wrong when trying multiprocessing or multithreading. All of these problems are caused by code that is not written in a \"threadsafe\" way. In the contexts you are likely to encounter while doing data analysis thread safety means that none of your different processes or workers are modifying the same files or variables, and that you have protected your functions so that they cannot be interrupted while running. \n",
        "\n",
        "\n",
        "\n",
        "*   Always define the portion to be run in parallel in a self-contained function (you need to do this anyway)\n",
        "*   Never define a variable in a script that is later used in one of the functions. Always either define the variable within the function itself, or pass a *copy* of the variable as an argument to the function\n",
        "*   When using multithreading use `threading.local` to encapsulated your function in a way that avoids interruption.\n",
        "*   Do not allow separate workers or threads to write to or modify the same file, generally it's OK to let them read the same file if you open with a read-only operation. \n",
        "\n",
        "[https://en.wikipedia.org/wiki/Thread_safety](https://en.wikipedia.org/wiki/Thread_safety)\n",
        "\n",
        "[https://en.wikipedia.org/wiki/Reentrancy_(computing)](https://en.wikipedia.org/wiki/Reentrancy_(computing))\n",
        "\n",
        "[https://en.wikipedia.org/wiki/Race_condition](https://en.wikipedia.org/wiki/Race_condition)\n",
        "\n",
        "[https://realpython.com/python-concurrency/](https://realpython.com/python-concurrency/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVVdkgNcYMiu",
        "colab_type": "text"
      },
      "source": [
        "# Multiprocessing: The Ultimate Kamehameha\n",
        "\n",
        "If your function takes more than a few seconds the gain in running it multiple times at once will outweigh the cost of the long initialization time. The bigger the computer the more speed gains you will get. \n",
        "\n",
        "[multiprocessing module documentation](https://python.readthedocs.io/en/latest/library/multiprocessing.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_6qm_ZeVrJD",
        "colab_type": "code",
        "outputId": "dd16c661-badd-4ca4-edb0-4d20ea29bccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "!rm get_max_eig.py\n",
        "!wget https://raw.githubusercontent.com/jojker/PML_Workshops/master/Summer%202019/Day%201%20-%20Process%20and%20Design%20for%20Rapid%20Progress/Ex%205%20-%20Computing%20clusters%2C%20parallelization%20and%20GPUs/get_max_eig.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'get_max_eig.py': No such file or directory\n",
            "--2019-07-17 23:12:38--  https://raw.githubusercontent.com/jojker/PML_Workshops/master/Summer%202019/Day%201%20-%20Process%20and%20Design%20for%20Rapid%20Progress/Ex%205%20-%20Computing%20clusters%2C%20parallelization%20and%20GPUs/get_max_eig.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 180 [text/plain]\n",
            "Saving to: ‘get_max_eig.py’\n",
            "\n",
            "get_max_eig.py      100%[===================>]     180  --.-KB/s    in 0s      \n",
            "\n",
            "2019-07-17 23:12:43 (3.51 MB/s) - ‘get_max_eig.py’ saved [180/180]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOKhXiBSzWVW",
        "colab_type": "code",
        "outputId": "eb5e1bc0-dd17-451a-b79e-5501c98b0fd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "import multiprocessing\n",
        "import time\n",
        "import numpy as np\n",
        "from get_max_eig import get_max_eig\n",
        "\n",
        "matsize=1000 # when set to 100 multithreading is faster, set to 1000 and multiprocessing is\n",
        "N_threads=2 # we only have two cpus in google colab\n",
        "\n",
        "# Joint output is a special kind of object that gets written to by each worker \n",
        "# as they work (instead of collecting everything with a function output assignment)\n",
        "\"\"\"\n",
        "joint_output=multiprocessing.Queue()\n",
        "\"\"\"\n",
        "\n",
        "# define a function which does NOT modify ANY global (previously defined) varaibles except a Queue\n",
        "# NOTE, for Windows environemnts, when running \"interactively\" (in ipython, and \n",
        "# IDE or in a notebook) the function you use must be imported and cannot be defined in the same script/session\n",
        "\"\"\"\n",
        "def get_max_eig(chunk):\n",
        "  idx=chunk[1] \n",
        "  arry=chunk[0]\n",
        "  eigvals=np.linalg.eigvalsh(arry)\n",
        "  joint_output.put((eigvals.max(),idx))\n",
        "  return (eigvals.max(),idx)\n",
        "\"\"\"\n",
        "        \n",
        "\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "# example 1, generating then separating arrays for independent work\n",
        "data=[(np.random.rand(matsize,matsize),x) for x in range(20)] # threading handles chunking for us!\n",
        "# # example 2, splitting an array and working on the peices\n",
        "# data = np.random.rand(20,20)\n",
        "# chunks = [(x,ndx) for ndx,x in enumerate np.array_split(data, N_threads)]\n",
        "print('confirm the data splitting')\n",
        "print([x[0].shape for x in data]) # verify the splitting\n",
        "\n",
        "start_time = time.time()\n",
        "pool = multiprocessing.Pool(N_threads)\n",
        "print('doing stuff')\n",
        "gathered_chunks=[x for x in pool.map(get_max_eig, data)]\n",
        "print('stuff is done')\n",
        "maxeig=[x[0] for x in gathered_chunks]\n",
        "index_out=[x[1] for x in gathered_chunks]\n",
        "maxeig_ndx=maxeig.index(max(maxeig))\n",
        "maxeig_ndx=gathered_chunks[maxeig_ndx][1]\n",
        "maxeig=max(maxeig)\n",
        "maxeig_array=data[maxeig_ndx]\n",
        "print('display maximum eigenvalue')\n",
        "print(maxeig)\n",
        "print('display the array which gives the maximum eigenvalue')\n",
        "print(maxeig_array)\n",
        "\"\"\"# do it again but with the joint output instead of the returned list\n",
        "joint_output=[joint_output.get() for x in range(joint_output.qsize())]\n",
        "# extra step because we can't index on joint_output.get()\n",
        "maxeig2=[x[0] for x in joint_output]\n",
        "index_out2=[x[1] for x in joint_output]\n",
        "maxeig_ndx2=maxeig2.index(max(maxeig2))\n",
        "maxeig_ndx2=joint_output[maxeig_ndx2][1]\n",
        "maxeig2=max(maxeig2)\n",
        "maxeig_array2=data[maxeig_ndx2]\n",
        "print('pool.map output indices')\n",
        "print(index_out)\n",
        "print('joint_output indices')\n",
        "print(index_out2)\n",
        "print('does maxeig match?')\n",
        "print(maxeig2==maxeig)\n",
        "print('does maxeig_array match?')\n",
        "print(maxeig_array2==maxeig_array)\"\"\"\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"It took {elapsed_time} seconds to process {len(data)} matrices\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "confirm the data splitting\n",
            "[(1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000)]\n",
            "doing stuff\n",
            "stuff is done\n",
            "display maximum eigenvalue\n",
            "500.7909554915367\n",
            "display the array which gives the maximum eigenvalue\n",
            "(array([[0.37778615, 0.04055442, 0.5150767 , ..., 0.03943158, 0.73087757,\n",
            "        0.0720365 ],\n",
            "       [0.01054151, 0.13469659, 0.65589384, ..., 0.73879981, 0.80468291,\n",
            "        0.52541305],\n",
            "       [0.19337457, 0.70387836, 0.92017039, ..., 0.15074992, 0.97590554,\n",
            "        0.89088734],\n",
            "       ...,\n",
            "       [0.22036265, 0.04114682, 0.89241079, ..., 0.68879522, 0.18420228,\n",
            "        0.56430834],\n",
            "       [0.14627942, 0.5395083 , 0.28127738, ..., 0.83322918, 0.9966674 ,\n",
            "        0.84475125],\n",
            "       [0.49913105, 0.35256568, 0.20839051, ..., 0.1089864 , 0.72080256,\n",
            "        0.90222864]]), 6)\n",
            "It took 4.349038362503052 seconds to process 20 matrices\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnPOR__jaUZQ",
        "colab_type": "text"
      },
      "source": [
        "#Multithreading: an opening maneuver\n",
        "\n",
        "Most useful for preprocessing raw data, or changing to significantly different formats\n",
        "Not so useful for analysis that involves large matrix calculations or lengthy analysis. \n",
        "\n",
        "[threading module documentation](https://python.readthedocs.io/en/stable/library/threading.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvM5lJo0X8ty",
        "colab_type": "code",
        "outputId": "5af8bc22-2ae1-408e-a1ff-6a4a1da7eb2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "import concurrent.futures\n",
        "import threading\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "matsize=1000 # when set to 100 multithreading is faster, set to 1000 and multiprocessing is\n",
        "thread_local = threading.local()\n",
        "N_threads=2 # set to 2 to make a fair comparison with multiprocessing\n",
        "\n",
        "\n",
        "def threadsafe_eigs(chunk):\n",
        "    if not hasattr(thread_local, \"get_max_eig\"):\n",
        "        thread_local.get_max_eig = get_max_eig\n",
        "    return thread_local.get_max_eig(chunk)\n",
        "\n",
        "\n",
        "def find_best_matrix(chunks):\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=N_threads) as executor:\n",
        "        gathered_chunks=[x for x in executor.map(threadsafe_eigs, chunks)]\n",
        "        maxeig=[x[0] for x in gathered_chunks]\n",
        "        maxeig_ndx=maxeig.index(max(maxeig))\n",
        "        maxeig_ndx=gathered_chunks[maxeig_ndx][1]\n",
        "        maxeig=max(maxeig)\n",
        "        maxeig_array=chunks[maxeig_ndx]\n",
        "        print('display maximum eigenvalue')\n",
        "        print(maxeig)\n",
        "        print('display the array which gives the maximum eigenvalue')\n",
        "        print(maxeig_array)\n",
        "        \n",
        "        \n",
        "def get_max_eig(chunk):\n",
        "  idx=chunk[1]\n",
        "  arry=chunk[0]\n",
        "  eigvals=np.linalg.eigvalsh(arry)\n",
        "  return (eigvals.max(),idx)\n",
        "        \n",
        "          \n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # example 1, generating then separating arrays for independent work\n",
        "  data=[(np.random.rand(matsize,matsize),x) for x in range(20)] # threading handles chunking for us!\n",
        "  # # example 2, splitting an array and working on the peices\n",
        "  # data = np.random.rand(20,20)\n",
        "  # chunks = [(x,ndx) for ndx,x in enumerate np.array_split(data, N_threads)]\n",
        "  print([x[0].shape for x in data]) # verify the splitting\n",
        "\n",
        "\n",
        "  start_time = time.time()\n",
        "  find_best_matrix(data)\n",
        "  elapsed_time = time.time() - start_time\n",
        "  print(f\"It took {elapsed_time} seconds to process {len(data)} matrices\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000), (1000, 1000)]\n",
            "display maximum eigenvalue\n",
            "500.73135604672143\n",
            "display the array which gives the maximum eigenvalue\n",
            "(array([[0.3149457 , 0.2068915 , 0.22573461, ..., 0.02841787, 0.95432047,\n",
            "        0.74206671],\n",
            "       [0.69095165, 0.313048  , 0.71878623, ..., 0.83661787, 0.33232412,\n",
            "        0.94947128],\n",
            "       [0.27470046, 0.26825978, 0.53783859, ..., 0.25204323, 0.86274221,\n",
            "        0.65192078],\n",
            "       ...,\n",
            "       [0.50701093, 0.3294186 , 0.17844519, ..., 0.15431113, 0.66597849,\n",
            "        0.39329202],\n",
            "       [0.42368908, 0.54263458, 0.10372451, ..., 0.43460681, 0.03996008,\n",
            "        0.03100962],\n",
            "       [0.04301358, 0.9771131 , 0.22318735, ..., 0.7582938 , 0.7450777 ,\n",
            "        0.73792397]]), 17)\n",
            "It took 16.927886486053467 seconds to process 20 matrices\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI6Dz9m5hTVp",
        "colab_type": "text"
      },
      "source": [
        "**Parallelization isn't always faster**\n",
        "\n",
        "Numpy and other libraries are often already optimized pretty well. Don't start with parallelization unless you're really sure you'll need it. In our example with finding the eigenvalues it's actually faster to run without any parallelization (on the Colab runtime). \n",
        "\n",
        "If we increase the size of the matrices, use a different eigenvalue solver, or have a machine that offers a larger number of cores then it may be faster to run in parallel. This just underscores that it's important to check for prejudice and test versions of your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TNKwfReNtpc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a954000b-21f0-4670-fe15-6efb9c07aa7f"
      },
      "source": [
        "# time the process on the GPU\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "matsize=1000 # when set to 100 multithreading is faster, set to 1000 and multiprocessing is\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # example 1, generating then separating arrays for independent work\n",
        "  start_time = time.time()\n",
        "  data=[np.random.rand(matsize,matsize) for x in range(20)] # threading handles chunking for us!\n",
        "  maxeig=[np.linalg.eigvalsh(x).max() for x in data]\n",
        "  maxeig_ndx=maxeig.index(max(maxeig))\n",
        "  maxeig=max(maxeig)\n",
        "  maxeig_array=data[maxeig_ndx]\n",
        "  print('display maximum eigenvalue')\n",
        "  print(maxeig)\n",
        "  print('display the array which gives the maximum eigenvalue')\n",
        "  print(maxeig_array)\n",
        "\n",
        "    \n",
        "  elapsed_time = time.time() - start_time\n",
        "  print(f\"It took {elapsed_time} seconds to process {len(data)} matrices\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "display maximum eigenvalue\n",
            "500.6734298972959\n",
            "display the array which gives the maximum eigenvalue\n",
            "[[0.98449371 0.43177062 0.61564363 ... 0.38888417 0.42846851 0.984318  ]\n",
            " [0.35795872 0.76382852 0.65613542 ... 0.28824312 0.99909606 0.76161075]\n",
            " [0.1807745  0.56936573 0.31718037 ... 0.58533865 0.94706122 0.53129491]\n",
            " ...\n",
            " [0.48159875 0.65400046 0.62950296 ... 0.21707006 0.25133658 0.67652186]\n",
            " [0.72972474 0.79512367 0.93688658 ... 0.39924894 0.60771342 0.75055381]\n",
            " [0.52199843 0.90473609 0.82041336 ... 0.53058552 0.44492439 0.94395564]]\n",
            "It took 3.8288612365722656 seconds to process 20 matrices\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5dhu8MqaQK0",
        "colab_type": "text"
      },
      "source": [
        "# WashU's Center for High Performance Computing\n",
        "**FORMERLY Mallinckrodt Center for High Performance Computing**\n",
        "\n",
        "The multiprocessing module used previously does NOT let you use multiple cores across multiple computers. It only lets you use different cores on the same computer. The CHPC is set up to enable one program to use the cores of multiple separate machines. It uses the Message Passing Interface (MPI), so we need to use a different python module: mpi4py. \n",
        "\n",
        "However if you are running your code on only one compute node then multiprocessing will suit you well. \n",
        "\n",
        "## Set up :\n",
        "\n",
        "\n",
        "\n",
        "1.   [Request and get access](https://www.mir.wustl.edu/research/research-support-facilities/center-for-high-performance-computing-chpc/services/request-an-account) (Click the link to the left)\n",
        "3.   Learn to use and set up your favorite SFTP client and SSH: [FileZilla](https://filezilla-project.org/) or [MobaXterm](https://mobaxterm.mobatek.net/) (for windows to run Graphical User Interface software remotely)\n",
        "2.   Always run:     \n",
        "`export PATH=/export/Anaconda3-5.2.0/bin:$PATH`    \n",
        "`module load openmpi-2.0.0-intel-15.0.1` (or some other MPI module if you need to run MPI)\n",
        "2.   Set up virtual environment    \n",
        "`conda create --name environment_name python=3`    \n",
        "`source activate environment_name`    \n",
        "This is because you can now install your own modules: `conda install --name name_of_package`    \n",
        "or from outside the environment: `conda install --name name_of_env name_of_package`\n",
        "3.   Email alerts are set up to the email you registered with in step one, if you want to change the address do the following.     \n",
        "got to your home directory `home/your_username/`    \n",
        "create a text file named `.forward` (not \"something.forward\" not \"\".forward.txt\" just \".forward\")    \n",
        "put one line an one line only in it, your email address: \"some_account@some_domain.name\"    \n",
        "Note: you need to explicitly enable email notifications in the next step.\n",
        "4.   PBS script    \n",
        "example below\n",
        "5.   MPI    \n",
        "example below\n",
        "\n",
        "## other resources\n",
        "[CHPC link](https://www.mir.wustl.edu/research/research-support-facilities/center-for-high-performance-computing-chpc/services) - [on campus wiki](http://mgt2.chpc.wustl.edu/wiki119/index.php/Main_Page)\n",
        "\n",
        "[queuing system](https://www.mir.wustl.edu/research/research-support-facilities/center-for-high-performance-computing-chpc/for-researchers/queuing-system) - [on campus wiki](http://mgt2.chpc.wustl.edu/wiki119/index.php/Queuing_System)\n",
        "[PBS](https://www.chpc.utah.edu/documentation/software/pbs-scheduler.php)\n",
        "\n",
        "\n",
        "software availability and limitations\n",
        "[software availability](https://www.mir.wustl.edu/research/research-support-facilities/center-for-high-performance-computing-chpc/services/software) - [on campus wiki](http://mgt2.chpc.wustl.edu/wiki119/index.php/Software)\n",
        "\n",
        "[data storage rules](https://www.mir.wustl.edu/research/research-support-facilities/center-for-high-performance-computing-chpc/for-researchers/rules-and-guidelines) - [on campus wiki](http://mgt2.chpc.wustl.edu/wiki119/index.php/FAQ#Storage) (very low data storage limits without paying)\n",
        "\n",
        "Monitoring current usage\n",
        "[Ganglia](http://mgt2.chpc.wustl.edu/ganglia)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91HVYGvLeDRm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PBS script example\n",
        "\n",
        "#!/bin/bash\n",
        "\n",
        "# NOTE: #PBS comments request resources and have to come BEFORE any commands\n",
        "# NOTE: after adapting this script to your needs you run it with the command:\n",
        "# qsub this_script_file_name\n",
        "# NOTE: the # comes immediately before PBS, no space\n",
        "\n",
        "\n",
        "# This option will send email when the job starts/finishes\n",
        "# To receive this email, you'll need a .forward file in your home\n",
        "# directory that contains the email address you wish to receive your\n",
        "# email at.\n",
        "#PBS -m be\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Give the job a name\n",
        "#PBS -N JKJ_test_job1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Specify the resources needed:\n",
        "\n",
        "# ** EXAMPLE 1/3 ** This assumes the job requires less \n",
        "# than 3GB of memory.  If you increase the memory requested, it\n",
        "# will limit the number of jobs you can run per node, so only  \n",
        "# increase it when necessary (i.e. the job gets killed for violating\n",
        "# the memory limit). \n",
        "# Because it requests only one node it *should* work with multiprocessing\n",
        "# #PBS -l nodes=1:ppn=2,walltime=00:15:00,mem=3gb\n",
        "\n",
        "# ** EXAMPLE 2/3 ** Specify 32 by specifying 4 nodes with 8 cores each\n",
        "# determine the nodes and cores by refering to the table on the page below:\n",
        "# https://www.mir.wustl.edu/research/research-support-facilities/center-for-high-performance-computing-chpc/for-researchers/queuing-system\n",
        "# each row is a node you can pick and the second column is the max \"ppn\".\n",
        "# Because it requests 4 nodes use mpi4py not multiprocessing.\n",
        "# #PBS -l nodes=4:ppn=8,walltime=24:00:00\n",
        "\n",
        "# ** EXAMPLE 3/3 ** Specify a number of gpus\n",
        "# #PBS -l nodes=1:ppn=1:gpus=1\n",
        "\n",
        "\n",
        "# actually specify the resources\n",
        "#PBS -l nodes=1:ppn=2,walltime=00:15:00,mem=3gb\n",
        "\n",
        "# Specify the default queue for the fastest nodes\n",
        "#PBS -q dque\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ** BEGIN OPTIONAL SECTION ** for issuing terminal commands, used when you didn't \n",
        "# already go to the right directory or need to shuttle files around.\n",
        "# NOTE: to run these commands delete the # and the space after it\n",
        "\n",
        "# cd Into the run directory; I'll create a new directory to run under\n",
        "# cd /scratch/mtobias\n",
        "# mkdir freesurfer\n",
        "# cd freesurfer\n",
        "\n",
        "# copy in the input data to the current directory\n",
        "# cp /export/freesurfer/subjects/sample-001.mgz .\n",
        "# copy in the input data to a subdirectory\n",
        "# cp /export/freesurfer/subjects/sample-002.mgz ./data_folder/\n",
        "\n",
        "# load software packages (other than your python scripts) see the software available below\n",
        "# https://www.mir.wustl.edu/research/research-support-facilities/center-for-high-performance-computing-chpc/services/software\n",
        "# module load package_name\n",
        "\n",
        "# create environment variables (i.e. set the paths)\n",
        "export PATH=/export/Anaconda3-5.2.0/bin:$PATH\n",
        "export PATH=/usr/lib64/openmpi-1.4/bin/:$PATH\n",
        "  \n",
        "# access your virtual environment with everything installed correctly\n",
        "source activate environment_name\n",
        "  \n",
        "# ** END OPTIONAL SECTION **\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# run your python script with MPI\n",
        "mpirun -np 2 python test1.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz-8Y8oFpQ46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MPI example script AKA \"test1.py\" in the PBS script above\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy as np\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "size = comm.Get_size()\n",
        "rank = comm.Get_rank()\n",
        "name = MPI.Get_processor_name()\n",
        "\n",
        "print(\"Hello from rank {0} of {1} on {2}\".format(rank, size, name))\n",
        "\n",
        "\n",
        "def get_max_eigs(chunk,rank):\n",
        "  maxeig=-np.inf*(1+1j)\n",
        "  ndcs=[];\n",
        "  for idx, arry in enumerate(chunk):\n",
        "    eigvals=np.linalg.eigvalsh(arry)\n",
        "    if eigvals.max().real>=maxeig.real and eigvals.max().imag>=maxeig.imag:\n",
        "      print(\"eigval is {0} on rank {1} index {2}\".format(eigvals.max(),rank,idx))\n",
        "      maxeig=eigvals.max()\n",
        "      ndcs.append((rank,idx))\n",
        "  return (maxeig,ndcs)\n",
        "\n",
        "\n",
        "\n",
        "# assemble data to be operated on in parallel\n",
        "if rank == 0: # if the root node AKA \"controller node\"\n",
        "  # load data from files at the root only (loading is not shown here)\n",
        "  \n",
        "  \n",
        "  # example 1, generating then separating arrays for independent work\n",
        "  data=[np.random.rand(10,10) for x in range(20)]\n",
        "  chunks=[np.array_split(chunk,chunk.shape[0]) for chunk in np.array_split(data,N_threads)]\n",
        "  data=[];\n",
        "  # # example 2, splitting an array and working on the peices\n",
        "  # data = np.random.rand(20,20)\n",
        "  # chunks = np.array_split(data, size)\n",
        "  print([x.shape for x in chunks]) # verify the splitting\n",
        "else:\n",
        "  chunks = None\n",
        "  \n",
        "# send the data off to the different workers\n",
        "chunk = comm.scatter(chunks, root=0)\n",
        "\n",
        "# perform the calculation that is done on all workers\n",
        "chunk_result=get_max_eigs(chunk,rank)\n",
        "# WARNING AVOID FILE READ/WRITES (SAVING) UNTIL YOU UNDERSTAND THREAD SAFETY\n",
        "\n",
        "# collect the data from the separate workers\n",
        "gathered_chunks = comm.gather(chunk_result, root=0) # gather items to the root\n",
        "\n",
        "if rank ==0: # do a final calculation at the root ONLY\n",
        "  maxeig=[x[0] for x in gathered_chunks]\n",
        "  maxeig_ndx=maxeig.index(max(maxeig))\n",
        "  maxeig_ndx=gathered_chunks[maxeig_ndx][1][0]\n",
        "  maxeig=max(maxeig)\n",
        "  maxeig_array=chunks[maxeig_ndx[0]][maxeig_ndx[1]]\n",
        "  print(maxeig)\n",
        "  print(maxeig_array)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GowSQKjF0vjS",
        "colab_type": "text"
      },
      "source": [
        "# Physics department High Performance Computing facility\n",
        "\n",
        "\n",
        "## Set up :\n",
        "\n",
        "\n",
        "\n",
        "1.   Get access by emailing Sai (sai@physics.wustl.edu), ask him to set up a data folder too, unless you can use a low data storage limit.    \n",
        "Get a list of the machines available and their capabilties here: [https://web.physics.wustl.edu/intranet/Pages/Computing/hpcCenter.php](https://web.physics.wustl.edu/intranet/Pages/Computing/hpcCenter.php)\n",
        "1.   Learn to use and set up your favorite SFTP client and SSH: [FileZilla](https://filezilla-project.org/) or [MobaXterm](https://mobaxterm.mobatek.net/) (for windows to run Graphical User Interface software remotely)\n",
        "1.   Log in to any node and run `hpcload` to get a list of machines and their current usage\n",
        "1.   Log in to a node with acceptable capabilities and usage\n",
        "1.   First run the command `htop` and hit F5 to go to \"tree view\" to view all the scripts that every user is running.    \n",
        "The number of workers allocated is 1/2 the number of lines for Matlab and multiprocessing methods.    \n",
        " **Watch for a while as resource usage can fluctuate/spike.** \n",
        "1.   **Determine the number of cores you want to allocate based on the previous step.** Remember the optimal solution to \"[tragedy of the commons](https://en.wikipedia.org/wiki/Tragedy_of_the_commons)\" type resource usage is that every new user requests X% of what ever remains (I guess 75%). \n",
        "1.   You shouldn't need to set up the paths like on the CHPC\n",
        "1.   Set up virtual environment    \n",
        "`conda create --name environment_name python=3`    \n",
        "`source activate environment_name`    \n",
        "This is because you can now install your own modules: `conda install --name name_of_package`    \n",
        "or from outside the environment: `conda install --name name_of_env name_of_package`\n",
        "1.   No email alerts unless you write a function to do it: [https://realpython.com/python-send-email/](https://realpython.com/python-send-email/)\n",
        "    \n",
        "    \n",
        "The cluster is very free and open, some users don't check very thoroughly before running.    \n",
        "Remember if someone's (my) code spikes to high usage every once in a while and combined with yours allocation goes above 100% of machine capabilities then the spike will last longer. If allocation is too far above 100% everyone's code will get slower and slower.    \n",
        "Always take the time to watch on `htop`. Never allocate above 100%. The one exception is when the extant code and new code both have low usage with rare spikes and there is a low probability that two spikes will co-occur. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP5t0a1Nv8Zw",
        "colab_type": "text"
      },
      "source": [
        "## GPU acceleration\n",
        "\n",
        "If your computer has a modern graphics card then it is possible to parallelize your code *thousands* of times by having your Graphics Processing Unit do the work instead of your CPU. However there are some pretty big catches. The memory available for each \"chunk\" is pretty limited, meaning you can't send large programs to each of the workers and the GPU handles information in a very different way. So the software available to do it is also pretty limited, and depends on having CUDA or openCL correctly configured on your machine.\n",
        "\n",
        "Unless you wanted to get into different programming languages and write a lot of lines of code, all just to handle tedious tasks then you will be reliant on python libraries such as cupy. CuPy has versions of common NumPy functions that have been adapted to run on GPUs. CuPy also lets you run custom functions, called \"kernels\". Another option is to use the GPU capabilities of the Numba library, which is better at enabling you to write custom functions without much tedium, but has limited expressivity. Numba has the benefit of also being able to handle multiprocessing on multiple CPUs, but in the same constrained way it handles GPU computation.\n",
        "\n",
        "### DO THIS\n",
        "Change your runtime type to include GPU acceleration (see the \"runtime menu above\"), then run the following cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnWOfZY8v7Dl",
        "colab_type": "code",
        "outputId": "8aa1855e-28c4-4b66-bb13-78e7d40a9a57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "# check if you have Cuda\n",
        "!nvcc --version"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2018 NVIDIA Corporation\n",
            "Built on Sat_Aug_25_21:08:01_CDT_2018\n",
            "Cuda compilation tools, release 10.0, V10.0.130\n",
            "Collecting cupy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/18/a8c5f594d6fc70d70e53c7847eabc9b922c037b3b7dff4af89a499217d42/cupy-6.1.0.tar.gz (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy) (1.16.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy) (1.12.0)\n",
            "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy) (0.4)\n",
            "Building wheels for collected packages: cupy\n",
            "  Building wheel for cupy (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeR9XW0vKmyw",
        "colab_type": "code",
        "outputId": "f526c55b-e5dd-41e3-8788-140fbde594ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Install CuPy for the CUDA version above\n",
        "!pip install cupy-cuda100\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cupy-cuda100 in /usr/local/lib/python3.6/dist-packages (5.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100) (1.16.4)\n",
            "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda100) (0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBWzWRZBwgD5",
        "colab_type": "code",
        "outputId": "ebfd8df3-b619-46be-db92-61dc2bc97f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# time the process on the GPU\n",
        "import time\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "\n",
        "matsize=1000 # when set to 100 multithreading is faster, set to 1000 and multiprocessing is\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # example 1, generating then separating arrays for independent work\n",
        "  start_time = time.time()\n",
        "  data=[cp.array(np.random.rand(matsize,matsize)) for x in range(20)] # threading handles chunking for us!\n",
        "  maxeig=[cp.linalg.eigvalsh(x).max() for x in data]\n",
        "  maxeig_ndx=maxeig.index(max(maxeig))\n",
        "  maxeig=max(maxeig)\n",
        "  maxeig_array=data[maxeig_ndx]\n",
        "  print('display maximum eigenvalue')\n",
        "  print(maxeig)\n",
        "  print('display the array which gives the maximum eigenvalue')\n",
        "  print(maxeig_array)\n",
        "\n",
        "    \n",
        "  elapsed_time = time.time() - start_time\n",
        "  print(f\"It took {elapsed_time} seconds to process {len(data)} matrices\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "display maximum eigenvalue\n",
            "501.02482052606683\n",
            "display the array which gives the maximum eigenvalue\n",
            "[[0.39853323 0.88142266 0.37750285 ... 0.6924593  0.70819448 0.0519846 ]\n",
            " [0.93231736 0.55569323 0.31035479 ... 0.43053994 0.56119633 0.32046951]\n",
            " [0.22635327 0.92344492 0.10111251 ... 0.93197159 0.90083852 0.79663857]\n",
            " ...\n",
            " [0.62390399 0.57356271 0.95838915 ... 0.57640116 0.11349385 0.27786203]\n",
            " [0.76045963 0.27599598 0.87402397 ... 0.05726385 0.54164207 0.60788906]\n",
            " [0.7960042  0.3108535  0.7417449  ... 0.16183558 0.75761693 0.76943369]]\n",
            "It took 4.932931184768677 seconds to process 20 matrices\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}