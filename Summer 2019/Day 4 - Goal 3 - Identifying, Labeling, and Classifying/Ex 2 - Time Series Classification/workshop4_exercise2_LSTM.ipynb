{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "workshop4_exercise2_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjoS2HQAosud",
        "colab_type": "text"
      },
      "source": [
        "# Workshop 4 Exercise 2 Time Series Classification\n",
        "\n",
        "link to the LSTM slides\n",
        "\n",
        "https://github.com/jojker/PML_Workshops/blob/master/Summer%202019/Day%204%20-%20Goal%203%20-%20Identifying%2C%20Labeling%2C%20and%20Classifying/Ex%202%20-%20Time%20Series%20Classification/LSTM_notes.pptx"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFjUTf4R8LLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this file immediately to extract a large zip file from a drive account\n",
        "# ~ 10 minutes\n",
        "\n",
        "# method to get a large file from google drive \n",
        "\n",
        "fileid='1Te8hQcCINvabe7WttGdpE1ojQtqogI89'\n",
        "filename='train_simplified.zip'\n",
        "\n",
        "## WGET ##\n",
        "!wget --save-cookies cookies.txt 'https://docs.google.com/uc?export=download&id='$fileid -O- \\\n",
        "     | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' > confirm.txt\n",
        "\n",
        "!wget --load-cookies cookies.txt -O $filename \\\n",
        "     'https://docs.google.com/uc?export=download&id='$fileid'&confirm='$(<confirm.txt)\n",
        "\n",
        "## CURL ##\n",
        "!curl -L -c cookies.txt 'https://docs.google.com/uc?export=download&id='$fileid \\\n",
        "     | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' > confirm.txt\n",
        "\n",
        "!curl -L -b cookies.txt -o $filename \\\n",
        "     'https://docs.google.com/uc?export=download&id='$fileid'&confirm='$(<confirm.txt)\n",
        "\n",
        "!rm -f confirm.txt cookies.txt\n",
        "\n",
        "# unzip the file\n",
        "!unzip train_simplified.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP50qnV5CBdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the test set from github\n",
        "fileid='1VOzqQDVMe5Jge2eD6RhmOnFsnll_A--b'\n",
        "filename='test_simplified.csv'\n",
        "\n",
        "## WGET ##\n",
        "!wget --save-cookies cookies.txt 'https://docs.google.com/uc?export=download&id='$fileid -O- \\\n",
        "     | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' > confirm.txt\n",
        "\n",
        "!wget --load-cookies cookies.txt -O $filename \\\n",
        "     'https://docs.google.com/uc?export=download&id='$fileid'&confirm='$(<confirm.txt)\n",
        "\n",
        "## CURL ##\n",
        "!curl -L -c cookies.txt 'https://docs.google.com/uc?export=download&id='$fileid \\\n",
        "     | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' > confirm.txt\n",
        "\n",
        "!curl -L -b cookies.txt -o $filename \\\n",
        "     'https://docs.google.com/uc?export=download&id='$fileid'&confirm='$(<confirm.txt)\n",
        "\n",
        "!rm -f confirm.txt cookies.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSj1yC3GOhcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get pretrained weights from a google drive account\n",
        "fileid='16reUvzJ0eYpNsht_6PyUJG0heVJtKyQu'\n",
        "filename='stroke_lstm_model_weights.best.hdf5'\n",
        "\n",
        "## WGET ##\n",
        "!wget --save-cookies cookies.txt 'https://docs.google.com/uc?export=download&id='$fileid -O- \\\n",
        "     | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' > confirm.txt\n",
        "\n",
        "!wget --load-cookies cookies.txt -O $filename \\\n",
        "     'https://docs.google.com/uc?export=download&id='$fileid'&confirm='$(<confirm.txt)\n",
        "\n",
        "## CURL ##\n",
        "!curl -L -c cookies.txt 'https://docs.google.com/uc?export=download&id='$fileid \\\n",
        "     | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1/p' > confirm.txt\n",
        "\n",
        "!curl -L -b cookies.txt -o $filename \\\n",
        "     'https://docs.google.com/uc?export=download&id='$fileid'&confirm='$(<confirm.txt)\n",
        "\n",
        "!rm -f confirm.txt cookies.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V62H947xPh-v"
      },
      "source": [
        "# Keras tutorial using TensorFlow backend\n",
        "The majority of the notebook was created by Kevin Mader for the \"Quick, Draw! Doodle Recognition Challenge\" on Kaggle \n",
        "\n",
        "https://www.kaggle.com/c/quickdraw-doodle-recognition\n",
        "\n",
        "https://www.kaggle.com/kmader/quickdraw-baseline-lstm-reading-and-submission/notebook\n",
        "\n",
        "Kaggle description of the challenge:\n",
        "\n",
        "\"Quick, Draw!\" was released as an experimental game to educate the public in a playful way about how AI works. The game prompts users to draw an image depicting a certain category, such as ”banana,” “table,” etc. The game generated more than 1B drawings, of which a subset was publicly released as the basis for this competition’s training set. That subset contains 50M drawings encompassing 340 label categories.\n",
        "\n",
        "Sounds fun, right? Here's the challenge: since the training data comes from the game itself, drawings can be incomplete or may not match the label. You’ll need to build a recognizer that can effectively learn from this noisy data and perform well on a manually-labeled test set from a different distribution.\n",
        "\n",
        "Your task is to build a better classifier for the existing Quick, Draw! dataset. By advancing models on this dataset, Kagglers can improve pattern recognition solutions more broadly. This will have an immediate impact on handwriting recognition and its robust applications in areas including OCR (Optical Character Recognition), ASR (Automatic Speech Recognition) & NLP (Natural Language Processing).\n",
        "\n",
        "<img src=https://hexacoto.files.wordpress.com/2016/11/screen-shot-2016-11-17-at-5-22-04-pm.png width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4O5K3F2PPh-x"
      },
      "source": [
        "# Kaggle description of the dataset\n",
        "The Quick Draw Dataset is a collection of millions of drawings across 300+ categories, contributed by players of Quick, Draw! The drawings were captured as timestamped vectors, tagged with metadata including what the player was asked to draw and in which country the player was located.\n",
        "\n",
        "Two versions of the data are given. The raw data is the exact input recorded from the user drawing, while the simplified version removes unnecessary points from the vector information. (For example, a straight line may have been recorded with 8 points, but since you only need 2 points to uniquely identify a line, 6 points can be dropped.) The simplified files are much smaller and provide effectively the same information.\n",
        "\n",
        "For this competition, you may use the raw files, the simplified files, or both. You can find out more details about the drawing format on the quickdraw-dataset github page.\n",
        "\n",
        "Your models should predict the correct \"word\" of the drawing. IMPORTANT: Some \"words\" are actually more than one word! The training data aligns to the Quick Draw dataset that that was previously released, and uses spaces to delimit multi-word labels. The Kaggle metric for this competition requires labels with no spaces, so you will need to adjust your label predictions to replace spaces with underscores. For example, \"roller coaster\" should be predicted as \"roller_coaster\". You may predict up to 3 guesses per drawing. See the Evaluation page for the correct format.\n",
        "\n",
        "Here we will use the simplified dataset!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5qS5IOJLPh-y"
      },
      "source": [
        "# Outline of the project\n",
        "1. preprocess data using 1D convolutions\n",
        "2. stack two LSTM (Long Short-Term Memory) neural networks\n",
        "3. apply two dense neural network layers\n",
        "4. classify shape\n",
        "5. create submittable file for Kaggle competition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tw0wtFNDPh-0"
      },
      "source": [
        "# CNN - Convolutional Neural Network\n",
        "Images are represented as matrices of numbers. For greyscale images these numbers range from 0 to 255. Matrices are convolved to created a feature map. For example a 7x7 matrix convolved with a 3x3 matrix may look like\n",
        "\n",
        "<img src=https://anhvnn.files.wordpress.com/2018/02/convolve.png? width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PMQo_ycTPh-2"
      },
      "source": [
        "# LSTM - Long Short-Term Memory\n",
        "LSTM networks are units of a Recurrent Neural Network (RNN) commonly composed of four parts. 1) cell - remembers values over a certain time 2) input gate - controls the extent to which a new value flows into the cell 3) output gate -  controls the extent to which the cell value controls the output activation 4) forget gate - controlling the extent to which the value remains in the cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jQXmpggfPh-3"
      },
      "source": [
        "# Model Parameters\n",
        "Here we keep track of the relevant parameters for the data preprocessing, model construction and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iEaovvuGPh-4",
        "colab": {}
      },
      "source": [
        "#batch_size = 32 # local machine will likely require a batch size much smaller than 2048\n",
        "batch_size = 2048\n",
        "STROKE_COUNT = 196\n",
        "TRAIN_SAMPLES = 750\n",
        "VALID_SAMPLES = 75\n",
        "TEST_SAMPLES = 50\n",
        "\n",
        "# check data storage on colab\n",
        "!df -h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kxZG56koPh-9",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "from keras.metrics import top_k_categorical_accuracy\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
        "from glob import glob\n",
        "\n",
        "# garbage collection (automated memory management)\n",
        "import gc\n",
        "gc.enable()\n",
        "\n",
        "# function for calculating the mean accuracy across k number of predictions for multiclass classification problems\n",
        "def top_3_accuracy(x,y): return top_k_categorical_accuracy(x,y, 3)\n",
        "\n",
        "# function for checking the number of available GPUs\n",
        "def get_available_gpus():\n",
        "    from tensorflow.python.client import device_lib\n",
        "    local_device_protos = device_lib.list_local_devices()\n",
        "    return [x.name for x in local_device_protos if x.device_type == 'GPU']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ph5d8hTjPhyc",
        "colab": {}
      },
      "source": [
        "# add the location of the data to the path\n",
        "working_dir = os.getcwd()\n",
        "\n",
        "# location of test data file\n",
        "test_path = os.path.join(working_dir, 'test_simplified.csv')\n",
        "\n",
        "# location of pre-trained weights or location of place to save weights\n",
        "weight_path = os.path.join(working_dir, \"{}_weights.best.hdf5\").format('stroke_lstm_model')\n",
        "new_weight_path = os.path.join(working_dir, \"{}_weights.new.hdf5\").format('stroke_lstm_model')\n",
        "\n",
        "print(working_dir)\n",
        "print(test_path)\n",
        "print(weight_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u7OgBOMePh_D",
        "colab": {}
      },
      "source": [
        "from ast import literal_eval\n",
        "\n",
        "# glob here finds all the csv files containing the data to train on\n",
        "ALL_TRAIN_PATHS = glob(os.path.join(working_dir, '*.csv'))\n",
        "\n",
        "test_simplified_file = os.path.join(working_dir, 'test_simplified.csv')\n",
        "\n",
        "# make sure the test file is not in the training file path\n",
        "if test_simplified_file in ALL_TRAIN_PATHS: ALL_TRAIN_PATHS.remove(test_simplified_file)\n",
        "\n",
        "print(\"number of csv files containing data: \", len(ALL_TRAIN_PATHS))\n",
        "\n",
        "# column names of the csv files\n",
        "COL_NAMES = ['countrycode', 'drawing', 'key_id', 'recognized', 'timestamp', 'word']\n",
        "\n",
        "def _stack_it(raw_strokes):\n",
        "    \"\"\"preprocess the string and make \n",
        "    a standard Nx3 stroke vector\"\"\"\n",
        "    stroke_vec = literal_eval(raw_strokes) # string->list\n",
        "    # unwrap the list\n",
        "    in_strokes = [(xi,yi,i)  \n",
        "     for i,(x,y) in enumerate(stroke_vec) \n",
        "     for xi,yi in zip(x,y)]\n",
        "    c_strokes = np.stack(in_strokes)\n",
        "    # replace stroke id with 1 for continue, 2 for new\n",
        "    c_strokes[:,2] = [1]+np.diff(c_strokes[:,2]).tolist()\n",
        "    c_strokes[:,2] += 1 # since 0 is no stroke\n",
        "    # pad the strokes with zeros\n",
        "    return pad_sequences(c_strokes.swapaxes(0, 1), \n",
        "                         maxlen=STROKE_COUNT, \n",
        "                         padding='post').swapaxes(0, 1)\n",
        "def read_batch(samples=5, \n",
        "               start_row=0,\n",
        "               max_rows = 1000):\n",
        "    \"\"\"\n",
        "    load and process the csv files\n",
        "    this function is horribly inefficient but simple\n",
        "    \"\"\"\n",
        "    out_df_list = []\n",
        "    for c_path in ALL_TRAIN_PATHS:\n",
        "        c_df = pd.read_csv(c_path, nrows=max_rows, skiprows=start_row)\n",
        "        c_df.columns=COL_NAMES\n",
        "        out_df_list += [c_df.sample(samples)[['drawing', 'word']]]\n",
        "    full_df = pd.concat(out_df_list)\n",
        "    full_df['drawing'] = full_df['drawing'].\\\n",
        "        map(_stack_it)\n",
        "    \n",
        "    return full_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PtkeVjyePh_H"
      },
      "source": [
        "# Reading and Parsing\n",
        "Since it is too much data (7GB, the full data set from kaggle is 23GB) to read in at once, we just take a portion of it for training, validation and hold-out testing. This should give us an idea about how well the model works, but leaves lots of room for improvement later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mwkecYprPh_I",
        "colab": {}
      },
      "source": [
        "# take a portion of the data from the unzipped csv files to be training data, validation data, and testing data\n",
        "train_args = dict(samples=TRAIN_SAMPLES, \n",
        "                  start_row=0, \n",
        "                  max_rows=int(TRAIN_SAMPLES*1.5))\n",
        "valid_args = dict(samples=VALID_SAMPLES, \n",
        "                  start_row=train_args['max_rows']+1, \n",
        "                  max_rows=VALID_SAMPLES+25)\n",
        "test_args = dict(samples=TEST_SAMPLES, \n",
        "                 start_row=valid_args['max_rows']+train_args['max_rows']+1, \n",
        "                 max_rows=TEST_SAMPLES+25)\n",
        "\n",
        "# read in the data using the definitions in previous cell\n",
        "train_df = read_batch(**train_args)\n",
        "valid_df = read_batch(**valid_args)\n",
        "test_df = read_batch(**test_args)\n",
        "\n",
        "# encode labels between 0 and num_classes-1\n",
        "word_encoder = LabelEncoder()\n",
        "word_encoder.fit(train_df['word'])\n",
        "print('words', len(word_encoder.classes_), '=>', ', '.join([x for x in word_encoder.classes_]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yXAtYhraPh_M"
      },
      "source": [
        "# Stroke-based Classification\n",
        "Here we use the stroke information to train a model and see if the strokes give us a better idea of what the shape could be.\n",
        "A stroke is a line made with the drawing utencil. The stacking function above builds the object by creating a list of strokes made (lines) used to build up the drawing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sMO5MBY-Ph_O",
        "colab": {}
      },
      "source": [
        "def get_Xy(in_df):\n",
        "    X = np.stack(in_df['drawing'], 0)\n",
        "    y = to_categorical(word_encoder.transform(in_df['word'].values))\n",
        "    return X, y\n",
        "train_X, train_y = get_Xy(train_df)\n",
        "valid_X, valid_y = get_Xy(valid_df)\n",
        "test_X, test_y = get_Xy(test_df)\n",
        "print(train_X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G9EN7EqZPh_S",
        "colab": {}
      },
      "source": [
        "# plot some of the drawings from the training set along with their labels\n",
        "fig, m_axs = plt.subplots(3,3, figsize = (16, 16))\n",
        "rand_idxs = np.random.choice(range(train_X.shape[0]), size = 9)\n",
        "for c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n",
        "    test_arr = train_X[c_id]\n",
        "    test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n",
        "    lab_idx = np.cumsum(test_arr[:,2]-1)\n",
        "    for i in np.unique(lab_idx):\n",
        "        c_ax.plot(test_arr[lab_idx==i,0], \n",
        "                np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n",
        "    c_ax.axis('off')\n",
        "    c_ax.set_title(word_encoder.classes_[np.argmax(train_y[c_id])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jLBGnIz0Ph_V"
      },
      "source": [
        "# LSTM to Parse Strokes\n",
        "The model suggested from the tutorial is\n",
        "\n",
        "\n",
        "\n",
        "<img src=https://www.tensorflow.org/images/quickdraw_model.png width=\"400\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SbKkVsGAPh_V",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import BatchNormalization, Conv1D, LSTM, Dense, Dropout\n",
        "\n",
        "if len(get_available_gpus())>0:\n",
        "    print(len(get_available_gpus()), \"GPUs are available\")\n",
        "    from keras.layers import CuDNNLSTM as LSTM # this one is about 3x faster on GPU instances\n",
        "\n",
        "# initialize the sequential model\n",
        "stroke_read_model = Sequential()\n",
        "\n",
        "# apply batch normalization to normalize the input layer by adjusting and scaling the activations\n",
        "stroke_read_model.add(BatchNormalization(input_shape = (None,)+train_X.shape[2:]))\n",
        "\n",
        "# convolution and dropout layers\n",
        "stroke_read_model.add(Conv1D(48, (5,)))\n",
        "stroke_read_model.add(Dropout(0.3))\n",
        "stroke_read_model.add(Conv1D(64, (5,)))\n",
        "stroke_read_model.add(Dropout(0.3))\n",
        "stroke_read_model.add(Conv1D(96, (3,)))\n",
        "stroke_read_model.add(Dropout(0.3))\n",
        "\n",
        "# LSTM layers with dropout\n",
        "stroke_read_model.add(LSTM(128, return_sequences = True))\n",
        "stroke_read_model.add(Dropout(0.3))\n",
        "stroke_read_model.add(LSTM(128, return_sequences = False))\n",
        "stroke_read_model.add(Dropout(0.3))\n",
        "stroke_read_model.add(Dense(512))\n",
        "stroke_read_model.add(Dropout(0.3))\n",
        "\n",
        "# fully connected (dense) layer with softmax activation for prediction\n",
        "stroke_read_model.add(Dense(len(word_encoder.classes_), activation = 'softmax'))\n",
        "\n",
        "# load partially pre-trained weights if desired\n",
        "#stroke_read_model.load_weights(weight_path)\n",
        "\n",
        "# compile the model using the adam optimizer\n",
        "stroke_read_model.compile(optimizer = 'adam', \n",
        "                          loss = 'categorical_crossentropy', \n",
        "                          metrics = ['categorical_accuracy', top_3_accuracy])\n",
        "\n",
        "# print a summary of the model\n",
        "stroke_read_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aAIrs460Ph_Z",
        "colab": {}
      },
      "source": [
        "# method to save weights after a certain amount of training\n",
        "checkpoint = ModelCheckpoint(new_weight_path, monitor='val_loss', verbose=1, \n",
        "                             save_best_only=True, mode='min', save_weights_only = True)\n",
        "\n",
        "# method to reduce the learning rate if the loss function plateaus\n",
        "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, \n",
        "                                   verbose=1, mode='auto', min_delta=0.0001, cooldown=5, min_lr=0.0001)\n",
        "\n",
        "# method to stop training early if the the algorithm does not improve after a certain amount of training\n",
        "early = EarlyStopping(monitor=\"val_loss\", \n",
        "                      mode=\"min\", \n",
        "                      patience=5) # probably needs to be more patient, but kaggle time is limited\n",
        "\n",
        "# place each development above in the model using the callbacks list\n",
        "callbacks_list = [checkpoint, early, reduceLROnPlat]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cKb63G3QPh_b",
        "colab": {}
      },
      "source": [
        "# Train the data for n epochs\n",
        "from IPython.display import clear_output\n",
        "stroke_read_model.fit(train_X, train_y,\n",
        "                      validation_data = (valid_X, valid_y), \n",
        "                      batch_size = batch_size,\n",
        "                      epochs = 2,\n",
        "                      callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q2FDr43fPh_e",
        "colab": {}
      },
      "source": [
        "# load some pretrained weights to see how well the model can do if we train for a while\n",
        "stroke_read_model.load_weights(weight_path)\n",
        "\n",
        "# evaluate the pretrained weights\n",
        "lstm_results = stroke_read_model.evaluate(test_X, test_y, batch_size = batch_size)\n",
        "\n",
        "print('Accuracy: %2.1f%%, Top 3 Accuracy %2.1f%%' % (100*lstm_results[1], 100*lstm_results[2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Tp2i16rPh_h",
        "colab": {}
      },
      "source": [
        "# test the model showing the precision, recall, and f1-score\n",
        "\n",
        "# keras has some convenient functions for evaluating the model\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "test_cat = np.argmax(test_y, 1)\n",
        "pred_y = stroke_read_model.predict(test_X, batch_size = batch_size)\n",
        "pred_cat = np.argmax(pred_y, 1)\n",
        "\n",
        "# show the confusoin matrix (just a way to see how well the model did)\n",
        "plt.matshow(confusion_matrix(test_cat, pred_cat))\n",
        "\n",
        "print(classification_report(test_cat, pred_cat, \n",
        "                            target_names = [x for x in word_encoder.classes_]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ot6kEofAPh_k"
      },
      "source": [
        "# Reading Point by Point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N6ScSuqlPh_m",
        "colab": {}
      },
      "source": [
        "# show the images point by point and see what the classifier believes the image\n",
        "# is at each step\n",
        "\n",
        "points_to_use = [5, 15, 20, 30, 40, 50]\n",
        "points_to_user = [108]\n",
        "samples = 12\n",
        "word_dex = lambda x: word_encoder.classes_[x]\n",
        "rand_idxs = np.random.choice(range(test_X.shape[0]), size = samples)\n",
        "fig, m_axs = plt.subplots(len(rand_idxs), len(points_to_use), figsize = (24, samples/8*24))\n",
        "for c_id, c_axs in zip(rand_idxs, m_axs):\n",
        "    res_idx = np.argmax(test_y[c_id])\n",
        "    goal_cat = word_encoder.classes_[res_idx]\n",
        "    \n",
        "    for pt_idx, (pts, c_ax) in enumerate(zip(points_to_use, c_axs)):\n",
        "        test_arr = test_X[c_id, :].copy()\n",
        "        test_arr[pts:] = 0 # short sequences make CudnnLSTM crash, ugh \n",
        "        stroke_pred = stroke_read_model.predict(np.expand_dims(test_arr,0))[0]\n",
        "        top_10_idx = np.argsort(-1*stroke_pred)[:10]\n",
        "        top_10_sum = np.sum(stroke_pred[top_10_idx])\n",
        "        \n",
        "        test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n",
        "        lab_idx = np.cumsum(test_arr[:,2]-1)\n",
        "        for i in np.unique(lab_idx):\n",
        "            c_ax.plot(test_arr[lab_idx==i,0], \n",
        "                    np.max(test_arr[:,1])-test_arr[lab_idx==i,1], # flip y\n",
        "                      '.-')\n",
        "        c_ax.axis('off')\n",
        "        if pt_idx == (len(points_to_use)-1):\n",
        "            c_ax.set_title('Answer: %s (%2.1f%%) \\nPredicted: %s (%2.1f%%)' % (goal_cat, 100*stroke_pred[res_idx]/top_10_sum, word_dex(top_10_idx[0]), 100*stroke_pred[top_10_idx[0]]/top_10_sum))\n",
        "        else:\n",
        "            c_ax.set_title('%s (%2.1f%%), %s (%2.1f%%)\\nCorrect: (%2.1f%%)' % (word_dex(top_10_idx[0]), 100*stroke_pred[top_10_idx[0]]/top_10_sum, \n",
        "                                                                 word_dex(top_10_idx[1]), 100*stroke_pred[top_10_idx[1]]/top_10_sum, \n",
        "                                                                 100*stroke_pred[res_idx]/top_10_sum))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jGIvz4c_NWpj"
      },
      "source": [
        "# Submission\n",
        "We can create a submission to Kaggle using the following model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdJeVgMUN3Ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(test_path)\n",
        "sub_df = pd.read_csv(test_path)\n",
        "sub_df['drawing'] = sub_df['drawing'].map(_stack_it)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJDfnrGVN3_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub_vec = np.stack(sub_df['drawing'].values, 0)\n",
        "sub_pred = stroke_read_model.predict(sub_vec, verbose=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAMEHa7eN8um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_3_pred = [word_encoder.classes_[np.argsort(-1*c_pred)[:3]] for c_pred in sub_pred]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzDKZG2bN-vf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "top_3_pred = [' '.join([col.replace(' ', '_') for col in row]) for row in top_3_pred]\n",
        "top_3_pred[:3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FeBN60UYPh_4"
      },
      "source": [
        "# Show some predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "00LugA8BPh_5",
        "colab": {}
      },
      "source": [
        "fig, m_axs = plt.subplots(3,3, figsize = (16, 16))\n",
        "rand_idxs = np.random.choice(range(sub_vec.shape[0]), size = 9)\n",
        "for c_id, c_ax in zip(rand_idxs, m_axs.flatten()):\n",
        "    test_arr = sub_vec[c_id]\n",
        "    test_arr = test_arr[test_arr[:,2]>0, :] # only keep valid points\n",
        "    lab_idx = np.cumsum(test_arr[:,2]-1)\n",
        "    for i in np.unique(lab_idx):\n",
        "        c_ax.plot(test_arr[lab_idx==i,0], \n",
        "                np.max(test_arr[:,1])-test_arr[lab_idx==i,1], '.-')\n",
        "    c_ax.axis('off')\n",
        "    c_ax.set_title(top_3_pred[c_id])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GG2WbznuPh_7",
        "colab": {}
      },
      "source": [
        "inc = 1 # save file increment number\n",
        "submission_file_path = working_dir+\"/submission_\"+str(inc)+\".csv\"\n",
        "\n",
        "while os.path.isfile(submission_file_path):\n",
        "  inc = inc + 1\n",
        "  submission_file_path = working_dir+\"/submission_\"+str(inc)+\".csv\"\n",
        "\n",
        "with open (submission_file_path,'a') as submission_file:\n",
        "  sub_df['word'] = top_3_pred\n",
        "  sub_df[['key_id', 'word']].to_csv(submission_file, index=False)\n",
        "\n",
        "print(\"Saving as next increment: %d\" %inc)\n",
        "print(\"Saved file: %s\" %\"submission_\"+str(inc)+\".csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HPeOXBOhQZzP"
      },
      "source": [
        "Things to try:\n",
        "1. different optimizers\n",
        "2. increasing the amount of data used \n",
        "3. different NN architecture\n",
        "4. run for more epochs with patience increased"
      ]
    }
  ]
}